=== .env ===
GROQ_API_KEY="your_key"
HF_API_KEY="your_key" 
TEXTGEN_API_KEY="your_key"


=== .gitignore ===


=== configs\base.yaml ===
project:
  name: "AI-code-assistant"
  version: "0.0.1"
  
paths:
  data_raw: "./data/raw"
  data_processed: "./data/processed"
  model_checkpoints: "./models"
  
languages:
  priority: ["python", "csharp", "c", "javascript", "typescript", "html", "css"]
  
quality_standards:
  min_complexity: 0.4  # 0-1 scale
  required_keys: ["answer", "explanation"]  # For structured responses
  banned_patterns:
    - "eval("
    - "system("
    - "os.popen"


=== configs\integration.yaml ===
integrations:
  # Plugin configurations
  plugins:
    ollama:
      enabled: true
      config:
        base_url: "http://localhost:11434"
        default_model: "codellama"
        timeout: 30
        batch_size: 4  # Added batching support

    vllm:
      enabled: true
      config:
        model: "codellama/CodeLlama-7b-hf"
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.9
        max_batch_size: 2048  # Tokens

    textgen:
      enabled: true
      config:
        base_url: "http://localhost:5000"
        api_key: "${TEXTGEN_API_KEY}"  # From environment
        batch_size: 4
        timeout: 45

    huggingface:
      enabled: false  # Disabled by default
      config:
        api_key: "${HF_API_KEY}"
        model_name: "codellama/CodeLlama-7b-hf"
        device: "auto"
        quantize: false
        batch_size: 2

    grok:
      enabled: true
      config:
        api_key: "${GROQ_API_KEY}"
        rate_limit: 5  # Requests per minute
        timeout: 15

    lmstudio:
      enabled: false  # Disabled by default
      config:
        base_url: "http://localhost:1234"
        timeout: 60
        batch_support: false

  # Global integration settings
  settings:
    default_timeout: 30  # Fallback timeout
    priority_order:  # Execution priority
      - "vllm"
      - "ollama" 
      - "grok"
      - "huggingface"
      - "textgen"
      - "lmstudio"

    # Batch processing defaults
    batching:
      enabled: true
      max_batch_size: 8
      max_wait_ms: 50

    # Monitoring
    health_check_interval: 60  # Seconds
  
  load_balancing:
    update_interval: 10  # Seconds
    min_requests: 20     # Minimum data points before activating
    priority_bump: 2.0   # Weight multiplier for high-priority queries


=== configs\model.yaml ===


=== configs\predictions.yaml ===
# configs/prediction.yaml
cache:
  warmers: 2
  max_predictions: 3
  min_confidence: 0.7


=== configs\sla_tiers.yaml ===
tiers:
  critical:
    min_accuracy: 0.96
    max_latency: 1.2
    allowed_providers: ["gpt-4", "claude-2"]
    cost_multiplier: 2.5
    
  standard:
    min_accuracy: 0.88  
    max_latency: 2.5
    allowed_providers: ["gpt-3.5", "claude-instant"]
    
  economy:
    min_accuracy: 0.75
    max_latency: 7.0
    allowed_providers: ["llama2", "local"]


=== core\analysis.py ===
# core/analysis.py
import re
from enum import Enum

class ContentType(Enum):
    CODE_PYTHON = "code_python"
    CODE_CSHARP = "code_csharp"
    MATH_SYMBOLIC = "math_symbolic"
    TEXT_QUERY = "text_query"

class ContentAnalyzer:
    CODE_PATTERNS = {
        ContentType.CODE_PYTHON: [
            r'def\s+\w+\(.*\):',
            r'import\s+\w+'
        ],
        ContentType.CODE_CSHARP: [
            r'public\s+(class|interface)\s+\w+',
            r'using\s+\w+;'
        ]
    }
    
    def analyze(self, text: str) -> ContentType:
        for content_type, patterns in self.CODE_PATTERNS.items():
            if any(re.search(p, text) for p in patterns):
                return content_type
        return ContentType.TEXT_QUERY


=== core\analysis\advanced_analyser.py ===
# core/analysis/advanced_analyzer.py
import ast
import re
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

class CodeComplexity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class CodeSmell(Enum):
    LONG_METHOD = "long_method"
    LARGE_CLASS = "large_class"
    DUPLICATE_CODE = "duplicate_code"
    COMPLEX_CONDITIONAL = "complex_conditional"
    MAGIC_NUMBER = "magic_number"
    LONG_PARAMETER_LIST = "long_parameter_list"

@dataclass
class CodeIssue:
    type: CodeSmell
    severity: str  # "low", "medium", "high", "critical"
    description: str
    line_number: int
    suggestion: str

@dataclass
class CodeMetrics:
    complexity: CodeComplexity
    maintainability: float  # 0-1 scale
    reliability: float    # 0-1 scale
    security_score: float  # 0-1 scale
    issues: List[CodeIssue]
    total_lines: int
    comment_ratio: float

class AdvancedCodeAnalyzer:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.code_patterns = self._load_code_patterns()
        self.security_patterns = self._load_security_patterns()
    
    def analyze_code(self, code: str, language: str) -> CodeMetrics:
        """Comprehensive code analysis"""
        if language == "python":
            return self._analyze_python_code(code)
        elif language == "javascript":
            return self._analyze_javascript_code(code)
        else:
            return self._analyze_generic_code(code)
    
    def _analyze_python_code(self, code: str) -> CodeMetrics:
        """Analyze Python code with AST"""
        try:
            tree = ast.parse(code)
        except SyntaxError:
            # Handle syntax errors
            return CodeMetrics(
                complexity=CodeComplexity.HIGH,
                maintainability=0.1,
                reliability=0.1,
                security_score=0.5,
                issues=[],
                total_lines=len(code.split('\n')),
                comment_ratio=0.0
            )
        
        # Calculate various metrics
        complexity = self._calculate_complexity(tree, code)
        maintainability = self._calculate_maintainability(tree, code)
        reliability = self._calculate_reliability(tree, code)
        security_score = self._calculate_security_score(tree, code)
        issues = self._detect_code_smells(tree, code)
        total_lines = len(code.split('\n'))
        comment_ratio = self._calculate_comment_ratio(code)
        
        return CodeMetrics(
            complexity=complexity,
            maintainability=maintainability,
            reliability=reliability,
            security_score=security_score,
            issues=issues,
            total_lines=total_lines,
            comment_ratio=comment_ratio
        )
    
    def _calculate_complexity(self, tree: ast.AST, code: str) -> CodeComplexity:
        """Calculate cyclomatic complexity"""
        complexity = 1  # Base complexity
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, (ast.And, ast.Or)):
                complexity += 1
        
        if complexity <= 5:
            return CodeComplexity.LOW
        elif complexity <= 10:
            return CodeComplexity.MEDIUM
        else:
            return CodeComplexity.HIGH
    
    def _calculate_maintainability(self, tree: ast.AST, code: str) -> float:
        """Calculate maintainability score (0-1)"""
        # Factors: function length, class size, comment ratio, naming conventions
        score = 1.0
        
        # Check function lengths
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_length = node.end_lineno - node.lineno
                if func_length > 50:
                    score -= 0.1
                elif func_length > 100:
                    score -= 0.2
        
        # Check class sizes
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_length = node.end_lineno - node.lineno
                if class_length > 200:
                    score -= 0.15
                elif class_length > 500:
                    score -= 0.3
        
        # Check naming conventions
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                if isinstance(node.ctx, ast.Store):
                    if not re.match(r'^[a-z_][a-z0-9_]*$', node.id):
                        score -= 0.05
        
        return max(0.0, min(1.0, score))
    
    def _calculate_reliability(self, tree: ast.AST, code: str) -> float:
        """Calculate reliability score (0-1)"""
        score = 1.0
        
        # Check for error handling
        try_blocks = [node for node in ast.walk(tree) if isinstance(node, ast.Try)]
        if not try_blocks:
            score -= 0.2
        
        # Check for bare excepts
        for node in try_blocks:
            for handler in node.handlers:
                if handler.type is None:
                    score -= 0.3
        
        # Check for resource management
        for node in ast.walk(tree):
            if isinstance(node, ast.With):
                score += 0.1
        
        return max(0.0, min(1.0, score))
    
    def _calculate_security_score(self, tree: ast.AST, code: str) -> float:
        """Calculate security score (0-1)"""
        score = 1.0
        
        # Check for dangerous patterns
        dangerous_patterns = [
            r'eval\s*\(',
            r'exec\s*\(',
            r'subprocess\.',
            r'os\.system\s*\(',
            r'pickle\.loads\s*\(',
            r'marshal\.loads\s*\('
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, code):
                score -= 0.2
        
        # Check for hardcoded credentials
        credential_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'api_key\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']'
        ]
        
        for pattern in credential_patterns:
            if re.search(pattern, code):
                score -= 0.3
        
        return max(0.0, min(1.0, score))
    
    def _detect_code_smells(self, tree: ast.AST, code: str) -> List[CodeIssue]:
        """Detect code smells and issues"""
        issues = []
        
        # Long methods
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                length = node.end_lineno - node.lineno
                if length > 50:
                    issues.append(CodeIssue(
                        type=CodeSmell.LONG_METHOD,
                        severity="medium",
                        description=f"Method {node.name} is too long ({length} lines)",
                        line_number=node.lineno,
                        suggestion="Consider breaking this method into smaller functions"
                    ))
        
        # Large classes
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                length = node.end_lineno - node.lineno
                if length > 200:
                    issues.append(CodeIssue(
                        type=CodeSmell.LARGE_CLASS,
                        severity="medium",
                        description=f"Class {node.name} is too large ({length} lines)",
                        line_number=node.lineno,
                        suggestion="Consider splitting this class into smaller classes"
                    ))
        
        # Magic numbers
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                if node.value not in [0, 1, -1, 0.0, 1.0, -1.0]:
                    issues.append(CodeIssue(
                        type=CodeSmell.MAGIC_NUMBER,
                        severity="low",
                        description=f"Magic number {node.value} found",
                        line_number=node.lineno,
                        suggestion="Consider using a named constant"
                    ))
        
        return issues
    
    def _calculate_comment_ratio(self, code: str) -> float:
        """Calculate comment to code ratio"""
        lines = code.split('\n')
        code_lines = 0
        comment_lines = 0
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                code_lines += 1
            elif line.startswith('#'):
                comment_lines += 1
        
        if code_lines == 0:
            return 0.0
        
        return comment_lines / (code_lines + comment_lines)
    
    def generate_code_improvements(self, code: str, language: str) -> Dict[str, Any]:
        """Generate code improvement suggestions"""
        metrics = self.analyze_code(code, language)
        
        improvements = {
            "original_metrics": {
                "complexity": metrics.complexity.value,
                "maintainability": metrics.maintainability,
                "reliability": metrics.reliability,
                "security_score": metrics.security_score
            },
            "suggested_improvements": []
        }
        
        # Generate suggestions based on metrics
        if metrics.maintainability < 0.7:
            improvements["suggested_improvements"].append({
                "type": "maintainability",
                "description": "Improve code maintainability",
                "suggestions": [
                    "Break down large functions into smaller ones",
                    "Use more descriptive variable names",
                    "Add comments to explain complex logic"
                ]
            })
        
        if metrics.reliability < 0.7:
            improvements["suggested_improvements"].append({
                "type": "reliability",
                "description": "Improve code reliability",
                "suggestions": [
                    "Add proper error handling",
                    "Use context managers for resource management",
                    "Add input validation"
                ]
            })
        
        if metrics.security_score < 0.8:
            improvements["suggested_improvements"].append({
                "type": "security",
                "description": "Improve code security",
                "suggestions": [
                    "Avoid using eval() or exec()",
                    "Use parameterized queries instead of string concatenation",
                    "Store sensitive data in environment variables"
                ]
            })
        
        # Add specific issue-based suggestions
        for issue in metrics.issues:
            improvements["suggested_improvements"].append({
                "type": issue.type.value,
                "description": issue.description,
                "suggestions": [issue.suggestion]
            })
        
        return improvements


=== core\analytics\dashboard.py ===
# core/analytics/dashboard.py
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from fastapi import APIRouter, HTTPException
from fastapi.responses import HTMLResponse
from pathlib import Path

class AnalyticsDashboard:
    def __init__(self, db_manager, redis_client):
        self.db_manager = db_manager
        self.redis_client = redis_client
        self.router = APIRouter(prefix="/analytics")
        self._setup_routes()
    
    def _setup_routes(self):
        @self.router.get("/dashboard", response_class=HTMLResponse)
        async def dashboard():
            """Main analytics dashboard"""
            return self._generate_dashboard_html()
        
        @self.router.get("/api/usage-stats")
        async def usage_stats():
            """Get usage statistics"""
            return await self._get_usage_statistics()
        
        @self.router.get("/api/performance-metrics")
        async def performance_metrics():
            """Get performance metrics"""
            return await self._get_performance_metrics()
        
        @self.router.get("/api/user-analytics")
        async def user_analytics():
            """Get user analytics"""
            return await self._get_user_analytics()
        
        @self.router.get("/api/code-quality-trends")
        async def code_quality_trends():
            """Get code quality trends"""
            return await self._get_code_quality_trends()
    
    def _generate_dashboard_html(self) -> str:
        """Generate dashboard HTML with embedded charts"""
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Open LLM Analytics Dashboard</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .dashboard-grid {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; }}
                .chart-container {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                .full-width {{ grid-column: 1 / -1; }}
                .metric-card {{ background: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 20px; }}
                .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
                .metric-label {{ color: #6c757d; }}
            </style>
        </head>
        <body>
            <h1>Open LLM Analytics Dashboard</h1>
            
            <div class="metric-cards">
                <div class="metric-card">
                    <div class="metric-label">Total Requests Today</div>
                    <div class="metric-value" id="total-requests">0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Active Users</div>
                    <div class="metric-value" id="active-users">0</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Avg Response Time</div>
                    <div class="metric-value" id="avg-response-time">0ms</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">Success Rate</div>
                    <div class="metric-value" id="success-rate">0%</div>
                </div>
            </div>
            
            <div class="dashboard-grid">
                <div class="chart-container">
                    <h3>Request Trends</h3>
                    <div id="request-trends-chart"></div>
                </div>
                
                <div class="chart-container">
                    <h3>Language Distribution</h3>
                    <div id="language-distribution-chart"></div>
                </div>
                
                <div class="chart-container">
                    <h3>Performance Metrics</h3>
                    <div id="performance-metrics-chart"></div>
                </div>
                
                <div class="chart-container">
                    <h3>User Activity</h3>
                    <div id="user-activity-chart"></div>
                </div>
            </div>
            
            <script>
                // Fetch data and render charts
                async function loadDashboardData() {{
                    try {{
                        // Load metrics
                        const metrics = await fetch('/analytics/api/usage-stats').then(r => r.json());
                        document.getElementById('total-requests').textContent = metrics.total_requests;
                        document.getElementById('active-users').textContent = metrics.active_users;
                        document.getElementById('avg-response-time').textContent = metrics.avg_response_time + 'ms';
                        document.getElementById('success-rate').textContent = metrics.success_rate + '%';
                        
                        // Load and render charts
                        await loadCharts();
                    }} catch (error) {{
                        console.error('Failed to load dashboard data:', error);
                    }}
                }}
                
                async function loadCharts() {{
                    // Request trends chart
                    const requestTrends = await fetch('/analytics/api/usage-stats').then(r => r.json());
                    const requestTrace = {{
                        x: requestTrends.hourly_requests.map(r => r.hour),
                        y: requestTrends.hourly_requests.map(r => r.count),
                        type: 'scatter',
                        mode: 'lines+markers',
                        name: 'Requests'
                    }};
                    
                    const requestLayout = {{
                        title: 'Request Trends (Last 24 Hours)',
                        xaxis: {{ title: 'Hour' }},
                        yaxis: {{ title: 'Requests' }}
                    }};
                    
                    Plotly.newPlot('request-trends-chart', [requestTrace], requestLayout);
                    
                    // Language distribution chart
                    const langData = await fetch('/analytics/api/code-quality-trends').then(r => r.json());
                    const langChart = {{
                        values: langData.language_distribution.map(d => d.count),
                        labels: langData.language_distribution.map(d => d.language),
                        type: 'pie'
                    }};
                    
                    const langLayout = {{
                        title: 'Language Distribution'
                    }};
                    
                    Plotly.newPlot('language-distribution-chart', [langChart], langLayout);
                    
                    // Performance metrics chart
                    const perfData = await fetch('/analytics/api/performance-metrics').then(r => r.json());
                    const perfTrace = {{
                        x: perfData.hourly_metrics.map(m => m.hour),
                        y: perfData.hourly_metrics.map(m => m.avg_latency),
                        type: 'bar',
                        name: 'Average Latency (ms)'
                    }};
                    
                    const perfLayout = {{
                        title: 'Performance Metrics',
                        xaxis: {{ title: 'Hour' }},
                        yaxis: {{ title: 'Latency (ms)' }}
                    }};
                    
                    Plotly.newPlot('performance-metrics-chart', [perfTrace], perfLayout);
                    
                    // User activity chart
                    const userData = await fetch('/analytics/api/user-analytics').then(r => r.json());
                    const userTrace = {{
                        x: userData.daily_activity.map(d => d.date),
                        y: userData.daily_activity.map(d => d.active_users),
                        type: 'scatter',
                        mode: 'lines+markers',
                        name: 'Active Users'
                    }};
                    
                    const userLayout = {{
                        title: 'User Activity (Last 30 Days)',
                        xaxis: {{ title: 'Date' }},
                        yaxis: {{ title: 'Active Users' }}
                    }};
                    
                    Plotly.newPlot('user-activity-chart', [userTrace], userLayout);
                }}
                
                // Load dashboard on page load
                loadDashboardData();
                
                // Refresh every 30 seconds
                setInterval(loadDashboardData, 30000);
            </script>
        </body>
        </html>
        """
    
    async def _get_usage_statistics(self) -> Dict[str, Any]:
        """Get usage statistics"""
        # Get data from cache or database
        cache_key = "usage_stats:today"
        cached_data = await self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Calculate statistics
        today = datetime.now().date()
        tomorrow = today + timedelta(days=1)
        
        async with self.db_manager.get_postgres_connection() as conn:
            # Total requests today
            total_requests = await conn.fetchval(
                "SELECT COUNT(*) FROM requests WHERE request_timestamp >= $1 AND request_timestamp < $2",
                today, tomorrow
            )
            
            # Hourly distribution
            hourly_requests = await conn.fetch(
                """
                SELECT 
                    EXTRACT(HOUR FROM request_timestamp) as hour,
                    COUNT(*) as count
                FROM requests 
                WHERE request_timestamp >= $1 AND request_timestamp < $2
                GROUP BY EXTRACT(HOUR FROM request_timestamp)
                ORDER BY hour
                """,
                today, tomorrow
            )
            
            # Active users today
            active_users = await conn.fetchval(
                "SELECT COUNT(DISTINCT user_id) FROM requests WHERE request_timestamp >= $1 AND request_timestamp < $2",
                today, tomorrow
            )
            
            # Average response time
            avg_response_time = await conn.fetchval(
                "SELECT AVG(EXTRACT(EPOCH FROM (response_timestamp - request_timestamp)) * 1000) FROM requests WHERE request_timestamp >= $1 AND request_timestamp < $2",
                today, tomorrow
            ) or 0
            
            # Success rate
            success_rate_result = await conn.fetch(
                """
                SELECT 
                    SUM(CASE WHEN status_code < 400 THEN 1 ELSE 0 END) as success,
                    COUNT(*) as total
                FROM requests 
                WHERE request_timestamp >= $1 AND request_timestamp < $2
                """,
                today, tomorrow
            )
            
            success_rate = (success_rate_result[0]['success'] / success_rate_result[0]['total'] * 100) if success_rate_result[0]['total'] > 0 else 0
        
        result = {
            "total_requests": total_requests,
            "active_users": active_users,
            "avg_response_time": round(avg_response_time, 2),
            "success_rate": round(success_rate, 2),
            "hourly_requests": [
                {"hour": int(row["hour"]), "count": row["count"]}
                for row in hourly_requests
            ]
        }
        
        # Cache for 5 minutes
        await self.redis_client.setex(cache_key, 300, json.dumps(result))
        
        return result
    
    async def _get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics"""
        cache_key = "performance_metrics:24h"
        cached_data = await self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Get last 24 hours of performance data
        start_time = datetime.now() - timedelta(hours=24)
        
        async with self.db_manager.get_postgres_connection() as conn:
            hourly_metrics = await conn.fetch(
                """
                SELECT 
                    EXTRACT(HOUR FROM request_timestamp) as hour,
                    AVG(EXTRACT(EPOCH FROM (response_timestamp - request_timestamp)) * 1000) as avg_latency,
                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (response_timestamp - request_timestamp) * 1000)) as p95_latency,
                    COUNT(*) as request_count
                FROM requests 
                WHERE request_timestamp >= $1
                GROUP BY EXTRACT(HOUR FROM request_timestamp)
                ORDER BY hour
                """,
                start_time
            )
        
        result = {
            "hourly_metrics": [
                {
                    "hour": int(row["hour"]),
                    "avg_latency": round(row["avg_latency"], 2),
                    "p95_latency": round(row["p95_latency"], 2),
                    "request_count": row["request_count"]
                }
                for row in hourly_metrics
            ]
        }
        
        # Cache for 10 minutes
        await self.redis_client.setex(cache_key, 600, json.dumps(result))
        
        return result
    
    async def _get_user_analytics(self) -> Dict[str, Any]:
        """Get user analytics"""
        cache_key = "user_analytics:30d"
        cached_data = await self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Get last 30 days of user activity
        start_time = datetime.now() - timedelta(days=30)
        
        async with self.db_manager.get_postgres_connection() as conn:
            daily_activity = await conn.fetch(
                """
                SELECT 
                    DATE(request_timestamp) as date,
                    COUNT(DISTINCT user_id) as active_users,
                    COUNT(*) as total_requests
                FROM requests 
                WHERE request_timestamp >= $1
                GROUP BY DATE(request_timestamp)
                ORDER BY date
                """,
                start_time
            )
            
            # Top users by request count
            top_users = await conn.fetch(
                """
                SELECT 
                    user_id,
                    COUNT(*) as request_count,
                    AVG(EXTRACT(EPOCH FROM (response_timestamp - request_timestamp)) * 1000) as avg_response_time
                FROM requests 
                WHERE request_timestamp >= $1
                GROUP BY user_id
                ORDER BY request_count DESC
                LIMIT 10
                """,
                start_time
            )
        
        result = {
            "daily_activity": [
                {
                    "date": row["date"].isoformat(),
                    "active_users": row["active_users"],
                    "total_requests": row["total_requests"]
                }
                for row in daily_activity
            ],
            "top_users": [
                {
                    "user_id": row["user_id"],
                    "request_count": row["request_count"],
                    "avg_response_time": round(row["avg_response_time"], 2)
                }
                for row in top_users
            ]
        }
        
        # Cache for 1 hour
        await self.redis_client.setex(cache_key, 3600, json.dumps(result))
        
        return result
    
    async def _get_code_quality_trends(self) -> Dict[str, Any]:
        """Get code quality trends"""
        cache_key = "code_quality_trends:7d"
        cached_data = await self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Get last 7 days of code quality data
        start_time = datetime.now() - timedelta(days=7)
        
        async with self.db_manager.get_postgres_connection() as conn:
            # Language distribution
            language_distribution = await conn.fetch(
                """
                SELECT 
                    metadata->>'language' as language,
                    COUNT(*) as count
                FROM requests 
                WHERE request_timestamp >= $1
                AND metadata->>'language' IS NOT NULL
                GROUP BY metadata->>'language'
                ORDER BY count DESC
                """,
                start_time
            )
            
            # Refactoring trends
            refactoring_trends = await conn.fetch(
                """
                SELECT 
                    DATE(request_timestamp) as date,
                    COUNT(*) as refactoring_count
                FROM requests 
                WHERE request_timestamp >= $1
                AND endpoint = '/refactor/analyze'
                GROUP BY DATE(request_timestamp)
                ORDER BY date
                """,
                start_time
            )
            
            # Quality metrics
            quality_metrics = await conn.fetch(
                """
                SELECT 
                    DATE(request_timestamp) as date,
                    AVG(CAST(metadata->>'quality_score' AS FLOAT)) as avg_quality_score,
                    COUNT(CASE WHEN metadata->>'validation_passed' = 'true' THEN 1 END) * 100.0 / COUNT(*) as success_rate
                FROM requests 
                WHERE request_timestamp >= $1
                AND metadata->>'quality_score' IS NOT NULL
                GROUP BY DATE(request_timestamp)
                ORDER BY date
                """,
                start_time
            )
        
        result = {
            "language_distribution": [
                {"language": row["language"], "count": row["count"]}
                for row in language_distribution
            ],
            "refactoring_trends": [
                {
                    "date": row["date"].isoformat(),
                    "refactoring_count": row["refactoring_count"]
                }
                for row in refactoring_trends
            ],
            "quality_metrics": [
                {
                    "date": row["date"].isoformat(),
                    "avg_quality_score": round(row["avg_quality_score"], 2),
                    "success_rate": round(row["success_rate"], 2)
                }
                for row in quality_metrics
            ]
        }
        
        # Cache for 1 hour
        await self.redis_client.setex(cache_key, 3600, json.dumps(result))
        
        return result


=== core\collaboration\session_manager.py ===
# core/collaboration/session_manager.py
import asyncio
import json
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum

class SessionRole(Enum):
    OWNER = "owner"
    EDITOR = "editor"
    VIEWER = "viewer"

class Permission(Enum):
    READ = "read"
    WRITE = "write"
    SHARE = "share"

@dataclass
class Collaborator:
    id: str
    name: str
    role: SessionRole
    permissions: List[Permission]
    joined_at: datetime
    last_active: datetime

@dataclass
class Session:
    id: str
    name: str
    owner_id: str
    code: str
    language: str
    collaborators: Dict[str, Collaborator]
    created_at: datetime
    last_modified: datetime
    is_public: bool

class CollaborationManager:
    def __init__(self):
        self.sessions: Dict[str, Session] = {}
        self.user_sessions: Dict[str, List[str]] = {}  # user_id -> session_ids
        self.websocket_connections: Dict[str, Any] = {}  # session_id -> websocket connections
    
    async def create_session(self, owner_id: str, name: str, code: str, language: str, is_public: bool = False) -> Session:
        """Create a new collaboration session"""
        session_id = str(uuid.uuid4())
        session = Session(
            id=session_id,
            name=name,
            owner_id=owner_id,
            code=code,
            language=language,
            collaborators={},
            created_at=datetime.now(),
            last_modified=datetime.now(),
            is_public=is_public
        )
        
        # Add owner as collaborator
        owner_collaborator = Collaborator(
            id=owner_id,
            name="Owner",
            role=SessionRole.OWNER,
            permissions=list(Permission),
            joined_at=datetime.now(),
            last_active=datetime.now()
        )
        session.collaborators[owner_id] = owner_collaborator
        
        self.sessions[session_id] = session
        
        # Update user sessions
        if owner_id not in self.user_sessions:
            self.user_sessions[owner_id] = []
        self.user_sessions[owner_id].append(session_id)
        
        return session
    
    async def join_session(self, session_id: str, user_id: str, user_name: str) -> Optional[Session]:
        """Join an existing collaboration session"""
        if session_id not in self.sessions:
            return None
        
        session = self.sessions[session_id]
        
        # Check if user is already a collaborator
        if user_id not in session.collaborators:
            # Add as viewer by default
            collaborator = Collaborator(
                id=user_id,
                name=user_name,
                role=SessionRole.VIEWER,
                permissions=[Permission.READ],
                joined_at=datetime.now(),
                last_active=datetime.now()
            )
            session.collaborators[user_id] = collaborator
        
        # Update user sessions
        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = []
        if session_id not in self.user_sessions[user_id]:
            self.user_sessions[user_id].append(session_id)
        
        return session
    
    async def leave_session(self, session_id: str, user_id: str) -> bool:
        """Leave a collaboration session"""
        if session_id not in self.sessions:
            return False
        
        session = self.sessions[session_id]
        
        # Remove collaborator (but keep owner)
        if user_id != session.owner_id and user_id in session.collaborators:
            del session.collaborators[user_id]
        
        # Update user sessions
        if user_id in self.user_sessions and session_id in self.user_sessions[user_id]:
            self.user_sessions[user_id].remove(session_id)
        
        # Delete session if no collaborators left
        if len(session.collaborators) <= 1:
            del self.sessions[session_id]
        
        return True
    
    async def update_code(self, session_id: str, user_id: str, code: str, cursor_position: int = None) -> bool:
        """Update code in a collaboration session"""
        if session_id not in self.sessions:
            return False
        
        session = self.sessions[session_id]
        collaborator = session.collaborators.get(user_id)
        
        # Check permissions
        if not collaborator or Permission.WRITE not in collaborator.permissions:
            return False
        
        # Update code
        session.code = code
        session.last_modified = datetime.now()
        collaborator.last_active = datetime.now()
        
        # Broadcast to other collaborators
        await self._broadcast_code_update(session_id, user_id, code, cursor_position)
        
        return True
    
    async def _broadcast_code_update(self, session_id: str, user_id: str, code: str, cursor_position: int = None):
        """Broadcast code update to all collaborators in the session"""
        if session_id not in self.websocket_connections:
            return
        
        message = {
            "type": "code_update",
            "user_id": user_id,
            "code": code,
            "cursor_position": cursor_position,
            "timestamp": datetime.now().isoformat()
        }
        
        # Send to all connected websockets except the sender
        for connection in self.websocket_connections[session_id]:
            if connection.user_id != user_id:
                await connection.send_json(message)
    
    async def add_websocket_connection(self, session_id: str, user_id: str, websocket):
        """Add a websocket connection to a session"""
        if session_id not in self.websocket_connections:
            self.websocket_connections[session_id] = []
        
        self.websocket_connections[session_id].append(websocket)
        
        # Send current state to the new connection
        session = self.sessions.get(session_id)
        if session:
            await websocket.send_json({
                "type": "session_state",
                "session": asdict(session),
                "timestamp": datetime.now().isoformat()
            })
    
    async def remove_websocket_connection(self, session_id: str, websocket):
        """Remove a websocket connection from a session"""
        if session_id in self.websocket_connections:
            if websocket in self.websocket_connections[session_id]:
                self.websocket_connections[session_id].remove(websocket)
    
    async def get_user_sessions(self, user_id: str) -> List[Session]:
        """Get all sessions for a user"""
        session_ids = self.user_sessions.get(user_id, [])
        return [self.sessions[session_id] for session_id in session_ids if session_id in self.sessions]
    
    async def get_public_sessions(self, limit: int = 50) -> List[Session]:
        """Get public sessions"""
        public_sessions = [session for session in self.sessions.values() if session.is_public]
        return sorted(public_sessions, key=lambda x: x.last_modified, reverse=True)[:limit]


=== core\completion.py ===
from transformers import pipeline
from typing import List, Dict
from shared.schemas import CompletionRequest

class CodeCompleter:
    def __init__(self, model_name="deepseek-coder-6.7b"):
        self.completion_pipeline = pipeline(
            "text-generation",
            model=model_name,
            device="cuda"  # Use "cpu" if no GPU
        )

    def generate_completions(self, request: CompletionRequest) -> Dict[str, List[str]]:
        """Generate code suggestions with context awareness"""
        prompt = self._build_prompt(request.context, request.cursor_context)
        outputs = self.completion_pipeline(
            prompt,
            num_return_sequences=3,
            max_new_tokens=50,
            temperature=0.7,
            stop_sequences=["\n\n"]
        )
        return {"completions": [o["generated_text"] for o in outputs]}

    def _build_prompt(self, context: str, cursor_context: str) -> str:
        """Structured prompt for code completion"""
        return f"""# Code Context:
{context}

# Cursor Position:
{cursor_context}

# Suggested Completion:"""


=== core\completion\intelligent_completer.py ===
# core/completion/intelligent_completer.py
import ast
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class CompletionContext:
    file_path: str
    language: str
    imports: List[str]
    variables: Dict[str, str]
    functions: List[str]
    classes: List[str]
    current_scope: str
    cursor_position: Tuple[int, int]

@dataclass
class CompletionSuggestion:
    text: str
    type: str  # "variable", "function", "class", "method", "keyword", "import"
    description: str
    confidence: float
    context: str

class IntelligentCodeCompleter:
    def __init__(self):
        self.context_cache = {}
        self.completion_history = defaultdict(list)
        self.vectorizer = TfidfVectorizer(max_features=5000)
        self.completion_embeddings = None
        self.language_patterns = self._load_language_patterns()
    
    def get_completions(self, code: str, cursor_pos: int, file_path: str, language: str) -> List[CompletionSuggestion]:
        """Get intelligent code completions"""
        # Parse and analyze context
        context = self._analyze_context(code, cursor_pos, file_path, language)
        
        # Generate different types of completions
        completions = []
        
        # Local variable completions
        completions.extend(self._get_variable_completions(context))
        
        # Function/method completions
        completions.extend(self._get_function_completions(context))
        
        # Class completions
        completions.extend(self._get_class_completions(context))
        
        # Keyword completions
        completions.extend(self._get_keyword_completions(context))
        
        # Import completions
        completions.extend(self._get_import_completions(context))
        
        # Context-aware completions
        completions.extend(self._get_context_aware_completions(context))
        
        # Sort by confidence and return top suggestions
        completions.sort(key=lambda x: x.confidence, reverse=True)
        return completions[:10]  # Return top 10 completions
    
    def _analyze_context(self, code: str, cursor_pos: int, file_path: str, language: str) -> CompletionContext:
        """Analyze code context for intelligent completions"""
        # Get current line and surrounding context
        lines = code.split('\n')
        current_line_idx = cursor_pos // (len(code) // len(lines)) if len(lines) > 0 else 0
        current_line = lines[current_line_idx]
        
        # Extract imports
        imports = self._extract_imports(code, language)
        
        # Extract variables, functions, and classes
        variables = self._extract_variables(code, language)
        functions = self._extract_functions(code, language)
        classes = self._extract_classes(code, language)
        
        # Determine current scope
        current_scope = self._determine_current_scope(code, cursor_pos, language)
        
        return CompletionContext(
            file_path=file_path,
            language=language,
            imports=imports,
            variables=variables,
            functions=functions,
            classes=classes,
            current_scope=current_scope,
            cursor_position=(current_line_idx, cursor_pos % len(lines[current_line_idx]) if current_line_idx < len(lines) else 0)
        )
    
    def _extract_imports(self, code: str, language: str) -> List[str]:
        """Extract import statements"""
        imports = []
        
        if language == "python":
            import_pattern = r'import\s+([a-zA-Z_][a-zA-Z0-9_]*)'
            from_pattern = r'from\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+import'
            
            imports.extend(re.findall(import_pattern, code))
            imports.extend(re.findall(from_pattern, code))
        
        elif language == "javascript":
            import_pattern = r'import\s+.*?from\s+[\'"]([^\'"]+)[\'"]'
            require_pattern = r'require\([\'"]([^\'"]+)[\'"]\)'
            
            imports.extend(re.findall(import_pattern, code))
            imports.extend(re.findall(require_pattern, code))
        
        return imports
    
    def _extract_variables(self, code: str, language: str) -> Dict[str, str]:
        """Extract variable declarations"""
        variables = {}
        
        if language == "python":
            # Variable assignments
            var_pattern = r'([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*([^#\n]+)'
            for match in re.finditer(var_pattern, code):
                var_name = match.group(1)
                var_value = match.group(2).strip()
                variables[var_name] = var_value
        
        elif language == "javascript":
            # Variable declarations
            var_patterns = [
                r'var\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*([^;\n]+)',
                r'let\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*([^;\n]+)',
                r'const\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*([^;\n]+)'
            ]
            
            for pattern in var_patterns:
                for match in re.finditer(pattern, code):
                    var_name = match.group(1)
                    var_value = match.group(2).strip()
                    variables[var_name] = var_value
        
        return variables
    
    def _extract_functions(self, code: str, language: str) -> List[str]:
        """Extract function definitions"""
        functions = []
        
        if language == "python":
            func_pattern = r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
            functions.extend(re.findall(func_pattern, code))
        
        elif language == "javascript":
            func_patterns = [
                r'function\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(',
                r'([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*function\s*\(',
                r'([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*\([^)]*\)\s*=>'
            ]
            
            for pattern in func_patterns:
                functions.extend(re.findall(pattern, code))
        
        return functions
    
    def _extract_classes(self, code: str, language: str) -> List[str]:
        """Extract class definitions"""
        classes = []
        
        if language == "python":
            class_pattern = r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)'
            classes.extend(re.findall(class_pattern, code))
        
        elif language == "javascript":
            class_pattern = r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)'
            classes.extend(re.findall(class_pattern, code))
        
        return classes
    
    def _determine_current_scope(self, code: str, cursor_pos: int, language: str) -> str:
        """Determine the current scope at cursor position"""
        # Simplified scope detection
        lines = code.split('\n')
        current_line_idx = cursor_pos // (len(code) // len(lines)) if len(lines) > 0 else 0
        
        # Look for class or function definitions before cursor
        for i in range(current_line_idx, -1, -1):
            line = lines[i].strip()
            
            if language == "python":
                if line.startswith('class '):
                    return f"class:{line.split()[1]}"
                elif line.startswith('def '):
                    return f"function:{line.split()[1]}"
            
            elif language == "javascript":
                if line.startswith('class '):
                    return f"class:{line.split()[1]}"
                elif line.startswith('function '):
                    return f"function:{line.split()[1]}"
                elif '=' in line and 'function' in line:
                    func_name = line.split('=')[0].strip()
                    return f"function:{func_name}"
        
        return "global"
    
    def _get_variable_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get variable completions"""
        completions = []
        
        for var_name, var_value in context.variables.items():
            # Only suggest variables that are accessible in current scope
            if self._is_accessible(var_name, context):
                completions.append(CompletionSuggestion(
                    text=var_name,
                    type="variable",
                    description=f"Variable: {var_value[:50]}...",
                    confidence=0.9,
                    context=context.current_scope
                ))
        
        return completions
    
    def _get_function_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get function completions"""
        completions = []
        
        for func_name in context.functions:
            completions.append(CompletionSuggestion(
                text=func_name,
                type="function",
                description=f"Function: {func_name}",
                confidence=0.8,
                context=context.current_scope
            ))
        
        return completions
    
    def _get_class_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get class completions"""
        completions = []
        
        for class_name in context.classes:
            completions.append(CompletionSuggestion(
                text=class_name,
                type="class",
                description=f"Class: {class_name}",
                confidence=0.8,
                context=context.current_scope
            ))
        
        return completions
    
    def _get_keyword_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get keyword completions"""
        completions = []
        
        if context.language == "python":
            keywords = [
                "def", "class", "if", "else", "elif", "for", "while", "try", "except",
                "finally", "with", "import", "from", "as", "return", "yield", "raise",
                "assert", "del", "pass", "break", "continue", "global", "nonlocal", "lambda",
                "and", "or", "not", "in", "is", "True", "False", "None"
            ]
        elif context.language == "javascript":
            keywords = [
                "function", "class", "if", "else", "for", "while", "do", "switch", "case",
                "break", "continue", "return", "yield", "await", "async", "try", "catch",
                "finally", "throw", "new", "this", "super", "import", "export", "default",
                "const", "let", "var", "true", "false", "null", "undefined", "typeof", "instanceof"
            ]
        
        for keyword in keywords:
            completions.append(CompletionSuggestion(
                text=keyword,
                type="keyword",
                description=f"Keyword: {keyword}",
                confidence=0.7,
                context=context.current_scope
            ))
        
        return completions
    
    def _get_import_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get import completions"""
        completions = []
        
        # Get current line to check if we're in an import statement
        lines = context.file_path.split('\n')
        current_line_idx = context.cursor_position[0]
        current_line = lines[current_line_idx] if current_line_idx < len(lines) else ""
        
        if context.language == "python":
            if current_line.strip().startswith(('import ', 'from ')):
                # Suggest common Python modules
                common_modules = [
                    "os", "sys", "json", "datetime", "math", "random", "re", "collections",
                    "itertools", "functools", "pathlib", "typing", "asyncio", "threading"
                ]
                
                for module in common_modules:
                    if module not in context.imports:
                        completions.append(CompletionSuggestion(
                            text=module,
                            type="import",
                            description=f"Module: {module}",
                            confidence=0.6,
                            context=context.current_scope
                        ))
        
        return completions
    
    def _get_context_aware_completions(self, context: CompletionContext) -> List[CompletionSuggestion]:
        """Get context-aware completions based on code patterns"""
        completions = []
        
        # Get current line content
        lines = context.file_path.split('\n')
        current_line_idx = context.cursor_position[0]
        current_line = lines[current_line_idx] if current_line_idx < len(lines) else ""
        
        # Pattern-based completions
        if context.language == "python":
            # For loop pattern
            if re.search(r'for\s+\w+\s+in', current_line):
                completions.append(CompletionSuggestion(
                    text="range(",
                    type="pattern",
                    description="For loop with range",
                    confidence=0.8,
                    context=context.current_scope
                ))
                completions.append(CompletionSuggestion(
                    text="enumerate(",
                    type="pattern",
                    description="For loop with enumerate",
                    confidence=0.8,
                    context=context.current_scope
                ))
            
            # List comprehension pattern
            if re.search(r'\[\s*\w+\s+for', current_line):
                completions.append(CompletionSuggestion(
                    text="if ",
                    type="pattern",
                    description="List comprehension condition",
                    confidence=0.7,
                    context=context.current_scope
                ))
            
            # Method call pattern
            if re.search(r'\.\w+\s*$', current_line):
                completions.append(CompletionSuggestion(
                    text="(",
                    type="pattern",
                    description="Method call with arguments",
                    confidence=0.9,
                    context=context.current_scope
                ))
        
        return completions
    
    def _is_accessible(self, var_name: str, context: CompletionContext) -> bool:
        """Check if a variable is accessible in the current scope"""
        # Simplified accessibility check
        # In practice, this would involve proper scope analysis
        return True
    
    def learn_from_user_selection(self, completion: CompletionSuggestion, context: CompletionContext):
        """Learn from user's completion selections"""
        # Store completion selection for future learning
        self.completion_history[context.language].append({
            "context": context,
            "completion": completion,
            "timestamp": datetime.now()
        })
        
        # Update completion model periodically
        if len(self.completion_history[context.language]) > 100:
            self._update_completion_model(context.language)
    
    def _update_completion_model(self, language: str):
        """Update completion model based on user feedback"""
        # Extract features from completion history
        contexts = []
        completions = []
        
        for entry in self.completion_history[language][-100:]:  # Last 100 entries
            contexts.append(entry["context"])
            completions.append(entry["completion"])
        
        # Update vectorizer and embeddings
        if contexts:
            context_texts = [self._context_to_text(ctx) for ctx in contexts]
            self.vectorizer.fit(context_texts)
            self.completion_embeddings = self.vectorizer.transform(context_texts)
    
    def _context_to_text(self, context: CompletionContext) -> str:
        """Convert context to text representation"""
        return f"{context.current_scope} {' '.join(context.variables.keys())} {' '.join(context.functions)} {' '.join(context.classes)}"


=== core\context.py ===
from shared.knowledge.graph import KnowledgeGraph
from shared.schemas import Query, Response
from typing import Dict, Any, List, Optional
import numpy as np
import hashlib
from datetime import datetime

class ContextManager:
    def __init__(self):
        self.graph = KnowledgeGraph()
        self._setup_foundational_knowledge()
        self.interaction_log = []
        self.routing_history = []  # NEW: Track routing decisions
        
    def _setup_foundational_knowledge(self):
        """Initialize with programming fundamentals"""
        foundations = [
            ("variable", "named storage location", ["storage", "memory"]),
            ("function", "reusable code block", ["abstraction", "parameters"]),
            ("loop", "iteration construct", ["repetition", "termination"]),
            ("class", "object blueprint", ["inheritance", "encapsulation"])
        ]
        
        for concept, desc, tags in foundations:
            node_id = self.graph.add_entity(
                content=concept,
                type="concept",
                metadata={
                    "description": desc,
                    "tags": tags,
                    "source": "system"
                }
            )
    
    # MODIFIED: Enhanced with routing metadata        
    def process_interaction(
        self, 
        query: Query, 
        response: Response,
        metadata: Optional[Dict] = None
    ):
        """
        Learn from user interactions with routing context
        Args:
            metadata: {
                "sla_tier": str,       # critical/standard/economy
                "reasoning_source": str, # graph/rule/llm
                "provider": str         # gpt-4/llama2/etc
            }
        """
        # Generate unique interaction ID
        interaction_id = hashlib.sha256(
            f"{datetime.now().isoformat()}:{query.content}".encode()
        ).hexdigest()
        
        # Enhanced logging (NEW)
        log_entry = {
            "id": interaction_id,
            "query": query.content,
            "response": response.content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        self.interaction_log.append(log_entry)
        
        # Track routing decisions separately (NEW)
        if metadata:
            self.routing_history.append({
                "timestamp": datetime.now().isoformat(),
                "query_hash": hashlib.sha256(query.content.encode()).hexdigest()[:8],
                **metadata
            })
        
        # Extract knowledge from both query and response
        self.graph.expand_from_text(
            query.content, 
            source="query",
            metadata={"sla_tier": metadata.get("sla_tier")} if metadata else None  # NEW
        )
        
        self.graph.expand_from_text(
            response.content,
            source="response",
            metadata={"provider": metadata.get("provider")} if metadata else None  # NEW
        )
        
        # Create relationship between query and response concepts
        query_nodes = self._extract_key_nodes(query.content)
        response_nodes = self._extract_key_nodes(response.content)
        
        for q_node in query_nodes:
            for r_node in response_nodes:
                self.graph.add_relation(
                    q_node, 
                    r_node, 
                    "elicits",
                    metadata=metadata  # NEW: Attach routing info to edges
                )
    
    # NEW METHOD
    def get_routing_context(self, query_content: str) -> Dict[str, Any]:
        """
        Get context specifically for routing decisions
        Returns:
            {
                "is_production": bool,
                "similar_past_queries": List[Dict],
                "preferred_llm": Optional[str],
                "complexity_score": float
            }
        """
        # Existing semantic matching
        matches = self.graph.find_semantic_matches(query_content)
        
        # NEW: Calculate complexity
        complexity = min(len(query_content.split()) / 10, 1.0)  # 0-1 scale
        
        return {
            "is_production": any(
                "production" in node["content"].lower() 
                for node in matches[:3]
            ),
            "similar_past_queries": [
                {
                    "query": self.graph.graph.nodes[m["node_id"]]["content"],
                    "source": self.graph.graph.nodes[m["node_id"]].get("source"),
                    "success": self._get_interaction_success(m["node_id"])
                }
                for m in matches[:3]
            ],
            "complexity_score": complexity,
            "preferred_llm": self._detect_preferred_provider(query_content)
        }
    
    # NEW HELPER METHODS
    def _get_interaction_success(self, node_id: str) -> bool:
        """Check if previous interactions with this node were successful"""
        edges = list(self.graph.graph.edges(node_id, data=True))
        return any(
            e[2].get("metadata", {}).get("success", True)
            for e in edges
        )
    
    def _detect_preferred_provider(self, query: str) -> Optional[str]:
        """Detect if query suggests a preferred provider"""
        query_lower = query.lower()
        if "openai" in query_lower:
            return "gpt-4"
        elif "local" in query_lower:
            return "llama2"
        return None
    
    # EXISTING METHODS (unchanged)
    def _extract_key_nodes(self, text: str) -> List[str]:
        """Identify most important nodes in text"""
        matches = self.graph.find_semantic_matches(text)
        return [m["node_id"] for m in matches[:3]]  # Top 3 matches
        
    def get_context(self, text: str) -> Dict[str, Any]:
        """Get relevant context for given text"""
        matches = self.graph.find_semantic_matches(text)
        context_nodes = set()
        
        # Get related nodes for each match
        for match in matches[:5]:  # Top 5 matches
            neighbors = list(self.graph.graph.neighbors(match["node_id"]))
            context_nodes.update(neighbors)
            
        return {
            "matches": matches,
            "related": [
                {"id": n, **self.graph.graph.nodes[n]}
                for n in context_nodes
            ]
        }


=== core\database\optimized_manager.py ===
# core/database/optimized_manager.py
import asyncio
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import asyncpg
from aioredis import Redis
from contextlib import asynccontextmanager

class OptimizedDatabaseManager:
    def __init__(self, postgres_url: str, redis_url: str):
        self.postgres_url = postgres_url
        self.redis_url = redis_url
        self.postgres_pool = None
        self.redis_client = None
        self.query_cache = {}
        self.cache_ttl = 3600  # 1 hour
    
    async def initialize(self):
        """Initialize database connections"""
        # Initialize PostgreSQL connection pool
        self.postgres_pool = await asyncpg.create_pool(
            self.postgres_url,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
        
        # Initialize Redis client
        self.redis_client = Redis.from_url(self.redis_url)
    
    @asynccontextmanager
    async def get_postgres_connection(self):
        """Get a PostgreSQL connection from the pool"""
        async with self.postgres_pool.acquire() as connection:
            yield connection
    
    async def cache_query_result(self, cache_key: str, result: Any, ttl: int = None):
        """Cache query result in Redis"""
        if ttl is None:
            ttl = self.cache_ttl
        
        await self.redis_client.setex(
            f"query_cache:{cache_key}",
            ttl,
            json.dumps(result, default=str)
        )
    
    async def get_cached_query(self, cache_key: str) -> Optional[Any]:
        """Get cached query result from Redis"""
        cached = await self.redis_client.get(f"query_cache:{cache_key}")
        if cached:
            return json.loads(cached)
        return None
    
    async def store_events_batch_optimized(self, events: List[Dict], batch_size: int = 100):
        """Optimized batch storage of events"""
        if not events:
            return
        
        # Process in batches
        for i in range(0, len(events), batch_size):
            batch = events[i:i + batch_size]
            await self._insert_events_batch(batch)
    
    async def _insert_events_batch(self, events: List[Dict]):
        """Insert a batch of events efficiently"""
        async with self.get_postgres_connection() as conn:
            # Use COPY for bulk insert
            await conn.executemany(
                """
                INSERT INTO events (event_id, event_type, timestamp, data)
                VALUES ($1, $2, $3, $4)
                """,
                [
                    (
                        event.get("event_id"),
                        event.get("event_type"),
                        event.get("timestamp", datetime.now()),
                        json.dumps(event.get("data", {}))
                    )
                    for event in events
                ]
            )
    
    async def get_analytics_data_optimized(self, time_range: timedelta) -> Dict[str, Any]:
        """Get analytics data with caching and optimization"""
        cache_key = f"analytics:{time_range.total_seconds()}"
        
        # Try cache first
        cached_result = await self.get_cached_query(cache_key)
        if cached_result:
            return cached_result
        
        # If not in cache, query database
        end_time = datetime.now()
        start_time = end_time - time_range
        
        async with self.get_postgres_connection() as conn:
            # Get event counts by type
            event_counts = await conn.fetch(
                """
                SELECT event_type, COUNT(*) as count
                FROM events
                WHERE timestamp BETWEEN $1 AND $2
                GROUP BY event_type
                """,
                start_time, end_time
            )
            
            # Get performance metrics
            performance_metrics = await conn.fetch(
                """
                SELECT 
                    AVG(EXTRACT(EPOCH FROM (response_timestamp - request_timestamp))) as avg_latency,
                    COUNT(*) as total_requests
                FROM requests
                WHERE request_timestamp BETWEEN $1 AND $2
                """,
                start_time, end_time
            )
        
        result = {
            "event_counts": dict(event_counts),
            "performance_metrics": dict(performance_metrics[0]) if performance_metrics else {},
            "time_range": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat()
            }
        }
        
        # Cache the result
        await self.cache_query_result(cache_key, result)
        
        return result
    
    async def create_indexes_optimized(self):
        """Create optimized database indexes"""
        async with self.get_postgres_connection() as conn:
            # Event indexes
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_events_timestamp 
                ON events(timestamp)
            """)
            
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_events_type_timestamp 
                ON events(event_type, timestamp)
            """)
            
            # Request indexes
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_requests_timestamp 
                ON requests(request_timestamp)
            """)
            
            # User session indexes
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_sessions_user_id 
                ON sessions(user_id)
            """)
            
            # Create partial indexes for better performance
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_events_recent 
                ON events(timestamp) 
                WHERE timestamp > NOW() - INTERVAL '7 days'
            """)
    
    async def cleanup_old_data(self, retention_days: int = 30):
        """Clean up old data to maintain performance"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        async with self.get_postgres_connection() as conn:
            # Delete old events
            deleted_events = await conn.execute(
                "DELETE FROM events WHERE timestamp < $1",
                cutoff_date
            )
            
            # Delete old requests
            deleted_requests = await conn.execute(
                "DELETE FROM requests WHERE request_timestamp < $1",
                cutoff_date
            )
            
            # Optimize tables after deletion
            await conn.execute("VACUUM ANALYZE events")
            await conn.execute("VACUUM ANALYZE requests")
        
        return {
            "deleted_events": deleted_events,
            "deleted_requests": deleted_requests,
            "cutoff_date": cutoff_date.isoformat()
        }


=== core\debugger.py ===
from typing import Dict, List
from dataclasses import dataclass
import ast
import traceback

@dataclass
class DebugFrame:
    file: str
    line: int
    context: str
    variables: Dict[str, str]
    error: str

class CodeDebugger:
    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph
        self.error_patterns = self._load_error_patterns()

    def analyze_traceback(self, code: str, error: str) -> List[DebugFrame]:
        """Convert traceback into structured debug frames"""
        frames = []
        tb = traceback.extract_tb(error.__traceback__)
        
        for frame in tb:
            context = self._get_code_context(code, frame.lineno)
            frames.append(DebugFrame(
                file=frame.filename,
                line=frame.lineno,
                context=context,
                variables=self._extract_variables(frame.locals),
                error=str(error)
            ))
        
        return frames

    def suggest_fixes(self, frames: List[DebugFrame]) -> Dict[str, List[str]]:
        """Generate fix suggestions using knowledge graph"""
        suggestions = {}
        for frame in frames:
            error_key = self._match_error_pattern(frame.error)
            if error_key in self.error_patterns:
                suggestions[frame.line] = self._enhance_suggestions(
                    self.error_patterns[error_key],
                    frame.context
                )
        return suggestions

    def _get_code_context(self, code: str, line: int, window=3) -> str:
        lines = code.split('\n')
        start = max(0, line - window - 1)
        end = min(len(lines), line + window)
        return '\n'.join(lines[start:end])

    def _extract_variables(self, locals_dict: Dict) -> Dict[str, str]:
        return {k: repr(v)[:100] for k, v in locals_dict.items() if not k.startswith('_')}

    def _match_error_pattern(self, error: str) -> str:
        for pattern in self.error_patterns:
            if pattern in error:
                return pattern
        return "unknown"

    def _enhance_suggestions(self, base_suggestions: List[str], context: str) -> List[str]:
        enhanced = []
        for suggestion in base_suggestions:
            # Augment with knowledge graph matches
            related = self.kg.find_semantic_matches(context + " " + suggestion)[:2]
            if related:
                enhanced.append(f"{suggestion} (Related: {', '.join(r['content'] for r in related)})")
            else:
                enhanced.append(suggestion)
        return enhanced

    def _load_error_patterns(self) -> Dict[str, List[str]]:
        return {
            "IndexError": [
                "Check list length before accessing",
                "Verify array bounds"
            ],
            "KeyError": [
                "Check if key exists in dictionary",
                "Use dict.get() for safe access"
            ],
            # ... other patterns
        }


=== core\errors\handlers.py ===
# core/errors/handlers.py
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException
import logging
import traceback
from typing import Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class ErrorResponse:
    @staticmethod
    def create_error_response(
        status_code: int,
        error_type: str,
        message: str,
        details: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        return {
            "error": {
                "type": error_type,
                "message": message,
                "details": details or {},
                "timestamp": datetime.utcnow().isoformat()
            }
        }

async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handle request validation errors"""
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content=ErrorResponse.create_error_response(
            status_code=422,
            error_type="validation_error",
            message="Request validation failed",
            details={"errors": exc.errors()}
        )
    )

async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions"""
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse.create_error_response(
            status_code=exc.status_code,
            error_type="http_error",
            message=exc.detail,
            details={"headers": dict(exc.headers)}
        )
    )

async def general_exception_handler(request: Request, exc: Exception):
    """Handle all other exceptions"""
    logger.error(f"Unhandled exception: {str(exc)}")
    logger.error(traceback.format_exc())
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content=ErrorResponse.create_error_response(
            status_code=500,
            error_type="internal_error",
            message="An unexpected error occurred",
            details={"trace_id": str(id(request))}
        )
    )

# core/errors/resilience.py
import asyncio
from functools import wraps
from typing import Callable, Any, Dict, Optional
import time

class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=30):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open
    
    def call(self, func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if self.state == "open":
                if time.time() - self.last_failure_time > self.recovery_timeout:
                    self.state = "half-open"
                else:
                    raise Exception("Circuit breaker is open")
            
            try:
                result = await func(*args, **kwargs)
                if self.state == "half-open":
                    self.state = "closed"
                    self.failure_count = 0
                return result
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.failure_threshold:
                    self.state = "open"
                
                raise e
        
        return wrapper

class RetryHandler:
    def __init__(self, max_retries=3, backoff_factor=1):
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor
    
    def retry(self, exceptions: tuple = (Exception,)):
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args, **kwargs):
                last_exception = None
                
                for attempt in range(self.max_retries + 1):
                    try:
                        return await func(*args, **kwargs)
                    except exceptions as e:
                        last_exception = e
                        
                        if attempt < self.max_retries:
                            delay = self.backoff_factor * (2 ** attempt)
                            await asyncio.sleep(delay)
                        else:
                            break
                
                raise last_exception
            
            return wrapper
        return decorator


=== core\feedback\processor.py ===
from datetime import datetime
from typing import Dict, Any
import numpy as np
from shared.knowledge.graph import KnowledgeGraph

class FeedbackProcessor:
    def __init__(self, context_manager):
        self.context = context_manager
        self.feedback_weights = {
            'explicit_rating': 0.7,
            'implicit_engagement': 0.3,
            'correction': 1.0
        }

    def process_feedback(self, feedback: Dict[str, Any]):
        """Handle both explicit and implicit feedback"""
        # Store raw feedback
        self._log_feedback(feedback)

        # Update knowledge graph
        if feedback['type'] == 'correction':
            self._apply_correction(feedback)
        else:
            self._update_edge_weights(feedback)

    def _apply_correction(self, feedback):
        """Direct knowledge corrections"""
        self.context.graph.update_node(
            node_id=feedback['target_node'],
            new_content=feedback['corrected_info'],
            metadata={'last_corrected': datetime.now()}
        )

    def _update_edge_weights(self, feedback):
        """Adjust relationship strengths"""
        current_weight = self.context.graph.get_edge_weight(
            feedback['query_node'],
            feedback['response_node']
        )
        
        new_weight = current_weight * (1 + self._calculate_feedback_impact(feedback))
        self.context.graph.update_edge(
            source=feedback['query_node'],
            target=feedback['response_node'],
            weight=min(new_weight, 1.0)  # Cap at 1.0
        )

    def _calculate_feedback_impact(self, feedback) -> float:
        """Calculate weighted feedback impact"""
        base_score = (
            feedback.get('rating', 0.5) * self.feedback_weights['explicit_rating'] +
            feedback.get('engagement', 0.2) * self.feedback_weights['implicit_engagement']
        )
        return base_score * (2 if feedback['type'] == 'positive' else -1)


=== core\health.py ===
from typing import Dict
import requests

class HealthChecker:
    @staticmethod
    def check_endpoint(url: str) -> Dict[str, bool]:
        try:
            resp = requests.get(f"{url}/health", timeout=3)
            return {
                "online": resp.status_code == 200,
                "models_loaded": resp.json().get("models_loaded", 0)
            }
        except:
            return {"online": False}
            
    def check_ollama(base_url="http://localhost:11434"):
        try:
            resp = requests.get(f"{base_url}/api/tags", timeout=3)
            return {
                "status": "online",
                "models": [m['name'] for m in resp.json().get('models', [])]
            }
        except Exception as e:
            return {"status": "error", "details": str(e)}




=== core\integrations\__init__.py ===
from importlib import import_module
from pathlib import Path
from typing import Dict, Type
from ..plugin import PluginBase

_PLUGINS: Dict[str, Type[PluginBase]] = {}

def _discover_plugins():
    package_dir = Path(__file__).parent
    for _, module_name, _ in pkgutil.iter_modules([str(package_dir)]):
        if module_name in ("__init__", "manager"):
            continue
        
        try:
            module = import_module(f".{module_name}", package=__package__)
            if (plugin_class := getattr(module, "Plugin", None)) and \
               issubclass(plugin_class, PluginBase):
                _PLUGINS[module_name] = plugin_class
        except (ImportError, TypeError) as e:
            import warnings
            warnings.warn(f"Failed to load {module_name}: {str(e)}")

def get_plugin(name: str) -> Type[PluginBase]:
    """Get plugin class by name (e.g., 'ollama')"""
    return _PLUGINS[name]  # Will raise KeyError if not found

def available_plugins() -> Dict[str, Type[PluginBase]]:
    """Return copy of registered plugins"""
    return _PLUGINS.copy()

# Initialize on import
import pkgutil
_discover_plugins()

__all__ = ['get_plugin', 'available_plugins']


=== core\integrations\grok.py ===
import requests
import time
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
from datetime import datetime

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="grok",
            version="0.3.0",
            required_config={
                "api_key": str,
                "rate_limit": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Grok AI API integration with batching"
        )

    def initialize(self):
        self.api_key = self.config["api_key"]
        self.rate_limit = self.config.get("rate_limit", 5)
        self.timeout = self.config.get("timeout", 10)
        self.last_calls = []
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        if not self._check_rate_limit():
            return {"error": "Rate limit exceeded"}

        try:
            self.last_calls.append(time.time())
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            "https://api.grok.ai/v1/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 150)
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _check_rate_limit(self):
        now = time.time()
        self.last_calls = [t for t in self.last_calls if t > now - 60]
        return len(self.last_calls) < self.rate_limit

    def health_check(self):
        return {
            "ready": self._initialized,
            "rate_limit": f"{len(self.last_calls)}/{self.rate_limit}",
            "last_call": self.last_calls[-1] if self.last_calls else None
        }


=== core\integrations\huggingface.py ===
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import torch
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="huggingface",
            version="0.5.0",
            required_config={
                "model_name": str,
                "device": str,
                "quantize": bool,
                "batch_size": int
            },
            dependencies=["transformers>=4.30.0", "torch"],
            description="HuggingFace Transformers with batching"
        )

    def initialize(self):
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config["model_name"],
                device_map="auto" if self.config["device"] == "auto" else None
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model_name"]
            )
            if self.config.get("quantize", False):
                self.model = torch.quantization.quantize_dynamic(
                    self.model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
            self.batch_size = self.config.get("batch_size", 4)
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"Initialization failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(input_data["prompt"], return_tensors="pt").to(self.config["device"])
        outputs = self.model.generate(**inputs)
        return {"response": self.tokenizer.decode(outputs[0])}

    def _batch_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(
            input_data["prompt"],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.config["device"])
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=input_data.get("max_tokens", 50),
            num_return_sequences=1,
            batch_size=self.batch_size
        )
        
        return {
            "responses": [
                self.tokenizer.decode(output, skip_special_tokens=True)
                for output in outputs
            ]
        }

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("huggingface").observe(time.time() - start_time)


=== core\integrations\lmstudio.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="lmstudio",
            version="0.4.0",
            required_config={
                "base_url": str,
                "timeout": int,
                "batch_support": bool
            },
            dependencies=["requests"],
            description="LM Studio local server with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.timeout = self.config.get("timeout", 60)
        self._batch_support = self.config.get("batch_support", False)
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self._batch_support

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/v1/completions",
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 200),
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("lmstudio").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/v1/models", timeout=5)
            base_status["status"] = "online" if resp.ok else "offline"
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\manager.py ===
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any
from core.monitoring.service import Monitoring

class Plugin(PluginBase):
    def __init__(self, config: Dict[str, Any], monitor: Monitoring):
        super().__init__(config)
        self.monitor = monitor

    def get_metadata(self):
        return PluginMetadata(
            name="manager",
            version="0.2.0",
            description="Enhanced plugin management with monitoring",
            required_config={}
        )

    @self.monitor.track_request('plugin_manager')
    def execute(self, command: Dict[str, Any]) -> Dict[str, Any]:
        """Monitored plugin management"""
        try:
            if command["action"] == "list_plugins":
                return self._list_plugins()
            elif command["action"] == "plugin_status":
                return self._plugin_status(command["plugin"])
            else:
                return {"error": "Unknown action"}
        except Exception as e:
            self.monitor.REQUEST_COUNT.labels('manager', 'failed').inc()
            raise

    def _list_plugins(self) -> Dict[str, Any]:
        return {
            "plugins": list(self.context.plugin_manager.plugins.keys()),
            "stats": {
                "ready": sum(1 for p in self.context.plugin_manager.plugins.values() if p.is_ready()),
                "total": len(self.context.plugin_manager.plugins)
            }
        }

    def _plugin_status(self, plugin_name: str) -> Dict[str, Any]:
        plugin = self.context.plugin_manager.get_plugin(plugin_name)
        return {
            "exists": plugin is not None,
            "ready": plugin.is_ready() if plugin else False,
            "metadata": plugin.metadata if plugin else None
        }


=== core\integrations\ollama.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="ollama",
            version="0.5.0",
            required_config={
                "base_url": str,
                "default_model": str,
                "batch_size": int
            },
            dependencies=["requests"],
            description="Ollama with experimental batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.default_model = self.config.get("default_model", "llama2")
        self.batch_size = self.config.get("batch_size", 1)  # Default to no batching
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": input_data.get("model", self.default_model),
                "prompt": input_data["prompt"],
                "stream": False,
                **input_data.get("options", {})
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        # Note: Ollama doesn't natively support batching, so we parallelize
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.batch_size) as executor:
            results = list(executor.map(
                lambda p: self._single_execute({"prompt": p}),
                input_data["prompt"]
            ))
        return {"responses": results}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("ollama").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/tags", timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "models": [m["name"] for m in resp.json().get("models", [])]
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\textgen.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="textgen",
            version="0.4.0",
            required_config={
                "base_url": str,
                "api_key": str,
                "batch_size": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Text Generation WebUI API with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.api_key = self.config["api_key"]
        self.batch_size = self.config.get("batch_size", 1)
        self.timeout = self.config.get("timeout", 30)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start_time)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/v1/generate",
            headers=self.headers,
            json={
                "prompt": input_data["prompt"],
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        """Execute multiple prompts as a batch"""
        responses = []
        prompts = input_data["prompt"]
        
        # Process in chunks of batch_size
        for i in range(0, len(prompts), self.batch_size):
            chunk = prompts[i:i + self.batch_size]
            response = requests.post(
                f"{self.base_url}/api/v1/generate_batch",  # Note: Your API must support this endpoint
                headers=self.headers,
                json={
                    "prompts": chunk,
                    **input_data.get("parameters", {})
                },
                timeout=self.timeout
            )
            response.raise_for_status()
            responses.extend(response.json()["results"])
            
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("textgen").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/v1/model", 
                             headers=self.headers,
                             timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "model": resp.json().get("model_name", "unknown")
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\vllm.py ===
from vllm import LLM, SamplingParams
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="vllm",
            version="0.4.0",
            required_config={
                "model": str,
                "tensor_parallel_size": int,
                "gpu_memory_utilization": float,
                "max_batch_size": int
            },
            dependencies=["vllm>=0.2.0"],
            description="High-performance batched inference"
        )

    def initialize(self):
        try:
            self.llm = LLM(
                model=self.config["model"],
                tensor_parallel_size=self.config.get("tensor_parallel_size", 1),
                gpu_memory_utilization=self.config.get("gpu_memory_utilization", 0.9),
                max_num_batched_tokens=self.config.get("max_batch_size", 2560)
            )
            self.default_params = SamplingParams(
                temperature=0.8,
                top_p=0.95
            )
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"vLLM init failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            params = self.default_params.copy()
            if "parameters" in input_data:
                params = SamplingParams(**input_data["parameters"])
            
            if isinstance(input_data["prompt"], list):
                outputs = self.llm.generate(input_data["prompt"], params)
                return {
                    "responses": [o.outputs[0].text for o in outputs]
                }
            else:
                output = self.llm.generate([input_data["prompt"]], params)
                return {"response": output[0].outputs[0].text}
        finally:
            self._log_latency(start)

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("vllm").observe(time.time() - start_time)

    def health_check(self):
        status = super().health_check()
        status["gpu_utilization"] = self._get_gpu_stats()
        status["batch_capacity"] = self.llm.llm_engine.scheduler_config.max_num_batched_tokens
        return status


=== core\interface.py ===
# core/interface.py
from fastapi import APIRouter, Depends, HTTPException, status
from typing import Dict, List, Optional
from shared.schemas import Query, Response, FeedbackRating, FeedbackCorrection
from core.context import ContextManager
from core.orchestrator import Orchestrator
from core.validation.quality_gates import QualityValidator
import logging

logger = logging.getLogger(__name__)

class InterfaceManager:
    def __init__(self, orchestrator: Orchestrator, context: ContextManager, validator: QualityValidator):
        self.orchestrator = orchestrator
        self.context = context
        self.validator = validator
        self.router = APIRouter()
        self._setup_routes()
    
    def _setup_routes(self):
        @self.router.post("/query", response_model=Response)
        async def process_query(query: Query):
            """Main query processing endpoint"""
            try:
                return await self.orchestrator.route_query(query)
            except Exception as e:
                logger.error(f"Query processing failed: {str(e)}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Processing failed: {str(e)}"
                )
        
        @self.router.get("/health")
        async def health_check():
            """System health status"""
            return {
                "status": "healthy",
                "components": {
                    "orchestrator": "operational",
                    "context": "operational",
                    "validator": "operational"
                }
            }
        
        @self.router.get("/stats")
        async def get_statistics():
            """System usage statistics"""
            return {
                "knowledge_graph": {
                    "nodes": len(self.context.graph.graph.nodes()),
                    "edges": len(self.context.graph.graph.edges()),
                    "interactions": len(self.context.interaction_log)
                },
                "modules": {
                    name: module.health_check()
                    for name, module in self.orchestrator.registry._instances.items()
                }
            }


=== core\ml\model_manager.py ===
# core/ml/model_manager.py
import asyncio
import json
import hashlib
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import aiofiles
import aiohttp
from dataclasses import dataclass
from enum import Enum

class ModelStatus(Enum):
    AVAILABLE = "available"
    LOADING = "loading"
    UPDATING = "updating"
    ERROR = "error"

class ModelType(Enum):
    CODE_COMPLETION = "code_completion"
    REFACTORING = "refactoring"
    MULTIMODAL = "multimodal"
    QUALITY_ANALYSIS = "quality_analysis"

@dataclass
class ModelInfo:
    name: str
    type: ModelType
    version: str
    status: ModelStatus
    path: str
    size_mb: float
    created_at: datetime
    last_used: datetime
    performance_metrics: Dict[str, float]
    metadata: Dict[str, Any]

class ModelManager:
    def __init__(self, models_dir: str = "models"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)
        self.loaded_models: Dict[str, ModelInfo] = {}
        self.model_registry: Dict[str, Dict[str, Any]] = {}
        self._load_model_registry()
    
    def _load_model_registry(self):
        """Load model registry from file"""
        registry_file = self.models_dir / "registry.json"
        if registry_file.exists():
            with open(registry_file, 'r') as f:
                self.model_registry = json.load(f)
        else:
            self.model_registry = self._initialize_registry()
            self._save_registry()
    
    def _initialize_registry(self) -> Dict[str, Dict[str, Any]]:
        """Initialize model registry with default models"""
        return {
            "code_completion": {
                "name": "code-completion",
                "type": ModelType.CODE_COMPLETION.value,
                "current_version": "1.0.0",
                "available_versions": ["1.0.0"],
                "download_url": "https://models.example.com/code_completion",
                "description": "Code completion model"
            },
            "refactoring": {
                "name": "refactoring",
                "type": ModelType.REFACTORING.value,
                "current_version": "1.0.0",
                "available_versions": ["1.0.0"],
                "download_url": "https://models.example.com/refactoring",
                "description": "Code refactoring analysis model"
            },
            "multimodal": {
                "name": "multimodal",
                "type": ModelType.MULTIMODAL.value,
                "current_version": "1.0.0",
                "available_versions": ["1.0.0"],
                "download_url": "https://models.example.com/multimodal",
                "description": "Multimodal code analysis model"
            }
        }
    
    def _save_registry(self):
        """Save model registry to file"""
        registry_file = self.models_dir / "registry.json"
        with open(registry_file, 'w') as f:
            json.dump(self.model_registry, f, indent=2)
    
    async def download_model(self, model_type: ModelType, version: str = None) -> bool:
        """Download a model"""
        if model_type.value not in self.model_registry:
            return False
        
        model_info = self.model_registry[model_type.value]
        download_version = version or model_info["current_version"]
        
        # Check if model already exists
        model_path = self.models_dir / f"{model_type.value}_{download_version}"
        if model_path.exists():
            return True
        
        try:
            # Create model directory
            model_path.mkdir(exist_ok=True)
            
            # Download model files
            download_url = f"{model_info['download_url']}/{download_version}"
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{download_url}/model.bin") as response:
                    if response.status == 200:
                        model_file = model_path / "model.bin"
                        async with aiofiles.open(model_file, 'wb') as f:
                            await f.write(await response.read())
                        
                        # Download metadata
                        async with session.get(f"{download_url}/metadata.json") as meta_response:
                            if meta_response.status == 200:
                                metadata_file = model_path / "metadata.json"
                                async with aiofiles.open(metadata_file, 'w') as f:
                                    await f.write(await meta_response.text())
                        
                        return True
            
            return False
        except Exception as e:
            print(f"Failed to download model: {e}")
            return False
    
    async def load_model(self, model_type: ModelType, version: str = None) -> Optional[ModelInfo]:
        """Load a model into memory"""
        if model_type.value not in self.model_registry:
            return None
        
        model_info = self.model_registry[model_type.value]
        load_version = version or model_info["current_version"]
        
        model_path = self.models_dir / f"{model_type.value}_{load_version}"
        if not model_path.exists():
            # Try


=== core\monitoring\service.py ===
import time
from prometheus_client import start_http_server, Counter, Gauge, Histogram

class Monitoring:
    def __init__(self, port=9090):
        # Metrics Definitions
        self.REQUEST_COUNT = Counter(
            'llm_requests_total',
            'Total API requests',
            ['module', 'status']
        )
        
        self.LATENCY = Histogram(
            'llm_response_latency_seconds',
            'Response latency distribution',
            ['provider']
        )
        
        self.CACHE_HITS = Gauge(
            'cache_hit_ratio',
            'Current cache hit percentage'
        )
        
        start_http_server(port)

    def track_request(self, module: str):
        """Decorator to monitor request metrics"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.time()
                try:
                    result = func(*args, **kwargs)
                    self.REQUEST_COUNT.labels(module, 'success').inc()
                    return result
                except Exception:
                    self.REQUEST_COUNT.labels(module, 'failed').inc()
                    raise
                finally:
                    self.LATENCY.labels(module).observe(time.time() - start)
            return wrapper
        return decorator

    def update_cache_metrics(self, hits: int, misses: int):
        """Update cache performance metrics"""
        self.CACHE_HITS.set(hits / max(hits + misses, 1))


=== core\multimodal\image_analyser.py ===
# core/multimodal/image_analyzer.py
import base64
import io
from typing import Dict, Any, List
from PIL import Image
import pytesseract
from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification
import re

class ImageAnalyzer:
    def __init__(self):
        self.ocr_reader = None
        self.code_classifier = None
        self.image_processor = None
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize ML models for image analysis"""
        try:
            # Initialize OCR
            self.ocr_reader = pytesseract
            
            # Initialize code image classifier
            self.code_classifier = pipeline(
                "image-classification",
                model="microsoft/swin-base-patch4-window7-224"
            )
            
            # Initialize image processor
            self.image_processor = AutoImageProcessor.from_pretrained(
                "microsoft/swin-base-patch4-window7-224"
            )
        except Exception as e:
            print(f"Failed to initialize image models: {e}")
    
    async def analyze_code_image(self, image_data: str) -> Dict[str, Any]:
        """
        Analyze code from image (screenshot, handwritten code, etc.)
        Returns extracted code, language detection, and structure analysis
        """
        try:
            # Decode base64 image
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            # Extract text using OCR
            extracted_text = self._extract_text(image)
            
            # Detect programming language
            language = self._detect_language(extracted_text)
            
            # Structure the code
            structured_code = self._structure_code(extracted_text, language)
            
            # Analyze code patterns
            patterns = self._analyze_patterns(structured_code, language)
            
            return {
                "success": True,
                "extracted_text": extracted_text,
                "language": language,
                "structured_code": structured_code,
                "patterns": patterns,
                "confidence": self._calculate_confidence(extracted_text)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _extract_text(self, image: Image.Image) -> str:
        """Extract text from image using OCR"""
        # Preprocess image for better OCR
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Use Tesseract OCR
        text = self.ocr_reader.image_to_string(image)
        return text
    
    def _detect_language(self, code: str) -> str:
        """Detect programming language from extracted text"""
        language_patterns = {
            "python": [r"def\s+\w+\(", r"import\s+\w+", r"if\s+__name__"],
            "javascript": [r"function\s+\w+\(", r"const\s+\w+\s*=", r"console\.log"],
            "java": [r"public\s+class\s+\w+", r"public\s+static\s+void", r"System\.out"],
            "csharp": [r"using\s+System", r"public\s+class\s+\w+", r"Console\.Write"],
            "cpp": [r"#include\s*<", r"int\s+main\(", r"std::"],
            "html": [r"<!DOCTYPE", r"<html>", r"</html>"],
            "css": [r"\{[^}]*:\s*[^;]*;"]
        }
        
        scores = {}
        for lang, patterns in language_patterns.items():
            score = sum(1 for pattern in patterns if re.search(pattern, code, re.IGNORECASE))
            if score > 0:
                scores[lang] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else "unknown"
    
    def _structure_code(self, text: str, language: str) -> str:
        """Structure and format the extracted code"""
        # Basic cleanup
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:  # Skip empty lines
                # Fix common OCR errors
                line = self._fix_ocr_errors(line, language)
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _fix_ocr_errors(self, line: str, language: str) -> str:
        """Fix common OCR errors for different languages"""
        fixes = {
            "python": {
                "def ": "def ",
                "imp ort": "import",
                "retu rn": "return",
                "class ": "class "
            },
            "javascript": {
                "fun ction": "function",
                "con st": "const",
                "var ": "var "
            }
        }
        
        lang_fixes = fixes.get(language, {})
        for error, correction in lang_fixes.items():
            line = line.replace(error, correction)
        
        return line
    
    def _analyze_patterns(self, code: str, language: str) -> List[Dict[str, Any]]:
        """Analyze code patterns and structures"""
        patterns = []
        
        if language == "python":
            # Check for functions
            func_matches = re.findall(r'def\s+(\w+)\s*\([^)]*\):', code)
            for func in func_matches:
                patterns.append({
                    "type": "function",
                    "name": func,
                    "line_start": code.find(f"def {func}")
                })
            
            # Check for classes
            class_matches = re.findall(r'class\s+(\w+):', code)
            for cls in class_matches:
                patterns.append({
                    "type": "class",
                    "name": cls,
                    "line_start": code.find(f"class {cls}")
                })
        
        elif language == "javascript":
            # Check for functions
            func_matches = re.findall(r'function\s+(\w+)\s*\(', code)
            for func in func_matches:
                patterns.append({
                    "type": "function",
                    "name": func,
                    "line_start": code.find(f"function {func}")
                })
            
            # Check for arrow functions
            arrow_matches = re.findall(r'(\w+)\s*=\s*\([^)]*\)\s*=>', code)
            for func in arrow_matches:
                patterns.append({
                    "type": "arrow_function",
                    "name": func,
                    "line_start": code.find(f"{func} =")
                })
        
        return patterns
    
    def _calculate_confidence(self, text: str) -> float:
        """Calculate confidence score for the extraction"""
        # Simple heuristic based on code-like patterns
        code_indicators = [
            r'[a-zA-Z_][a-zA-Z0-9_]*\s*\(',  # Function calls
            r'[a-zA-Z_][a-zA-Z0-9_]*\s*=',  # Variable assignments
            r'\b(if|else|for|while|class|def|function)\b',  # Keywords
            r'[{}[\]()<>]',  # Brackets
        ]
        
        score = 0
        for indicator in code_indicators:
            matches = re.findall(indicator, text)
            score += len(matches)
        
        # Normalize to 0-1 range
        return min(score / 10, 1.0)


=== core\orchestration\budget_router.py ===
from typing import Dict, Literal
from ..performance.cost import CostMonitor

class BudgetRouter:
    def __init__(self, cost_monitor: CostMonitor):
        self.cost = cost_monitor

    def select_llm(self, query: Dict) -> Literal['premium', 'standard', 'local']:
        """Choose LLM tier based on budget and query criticality"""
        forecast = self.cost.get_spend_forecast()
        criticality = query.get("criticality", 0.5)
        
        if forecast["burn_rate"] > forecast["budget_remaining"] / 7:  # Weekly burn
            return 'local'
        elif criticality > 0.8 and forecast["budget_remaining"] > 50:
            return 'premium'
        else:
            return 'standard'


=== core\orchestration\load_balancer.py ===
from typing import Dict, List
import numpy as np
from collections import deque
from ..performance.tracker import PerformanceTracker

class LoadBalancer:
    def __init__(self, tracker: PerformanceTracker):
        self.tracker = tracker
        self.weights = {}  # Provider -> weight
        self.history = deque(maxlen=100)  # Tracks last 100 routing decisions

    def update_weights(self):
        """Calculate new weights based on performance"""
        metrics = self.tracker.get_provider_metrics()
        total = sum(m['requests_per_second'] / (m['avg_latency'] + 1e-6) for m in metrics.values())
        
        self.weights = {
            provider: (m['requests_per_second'] / (m['avg_latency'] + 1e-6)) / total
            for provider, m in metrics.items()
        }

    def select_provider(self, query: Dict) -> str:
        """Select provider using weighted random choice"""
        providers = list(self.weights.keys())
        weights = list(self.weights.values())
        choice = np.random.choice(providers, p=weights)
        self.history.append((query['content'][:50], choice))
        return choice


=== core\orchestration\sla_router.py ===
from typing import Dict, Literal
from dataclasses import dataclass
from ..performance.cost import CostMonitor
from ..performance.tracker import PerformanceTracker
import numpy as np

@dataclass
class SLATier:
    name: str
    min_accuracy: float
    max_latency: float  # seconds
    allowed_providers: list
    cost_multiplier: float = 1.0

class SLARouter:
    def __init__(self, cost_monitor: CostMonitor, perf_tracker: PerformanceTracker):
        self.cost = cost_monitor
        self.performance = perf_tracker
        
        # Define service tiers
        self.tiers = {
            "critical": SLATier(
                name="critical",
                min_accuracy=0.95,
                max_latency=1.5,
                allowed_providers=["gpt-4", "claude-2", "vllm"],
                cost_multiplier=2.0
            ),
            "standard": SLATier(
                name="standard",
                min_accuracy=0.85,
                max_latency=3.0,
                allowed_providers=["gpt-3.5", "claude-instant", "llama2"]
            ),
            "economy": SLATier(
                name="economy",
                min_accuracy=0.70,
                max_latency=5.0,
                allowed_providers=["llama2", "local"]
            )
        }

    def select_provider(self, query: Dict) -> Dict[str, str]:
        """Select optimal provider based on SLA and budget"""
        # Determine SLA tier
        tier = self._determine_tier(query)
        
        # Get eligible providers
        candidates = [
            p for p in self.performance.get_available_providers()
            if p in tier.allowed_providers
        ]
        
        # Rank by performance/cost tradeoff
        ranked = sorted(
            candidates,
            key=lambda p: self._score_provider(p, tier)
        )
        
        return {
            "provider": ranked[0],
            "tier": tier.name,
            "reason": f"Best match for {tier.name} SLA"
        }

    def _determine_tier(self, query: Dict) -> SLATier:
        """Auto-select SLA tier based on query properties"""
        if query.get("user_priority") == "high":
            return self.tiers["critical"]
        
        # Auto-detect critical queries
        if ("error" in query.get("intent", "") or 
            "production" in query.get("context", "")):
            return self.tiers["critical"]
            
        # Budget-aware fallback
        budget_status = self.cost.get_spend_forecast()
        if budget_status["burn_rate"] > budget_status["budget_remaining"] / 10:
            return self.tiers["economy"]
            
        return self.tiers["standard"]

    def _score_provider(self, provider: str, tier: SLATier) -> float:
        """Score providers (0-1) based on SLA fit"""
        metrics = self.performance.get_provider_metrics(provider)
        
        # Normalized performance score (higher better)
        accuracy_score = metrics["accuracy"] / tier.min_accuracy
        latency_score = tier.max_latency / max(metrics["latency"], 0.1)
        
        # Cost penalty (lower better)
        cost_rate = self.cost._get_rate(provider.split('-')[0], provider)
        cost_penalty = cost_rate["input"] * tier.cost_multiplier
        
        return np.mean([accuracy_score, latency_score]) / cost_penalty


=== core\orchestrator.py ===
from typing import Dict, List
from shared.schemas import Query, Response
from modules.base_module import BaseModule
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.validation.quality_gates import QualityValidator
from core.orchestration.sla_router import SLARouter
from core.orchestration.load_balancer import LoadBalancer
from core.reasoning.engine import HybridEngine
from core.prediction.warmer import CacheWarmer
from core.monitoring.service import Monitoring
from core.processing.batcher import AdaptiveBatcher
import logging
import asyncio
import numpy as np

class Orchestrator:
    def __init__(
        self,
        validator: QualityValidator,
        sla_router: SLARouter,
        load_balancer: LoadBalancer,
        registry,
        healing_controller: SelfHealingController,
        context_manager: ContextManager,
        reasoning_engine: HybridEngine,
        monitoring: Monitoring
    ):
        self.validator = validator
        self.sla_router = sla_router
        self.load_balancer = load_balancer
        self.registry = registry
        self.healing = healing_controller
        self.context = context_manager
        self.reasoning = reasoning_engine
        self.monitor = monitoring
        self.logger = logging.getLogger("orchestrator")
        self.cache_warmer = CacheWarmer(self, self.context.cache_predictor)
        self.batcher = AdaptiveBatcher(
            max_batch_size=self.context.config.get("batching.max_size", 8),
            max_wait_ms=self.context.config.get("batching.max_wait_ms", 50)
        )
        self._setup_fallback_strategies()
        asyncio.create_task(self.batcher.background_flush())
        asyncio.create_task(self._update_balancer_weights())

    def _setup_fallback_strategies(self):
        self.fallback_map = {
            "python": "code_generic",
            "csharp": "code_generic",
            "math": "math_basic",
            "chat": "generic"
        }

    async def _update_balancer_weights(self):
        """Periodically update load balancer weights"""
        while True:
            await asyncio.sleep(
                self.context.config.get("load_balancing.update_interval", 10)
            )
            if len(self.load_balancer.history) >= self.context.config.get("load_balancing.min_requests", 20):
                self.load_balancer.update_weights()

    @self.monitor.track_request('orchestrator')
    async def route_query(self, query: Query) -> Response:
        """Enhanced query processing pipeline"""
        try:
            # 1. Get context and routing info
            pre_context = self.context.get_context(query.content)
            
            # Dynamic provider selection (NEW)
            if query.metadata.get("priority", 0) > 0:
                # High-priority uses SLA routing
                routing_decision = self.sla_router.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "user_priority": query.metadata.get("priority", "normal")
                })
                provider = routing_decision["provider"]
            else:
                # Normal traffic uses load balancing
                provider = self.load_balancer.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "priority": query.metadata.get("priority", 0)
                })
                routing_decision = {"provider": provider, "tier": "balanced"}
            
            query.provider = provider

            # 2. Hybrid reasoning
            reasoning_result = await self.reasoning.process({
                "query": query.content,
                "context": pre_context,
                "llm_preference": provider
            })

            # 3. Module processing with quality gates
            module = self._select_module(
                query,
                pre_context,
                reasoning_source=reasoning_result.get("source")
            )
            enriched_query = query.with_additional_context(reasoning_result)
            
            # Process with batching if enabled
            if query.metadata.get("allow_batching", True):
                batch = await self.batcher.add_query(
                    enriched_query.model_dump(),
                    priority=query.metadata.get("priority", 0)
                )
                if len(batch) > 1:
                    return await self._batch_process(batch)

            raw_response = await module.process(enriched_query)

            # 4. Validate and enhance response
            validation = self.validator.validate(raw_response)
            if not validation["passed"]:
                return await self._handle_quality_failure(enriched_query, validation)

            final_response = self._augment_response(
                validation["original_response"],
                pre_context,
                reasoning_metadata={
                    "sla_tier": routing_decision["tier"],
                    "provider": provider,
                    "reasoning_path": reasoning_result["source"]
                }
            )

            # 5. Learn and cache
            self.context.process_interaction(
                query,
                final_response,
                metadata={
                    "sla_tier": routing_decision["tier"],
                    "reasoning_source": reasoning_result["source"],
                    "provider": provider
                }
            )
            asyncio.create_task(self.cache_warmer.warm_cache(query.content))

            return final_response

        except Exception as e:
            self.logger.error(f"Routing failed: {str(e)}")
            return await self._handle_failure(query, e)

    async def _batch_process(self, batch: List[Dict]) -> Response:
        """Process batched queries through LLM"""
        try:
            # Get first provider that supports batching
            provider = next(
                p for p in {
                    query.get("provider") for query in batch
                } 
                if (plugin := self.context.plugin_manager.get_plugin(p)) 
                and plugin.supports_batching
            )
            
            llm = self.context.plugin_manager.get_plugin(provider)
            combined = [q["content"] for q in batch]
            responses = await llm.batch_complete(combined)
            
            # Return only the response for our original query
            original_query = batch[0]["content"]
            return next(
                Response(content=r) 
                for q, r in zip(combined, responses)
                if q == original_query
            )
        except Exception as e:
            self.logger.warning(f"Batch processing failed: {str(e)}")
            return await self.route_query(Query(**batch[0]))

    async def _handle_quality_failure(self, query: Query, validation: Dict) -> Response:
        """Process failed quality checks"""
        self.logger.warning(f"Quality check failed: {validation['checks']}")
        return await self._retry_with_stricter_llm(query)

    def _select_module(self, query: Query, context: dict, reasoning_source: str = None) -> BaseModule:
        """Enhanced module selection"""
        if reasoning_source == "graph":
            return self.registry.get_module("knowledge")
        
        if any(match["type"] == "code" for match in context["matches"]):
            lang = self._detect_language(context["matches"])
            return self.registry.get_module(f"code_{lang}")
            
        return self.registry.get_module("chat")
        
    def _detect_language(self, matches: List[dict]) -> str:
        """Detect programming language from knowledge matches"""
        lang_keywords = {
            "python": ["def", "import", "lambda"],
            "csharp": ["var", "using", "namespace"]
        }
        
        for match in matches:
            content = match.get("content", "").lower()
            for lang, keywords in lang_keywords.items():
                if any(kw in content for kw in keywords):
                    return lang
        return "generic"
        
    def _augment_response(self, response: Response, context: dict, reasoning_metadata: dict = None) -> Response:
        """Enhance response with contextual knowledge"""
        if not response.metadata:
            response.metadata = {}
            
        response.metadata.update({
            "context": {
                "matched_concepts": [
                    {"id": m["node_id"], "content": m["content"]}
                    for m in context["matches"][:3]
                ],
                "related_concepts": [
                    {"id": n["id"], "content": n["content"]}
                    for n in context["related"][:5]
                ]
            },
            "processing": reasoning_metadata or {}
        })
        return response
        
    async def _handle_failure(self, query: Query, error: Exception) -> Response:
        """Handle routing failures with fallback logic"""
        module_id = query.content_type.split("_")[-1]
        fallback_id = self.fallback_map.get(module_id, "generic")
        
        if fallback := self.registry.get_module(fallback_id):
            response = await fallback.process(query)
            self.context.process_interaction(query, response)
            return response
            
        raise RuntimeError("All fallback strategies failed")

    async def _retry_with_stricter_llm(self, query: Query) -> Response:
        """Fallback strategy for quality failures"""
        query.metadata["require_quality"] = True
        return await self.route_query(query)


=== core\performance\cost.py ===
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, Literal, Optional
import warnings

Provider = Literal['openai', 'anthropic', 'ollama', 'huggingface']

class CostMonitor:
    def __init__(self, config: Dict):
        self.config = config
        self.cost_db = Path("data/cost_tracking.json")
        self._init_db()
        self.current_spend = 0.0
        self._load_current_period()

    def _init_db(self):
        """Initialize cost database with default structure"""
        if not self.cost_db.exists():
            with open(self.cost_db, 'w') as f:
                json.dump({
                    "monthly_budget": self.config.get("monthly_budget", 100.0),
                    "periods": [],
                    "provider_rates": {
                        "openai": {"gpt-4": 0.03, "gpt-3.5": 0.002},
                        "anthropic": {"claude-2": 0.0465, "claude-instant": 0.0163},
                        "ollama": {"llama2": 0.0, "mistral": 0.0},
                        "huggingface": {"default": 0.0}
                    }
                }, f)

    def _load_current_period(self):
        """Load or create current billing period"""
        with open(self.cost_db, 'r') as f:
            data = json.load(f)
        
        current_date = datetime.now().strftime("%Y-%m")
        if not data["periods"] or data["periods"][-1]["period"] != current_date:
            data["periods"].append({
                "period": current_date,
                "total_spend": 0.0,
                "breakdown": {p: 0.0 for p in data["provider_rates"].keys()}
            })
        
        self.current_period = data["periods"][-1]
        self.current_spend = self.current_period["total_spend"]

    def record_llm_call(
        self,
        provider: Provider,
        model: str,
        input_tokens: int,
        output_tokens: int
    ):
        """Calculate and record API call costs"""
        rate = self._get_rate(provider, model)
        cost = (input_tokens * rate["input"] + output_tokens * rate["output"]) / 1000
        
        with open(self.cost_db, 'r+') as f:
            data = json.load(f)
            current = data["periods"][-1]
            current["total_spend"] += cost
            current["breakdown"][provider] += cost
            self.current_spend = current["total_spend"]
            
            # Check budget threshold (80% warning)
            if current["total_spend"] > data["monthly_budget"] * 0.8:
                warnings.warn(
                    f"Approaching budget limit: {current['total_spend']:.2f}/{data['monthly_budget']}",
                    RuntimeWarning
                )
            
            f.seek(0)
            json.dump(data, f, indent=2)

    def _get_rate(self, provider: Provider, model: str) -> Dict[str, float]:
        """Get current token rates for a provider/model"""
        with open(self.cost_db, 'r') as f:
            rates = json.load(f)["provider_rates"]
            provider_rates = rates.get(provider, {})
            return {
                "input": provider_rates.get(model, provider_rates.get("default", 0.0)),
                "output": provider_rates.get(model, provider_rates.get("default", 0.0))
            }

    def get_spend_forecast(self) -> Dict:
        """Predict end-of-period spend"""
        now = datetime.now()
        days_in_month = (now.replace(month=now.month+1, day=1) - timedelta(days=1)).day
        days_elapsed = now.day
        daily_avg = self.current_spend / days_elapsed
        
        return {
            "current_spend": self.current_spend,
            "projected_spend": daily_avg * days_in_month,
            "budget_remaining": self.config["monthly_budget"] - self.current_spend,
            "burn_rate": daily_avg
        }


=== core\performance\hashing.py ===
import hashlib
import json
from typing import Dict, Any

class QueryHasher:
    @staticmethod
    def hash_query(query: Dict[str, Any]) -> str:
        """Create consistent hash for similar queries"""
        normalized = {
            "code": query.get("code", "").strip(),
            "intent": query.get("intent", ""),
            "context": sorted(query.get("context", []))
        }
        return hashlib.sha256(
            json.dumps(normalized, sort_keys=True).encode()
        ).hexdigest()


=== core\performance\optimisation.py ===
# core/performance/optimizations.py
import asyncio
import aioredis
from typing import Dict, Any, Optional
from contextlib import asynccontextmanager
import time
from functools import wraps

class ConnectionPool:
    def __init__(self, max_connections=10):
        self.max_connections = max_connections
        self.connections = asyncio.Queue(maxsize=max_connections)
        self.created_connections = 0
    
    async def get_connection(self):
        """Get a connection from the pool"""
        if self.connections.empty() and self.created_connections < self.max_connections:
            # Create new connection
            conn = await self._create_connection()
            self.created_connections += 1
            return conn
        
        return await self.connections.get()
    
    async def release_connection(self, conn):
        """Release a connection back to the pool"""
        await self.connections.put(conn)
    
    async def _create_connection(self):
        """Create a new connection (example for Redis)"""
        return await aioredis.create_redis_pool('redis://localhost')
    
    @asynccontextmanager
    async def get_connection_context(self):
        """Context manager for connection handling"""
        conn = await self.get_connection()
        try:
            yield conn
        finally:
            await self.release_connection(conn)

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
    
    def track_performance(self, metric_name: str):
        """Decorator to track performance metrics"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    execution_time = time.time() - start_time
                    
                    # Update metrics
                    if metric_name not in self.metrics:
                        self.metrics[metric_name] = []
                    self.metrics[metric_name].append(execution_time)
                    
                    return result
                except Exception as e:
                    execution_time = time.time() - start_time
                    
                    # Track error metrics
                    error_metric = f"{metric_name}_errors"
                    if error_metric not in self.metrics:
                        self.metrics[error_metric] = []
                    self.metrics[error_metric].append(execution_time)
                    
                    raise e
            
            return wrapper
        return decorator
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        result = {}
        for metric_name, times in self.metrics.items():
            if times:
                result[metric_name] = {
                    "count": len(times),
                    "avg": sum(times) / len(times),
                    "min": min(times),
                    "max": max(times)
                }
        return result

# Enhanced knowledge graph with caching
class CachedKnowledgeGraph:
    def __init__(self, base_graph, redis_pool: ConnectionPool):
        self.base_graph = base_graph
        self.redis_pool = redis_pool
        self.cache_ttl = 3600  # 1 hour
    
    async def find_semantic_matches(self, query: str, threshold: float = 0.7):
        """Cached version of semantic matching"""
        cache_key = f"semantic_match:{hash(query)}"
        
        async with self.redis_pool.get_connection_context() as conn:
            # Try to get from cache
            cached_result = await conn.get(cache_key)
            if cached_result:
                return json.loads(cached_result)
            
            # Compute result
            result = self.base_graph.find_semantic_matches(query, threshold)
            
            # Cache the result
            await conn.setex(cache_key, self.cache_ttl, json.dumps(result))
            
            return result


=== core\performance\tracker.py ===
from datetime import datetime
from pathlib import Path
import json
import statistics
from typing import Dict, List, Literal

SolutionSource = Literal['graph', 'rule', 'llm', 'learned_rule']

class PerformanceTracker:
    def __init__(self):
        self.metrics_path = Path("data/performance_metrics.json")
        self._init_storage()
        self.session_metrics: List[Dict] = []

    def _init_storage(self):
        self.metrics_path.parent.mkdir(exist_ok=True)
        if not self.metrics_path.exists():
            with open(self.metrics_path, 'w') as f:
                json.dump({"sessions": []}, f)

    def record_metric(
        self,
        source: SolutionSource,
        latency: float,
        success: bool,
        query_hash: str
    ):
        """Record performance metrics for each solution"""
        metric = {
            "timestamp": datetime.utcnow().isoformat(),
            "source": source,
            "latency_ms": latency * 1000,
            "success": success,
            "query_hash": query_hash[:8]  # Truncated for privacy
        }
        self.session_metrics.append(metric)

    def get_recommended_source(self, query_hash: str) -> SolutionSource:
        """Determine optimal solution source based on history"""
        history = self._load_history()
        
        # Check for identical past queries
        if query_hash:
            for m in reversed(history):
                if m['query_hash'] == query_hash:
                    if m['success']:
                        return m['source']
                    break

        # Calculate source effectiveness
        success_rates = {}
        latencies = {}
        
        for source in ['graph', 'rule', 'llm', 'learned_rule']:
            source_metrics = [m for m in history if m['source'] == source]
            if source_metrics:
                success_rates[source] = sum(
                    1 for m in source_metrics if m['success']
                ) / len(source_metrics)
                latencies[source] = statistics.median(
                    [m['latency_ms'] for m in source_metrics]
                )

        # Prioritize by success then speed
        if success_rates:
            best_source = max(
                success_rates.keys(),
                key=lambda k: (success_rates[k], -latencies[k])
            )
            return best_source
        return 'llm'  # Default fallback

    def _load_history(self) -> List[Dict]:
        """Load historical metrics"""
        with open(self.metrics_path, 'r') as f:
            data = json.load(f)
            return data['sessions'] + self.session_metrics
            
    def get_provider_metrics(self) -> Dict[str, Dict]:
        """Calculate real-time performance metrics"""
        history = self._load_history()
        window = [m for m in history if m['timestamp'] > time.time() - 60]  # Last 60s
        
        metrics = {}
        for provider in set(m['source'] for m in window):
            provider_metrics = [m for m in window if m['source'] == provider]
            metrics[provider] = {
                'requests_per_second': len(provider_metrics) / 60,
                'avg_latency': np.mean([m['latency_ms'] for m in provider_metrics]) / 1000,
                'error_rate': sum(1 for m in provider_metrics if not m['success']) / len(provider_metrics)
            }
        return metrics

    def get_available_providers(self) -> List[str]:
        """List all currently enabled providers"""
        return ["gpt-4", "gpt-3.5", "claude-2", "llama2", "local"]  # From config


=== core\personalization\user_profile.py ===
# core/personalization/user_profile.py
import json
import hashlib
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

@dataclass
class UserPreference:
    preferred_language: str
    preferred_llm: str
    code_style: str  # "concise", "verbose", "documented"
    complexity_tolerance: float  # 0-1 scale
    response_length: str  # "short", "medium", "long"
    auto_completion: bool
    show_explanations: bool
    theme: str  # "light", "dark", "auto"

@dataclass
class UserBehavior:
    query_patterns: Dict[str, int]  # query_type -> count
    completion_acceptance_rate: float
    feedback_history: List[Dict[str, Any]]
    learning_progress: Dict[str, float]  # topic -> mastery level
    session_frequency: Dict[str, int]  # day_of_week -> count
    preferred_time_slots: List[str]  # ["morning", "afternoon", "evening", "night"]

@dataclass
class UserProfile:
    user_id: str
    preferences: UserPreference
    behavior: UserBehavior
    created_at: datetime
    last_updated: datetime
    skill_level: Dict[str, float]  # language -> skill level (0-1)

class UserProfileManager:
    def __init__(self, storage_path: str = "data/user_profiles"):
        self.storage_path = storage_path
        self.profiles: Dict[str, UserProfile] = {}
        self._load_profiles()
    
    def _load_profiles(self):
        """Load user profiles from storage"""
        import os
        if os.path.exists(self.storage_path):
            for filename in os.listdir(self.storage_path):
                if filename.endswith('.json'):
                    user_id = filename[:-5]  # Remove .json extension
                    with open(os.path.join(self.storage_path, filename), 'r') as f:
                        profile_data = json.load(f)
                        self.profiles[user_id] = self._deserialize_profile(profile_data)
    
    def _deserialize_profile(self, profile_data: Dict[str, Any]) -> UserProfile:
        """Deserialize profile data from JSON"""
        return UserProfile(
            user_id=profile_data["user_id"],
            preferences=UserPreference(**profile_data["preferences"]),
            behavior=UserBehavior(**profile_data["behavior"]),
            created_at=datetime.fromisoformat(profile_data["created_at"]),
            last_updated=datetime.fromisoformat(profile_data["last_updated"]),
            skill_level=profile_data["skill_level"]
        )
    
    def get_profile(self, user_id: str) -> UserProfile:
        """Get or create user profile"""
        if user_id not in self.profiles:
            self.profiles[user_id] = self._create_default_profile(user_id)
        
        return self.profiles[user_id]
    
    def _create_default_profile(self, user_id: str) -> UserProfile:
        """Create default user profile"""
        return UserProfile(
            user_id=user_id,
            preferences=UserPreference(
                preferred_language="python",
                preferred_llm="gpt-4",
                code_style="concise",
                complexity_tolerance=0.5,
                response_length="medium",
                auto_completion=True,
                show_explanations=True,
                theme="auto"
            ),
            behavior=UserBehavior(
                query_patterns={},
                completion_acceptance_rate=0.5,
                feedback_history=[],
                learning_progress={},
                session_frequency={},
                preferred_time_slots=[]
            ),
            created_at=datetime.now(),
            last_updated=datetime.now(),
            skill_level={"python": 0.5, "javascript": 0.3, "java": 0.2}
        )
    
    def update_profile(self, user_id: str, updates: Dict[str, Any]) -> UserProfile:
        """Update user profile"""
        profile = self.get_profile(user_id)
        
        # Update preferences
        if "preferences" in updates:
            for key, value in updates["preferences"].items():
                if hasattr(profile.preferences, key):
                    setattr(profile.preferences, key, value)
        
        # Update behavior
        if "behavior" in updates:
            for key, value in updates["behavior"].items():
                if hasattr(profile.behavior, key):
                    setattr(profile.behavior, key, value)
        
        # Update skill level
        if "skill_level" in updates:
            profile.skill_level.update(updates["skill_level"])
        
        profile.last_updated = datetime.now()
        self._save_profile(profile)
        
        return profile
    
    def record_interaction(self, user_id: str, interaction_data: Dict[str, Any]):
        """Record user interaction for learning"""
        profile = self.get_profile(user_id)
        
        # Update query patterns
        query_type = interaction_data.get("query_type", "general")
        profile.behavior.query_patterns[query_type] = profile.behavior.query_patterns.get(query_type, 0) + 1
        
        # Update completion acceptance
        if "completion_accepted" in interaction_data:
            if profile.behavior.completion_acceptance_rate is None:
                profile.behavior.completion_acceptance_rate = 0.0
            
            # Update acceptance rate with exponential moving average
            alpha = 0.1  # Learning rate
            if interaction_data["completion_accepted"]:
                profile.behavior.completion_acceptance_rate = (
                    alpha * 1.0 + (1 - alpha) * profile.behavior.completion_acceptance_rate
                )
            else:
                profile.behavior.completion_acceptance_rate = (
                    alpha * 0.0 + (1 - alpha) * profile.behavior.completion_acceptance_rate
                )
        
        # Update feedback history
        if "feedback" in interaction_data:
            profile.behavior.feedback_history.append({
                "timestamp": datetime.now().isoformat(),
                "feedback": interaction_data["feedback"],
                "query": interaction_data.get("query", ""),
                "response": interaction_data.get("response", "")
            })
            
            # Keep only last 100 feedback entries
            if len(profile.behavior.feedback_history) > 100:
                profile.behavior.feedback_history = profile.behavior.feedback_history[-100:]
        
        # Update session frequency
        if "session_time" in interaction_data:
            session_time = datetime.fromisoformat(interaction_data["session_time"])
            day_of_week = session_time.strftime("%A").lower()
            profile.behavior.session_frequency[day_of_week] = profile.behavior.session_frequency.get(day_of_week, 0) + 1
        
        # Update preferred time slots
        if "session_time" in interaction_data:
            hour = session_time.hour
            if 6 <= hour < 12:
                time_slot = "morning"
            elif 12 <= hour < 18:
                time_slot = "afternoon"
            elif 18 <= hour < 24:
                time_slot = "evening"
            else:
                time_slot = "night"
            
            if time_slot not in profile.behavior.preferred_time_slots:
                profile.behavior.preferred_time_slots.append(time_slot)
        
        # Update learning progress
        if "language" in interaction_data and "success" in interaction_data:
            language = interaction_data["language"]
            success = interaction_data["success"]
            
            if language not in profile.behavior.learning_progress:
                profile.behavior.learning_progress[language] = 0.5
            
            # Update learning progress
            alpha = 0.05  # Small learning rate for skill level
            if success:
                profile.behavior.learning_progress[language] = min(1.0, 
                    profile.behavior.learning_progress[language] + alpha)
            else:
                profile.behavior.learning_progress[language] = max(0.0, 
                    profile.behavior.learning_progress[language] - alpha * 0.5)
        
        # Update skill level based on learning progress
        for language, progress in profile.behavior.learning_progress.items():
            profile.skill_level[language] = progress
        
        profile.last_updated = datetime.now()
        self._save_profile(profile)
    
    def _save_profile(self, profile: UserProfile):
        """Save profile to storage"""
        import os
        os.makedirs(self.storage_path, exist_ok=True)
        
        profile_data = {
            "user_id": profile.user_id,
            "preferences": asdict(profile.preferences),
            "behavior": asdict(profile.behavior),
            "created_at": profile.created_at.isoformat(),
            "last_updated": profile.last_updated.isoformat(),
            "skill_level": profile.skill_level
        }
        
        with open(os.path.join(self.storage_path, f"{profile.user_id}.json"), 'w') as f:
            json.dump(profile_data, f, indent=2)
    
    def get_personalized_response_config(self, user_id: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Get personalized response configuration based on user profile"""
        profile = self.get_profile(user_id)
        
        config = {
            "language": profile.preferences.preferred_language,
            "llm": profile.preferences.preferred_llm,
            "style": profile.preferences.code_style,
            "length": profile.preferences.response_length,
            "show_explanations": profile.preferences.show_explanations,
            "complexity_level": profile.preferences.complexity_tolerance
        }
        
        # Adjust based on context
        if "language" in context:
            config["language"] = context["language"]
        
        # Adjust complexity based on user skill level
        if "language" in context and context["language"] in profile.skill_level:
            skill_level = profile.skill_level[context["language"]]
            if skill_level > 0.8:
                config["complexity_level"] = min(1.0, config["complexity_level"] + 0.2)
            elif skill_level < 0.3:
                config["complexity_level"] = max(0.0, config["complexity_level"] - 0.2)
        
        return config
    
    def get_learning_recommendations(self, user_id: str) -> Dict[str, Any]:
        """Get personalized learning recommendations"""
        profile = self.get_profile(user_id)
        
        recommendations = {
            "languages_to_learn": [],
            "skills_to_improve": [],
            "practice_suggestions": [],
            "resource_recommendations": []
        }
        
        # Analyze skill levels
        for language, skill_level in profile.skill_level.items():
            if skill_level < 0.5:
                recommendations["languages_to_learn"].append({
                    "language": language,
                    "current_level": skill_level,
                    "reason": f"Your {language} skills need improvement"
                })
        
        # Analyze query patterns
        if profile.behavior.query_patterns:
            most_common_queries = sorted(profile.behavior.query_patterns.items(), 
                                       key=lambda x: x[1], reverse=True)[:3]
            
            for query_type, count in most_common_queries:
                if query_type == "debugging":
                    recommendations["skills_to_improve"].append({
                        "skill": "debugging",
                        "reason": "You frequently ask debugging questions"
                    })
                elif query_type == "optimization":
                    recommendations["skills_to_improve"].append({
                        "skill": "optimization",
                        "reason": "You frequently ask about optimization"
                    })
        
        # Generate practice suggestions based on feedback
        if profile.behavior.feedback_history:
            recent_feedback = profile.behavior.feedback_history[-10:]
            
            # Analyze feedback patterns
            negative_feedback = [f for f in recent_feedback if f.get("rating", 0) < 3]
            
            if negative_feedback:
                recommendations["practice_suggestions"].append({
                    "area": "general improvement",
                    "reason": "Recent feedback suggests areas for improvement"
                })
        
        # Resource recommendations based on skill levels
        for language, skill_level in profile.skill_level.items():
            if skill_level < 0.3:
                recommendations["resource_recommendations"].append({
                    "language": language,
                    "level": "beginner",
                    "resources": [
                        f"Interactive {language} tutorial",
                        f"{language} basics course"
                    ]
                })
            elif 0.3 <= skill_level < 0.7:
                recommendations["resource_recommendations"].append({
                    "language": language,
                    "level": "intermediate",
                    "resources": [
                        f"Advanced {language} patterns",
                        f"{language} best practices"
                    ]
                })
            else:
                recommendations["resource_recommendations"].append({
                    "language": language,
                    "level": "advanced",
                    "resources": [
                        f"{language} design patterns",
                        f"Advanced {language} techniques"
                    ]
                })
        
        return recommendations
    
    def analyze_user_clusters(self) -> Dict[str, Any]:
        """Analyze user behavior clusters for system improvements"""
        if not self.profiles:
            return {"error": "No user profiles available"}
        
        # Extract features for clustering
        features = []
        user_ids = []
        
        for user_id, profile in self.profiles.items():
            feature_vector = [
                profile.preferences.complexity_tolerance,
                profile.behavior.completion_acceptance_rate or 0.5,
                len(profile.behavior.query_patterns),
                len(profile.behavior.feedback_history),
                sum(profile.skill_level.values()) / len(profile.skill_level)
            ]
            features.append(feature_vector)
            user_ids.append(user_id)
        
        # Normalize features
        scaler = StandardScaler()
        features_normalized = scaler.fit_transform(features)
        
        # Cluster users
        kmeans = KMeans(n_clusters=3, random_state=42)
        clusters = kmeans.fit_predict(features_normalized)
        
        # Analyze clusters
        cluster_analysis = {}
        for cluster_id in range(3):
            cluster_users = [user_ids[i] for i, c in enumerate(clusters) if c == cluster_id]
            cluster_profiles = [self.profiles[uid] for uid in cluster_users]
            
            # Calculate cluster characteristics
            avg_complexity = np.mean([p.preferences.complexity_tolerance for p in cluster_profiles])
            avg_acceptance = np.mean([p.behavior.completion_acceptance_rate or 0.5 for p in cluster_profiles])
            avg_skill = np.mean([sum(p.skill_level.values()) / len(p.skill_level) for p in cluster_profiles])
            
            cluster_analysis[f"cluster_{cluster_id}"] = {
                "user_count": len(cluster_users),
                "avg_complexity_tolerance": avg_complexity,
                "avg_completion_acceptance": avg_acceptance,
                "avg_skill_level": avg_skill,
                "characteristics": self._describe_cluster(avg_complexity, avg_acceptance, avg_skill)
            }
        
        return {
            "cluster_analysis": cluster_analysis,
            "cluster_centers": kmeans.cluster_centers_.tolist(),
            "user_clusters": {user_ids[i]: int(clusters[i]) for i in range(len(user_ids))}
        }
    
    def _describe_cluster(self, complexity


=== core\plugin.py ===
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Type
from dataclasses import dataclass
import importlib
import logging
from pathlib import Path

# ---------- Core Definitions ----------
@dataclass
class PluginMetadata:
    name: str
    version: str
    author: str = "Unknown"
    compatible_versions: str = ">=0.1.0"
    required_config: Dict[str, Any] = None
    dependencies: List[str] = None
    description: str = ""

class PluginBase(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metadata = self.get_metadata()
        self._initialized = False
        self.logger = logging.getLogger(f"plugin.{self.metadata.name}")
        self._validate_config()

    # ---------- Required Interface ----------
    @abstractmethod
    def get_metadata(self) -> PluginMetadata:
        """Return plugin metadata"""
        pass

    @abstractmethod
    def initialize(self) -> bool:
        """Initialize plugin resources"""
        pass

    @abstractmethod
    def execute(self, input_data: Any) -> Any:
        """Main execution method"""
        pass

    # ---------- Core Functionality ----------
    def is_ready(self) -> bool:
        """Check if plugin is operational"""
        return self._initialized

    def cleanup(self):
        """Release all resources"""
        self._initialized = False
        self.logger.info(f"Cleanup completed for {self.metadata.name}")

    # ---------- Advanced Features ----------
    def _validate_config(self):
        """Validate configuration against metadata requirements"""
        if self.metadata.required_config:
            for field, expected_type in self.metadata.required_config.items():
                if field not in self.config:
                    raise ValueError(f"Missing config field: {field}")
                if not isinstance(self.config[field], expected_type):
                    raise TypeError(
                        f"Config field {field} requires {expected_type}, "
                        f"got {type(self.config[field])}"
                    )

    def health_check(self) -> Dict[str, Any]:
        """Detailed health report"""
        return {
            "name": self.metadata.name,
            "ready": self.is_ready(),
            "config_keys": list(self.config.keys()),
            "dependencies": self.metadata.dependencies or []
        }

    # ---------- Context Manager Support ----------
    def __enter__(self):
        if not self._initialized:
            self._initialized = self.initialize()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

# ---------- Plugin Manager ----------
class PluginManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.plugins: Dict[str, PluginBase] = {}
        self._discover_plugins()

    def _discover_plugins(self):
        """Discover and initialize all available plugins"""
        plugin_dir = Path(__file__).parent / "integrations"
        for py_file in plugin_dir.glob("*.py"):
            if py_file.stem == "__init__":
                continue
            
            try:
                module = importlib.import_module(
                    f"core.integrations.{py_file.stem}"
                )
                if hasattr(module, "Plugin"):
                    plugin_class = getattr(module, "Plugin")
                    if issubclass(plugin_class, PluginBase):
                        self._load_plugin(plugin_class)
            except Exception as e:
                logging.error(f"Failed to load {py_file.stem}: {str(e)}")

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Initialize and register a plugin"""
        plugin_name = plugin_class.__name__.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            plugin = plugin_class(plugin_config)
            if plugin.initialize():
                self.plugins[plugin_name] = plugin
                logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")

    def get_plugin(self, name: str) -> Optional[PluginBase]:
        """Retrieve a loaded plugin by name"""
        return self.plugins.get(name.lower())

    def list_plugins(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all plugins"""
        return {
            name: {
                "metadata": plugin.metadata,
                "ready": plugin.is_ready()
            }
            for name, plugin in self.plugins.items()
        }
    
    def reload_plugin(self, name: str) -> bool:
        """Hot-reload a plugin by name"""
        if name not in self.plugins:
            return False

        plugin = self.plugins[name]
        plugin.cleanup()
        
        try:
            module = importlib.import_module(f"core.integrations.{name}")
            importlib.reload(module)
            plugin_class = getattr(module, "Plugin")
            self._load_plugin(plugin_class)
            return True
        except Exception as e:
            logging.error(f"Failed to reload {name}: {str(e)}")
            return False

    def _resolve_dependencies(self, metadata: PluginMetadata) -> bool:
        """Install missing dependencies automatically"""
        if not metadata.dependencies:
            return True

        missing = []
        for dep in metadata.dependencies:
            try:
                req = requirements.Requirement(dep)
                importlib.import_module(req.name)
            except (ImportError, requirements.InvalidRequirement):
                missing.append(dep)

        if missing:
            logging.info(f"Installing dependencies: {', '.join(missing)}")
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", *missing],
                    stdout=subprocess.DEVNULL
                )
                return True
            except subprocess.CalledProcessError:
                logging.error(f"Failed to install dependencies: {missing}")
                return False
        return True

    def _check_version_compatibility(self, metadata: PluginMetadata) -> bool:
        """Verify plugin matches core version requirements"""
        try:
            core_req = requirements.Requirement(f"open_llm{metadata.compatible_versions}")
            current_version = requirements.Requirement(f"open_llm=={self.config['version']}")
            return current_version.specifier in core_req.specifier
        except requirements.InvalidRequirement:
            logging.warning(f"Invalid version spec: {metadata.compatible_versions}")
            return False

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Enhanced plugin loading with new features"""
        metadata = plugin_class({}).get_metadata()
        
        if not self._check_version_compatibility(metadata):
            logging.error(f"Version mismatch for {metadata.name}")
            return

        if not self._resolve_dependencies(metadata):
            logging.error(f"Missing dependencies for {metadata.name}")
            return

        plugin_name = metadata.name.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            with plugin_class(plugin_config) as plugin:
                if plugin.is_ready():
                    self.plugins[plugin_name] = plugin
                    logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")


=== core\prediction\cache.py ===
# core/prediction/cache.py
from typing import List, Dict
import numpy as np
from collections import deque

class CachePredictor:
    def __init__(self, context_manager, max_predictions=5):
        self.context = context_manager
        self.query_buffer = deque(maxlen=10)
        self.predictions = []
        
    def analyze_query_stream(self, new_query: str) -> List[str]:
        """Predict next 3 likely questions"""
        self.query_buffer.append(new_query)
        
        # 1. Get similar historical sequences
        similar_flows = self._find_similar_flows()
        
        # 2. Generate predictions (simplified example)
        return [
            "How to debug this?",
            "Better implementation?",
            "Related documentation"
        ][:max_predictions]

    def _find_similar_flows(self) -> List[Dict]:
        """Find similar query patterns in history"""
        # Implementation using your KnowledgeGraph
        return self.context.graph.find_similar_sequences(list(self.query_buffer))


=== core\prediction\warmer.py ===
# core/prediction/warmer.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

class CacheWarmer:
    def __init__(self, orchestrator, cache_predictor):
        self.orchestrator = orchestrator
        self.predictor = cache_predictor
        self.executor = ThreadPoolExecutor(2)

    async def warm_cache(self, current_query: str):
        """Pre-generate responses for predicted queries"""
        predicted = self.predictor.analyze_query_stream(current_query)
        
        # Run in background thread
        await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self._generate_responses,
            predicted
        )

    def _generate_responses(self, queries: List[str]):
        for query in queries:
            self.orchestrator.route_query(Query(content=query))


=== core\processing\batcher.py ===
# core/processing/batcher.py
from typing import List, Dict
import heapq
from dataclasses import dataclass, field
from sortedcontainers import SortedList

@dataclass(order=True)
class BatchItem:
    priority: int
    query: Dict = field(compare=False)
    created_at: float = field(default_factory=time.time, compare=False)

class AdaptiveBatcher:
    def __init__(self, max_batch_size=8, max_wait_ms=50):
        self.max_batch_size = max_batch_size
        self.max_wait = max_wait_ms / 1000
        self.pending = SortedList(key=lambda x: -x.priority)
        self.semaphore = asyncio.Semaphore(0)

    async def add_query(self, query: Dict, priority: int = 0) -> List[Dict]:
        """Add query to current batch, return completed batches if ready"""
        heapq.heappush(self.pending, BatchItem(priority, query))
        
        if len(self.pending) >= self.max_batch_size:
            return self._release_batch()
        
        await asyncio.wait_for(
            self.semaphore.acquire(),
            timeout=self.max_wait
        )
        return self._release_batch()

    def _release_batch(self) -> List[Dict]:
        """Extract queries for processing"""
        batch = [item.query for item in 
                heapq.nsmallest(self.max_batch_size, self.pending)]
        del self.pending[:len(batch)]
        return batch

    async def background_flush(self):
        """Periodically flush partial batches"""
        while True:
            await asyncio.sleep(self.max_wait)
            if self.pending:
                self.semaphore.release()


=== core\reasoning\engine.py ===
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from ..knowledge.graph import KnowledgeGraph
from ..context import ContextManager
import logging

class HybridEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph = context.graph
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.logger = logging.getLogger("reasoning.engine")
        self._init_rules()
        self.learning = SelfLearningEngine(context)
        self.performance = PerformanceTracker()
        self.query_hasher = QueryHasher()

    def _init_rules(self):
        """Load rule-based patterns"""
        self.rules = {
            'list_comp': {
                'pattern': '[x for x in {iterable} if {condition}]',
                'vars': ['iterable', 'condition']
            },
            'context_mgr': {
                'pattern': 'with {expr} as {var}:',
                'vars': ['expr', 'var']
            }
        }

    async def process(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """Main reasoning pipeline"""
        # Stage 1: Local Graph Check
        if graph_result := await self._check_graph(query):
            return graph_result

        # Stage 2: Rule Application
        if rule_result := self._apply_rules(query):
            return rule_result

        # Stage 3: LLM Fallback
        return await self._query_llm(query)
        
        result = await self._process_query(query)
        
        # Self-learning hook
        if result.get('success', True):
            self.learning.observe_solution(
                problem=query.get('code', ''),
                solution=str(result),
                source=result.get('source', 'llm')
            )
        
        return result
        
        query_hash = self.query_hasher.hash_query(query)
        recommended_source = self.performance.get_recommended_source(query_hash)
        
        # Route based on performance
        if recommended_source == 'graph':
            result = await self._check_graph(query)
        elif recommended_source == 'rule':
            result = self._apply_rules(query)
        else:
            result = await self._query_llm(query)

        # Record metrics
        self.performance.record_metric(
            source=result.get('source', 'llm'),
            latency=result['latency'],
            success=result.get('success', True),
            query_hash=query_hash
        )
        
        return result

    async def _check_graph(self, query: Dict) -> Optional[Dict]:
        """Check knowledge graph for solutions"""
        try:
            if 'code_context' in query:
                matches = self.graph.find_similar(
                    query['code_context'],
                    threshold=0.7
                )
                if matches:
                    return {'source': 'graph', 'result': matches[0]['solution']}
        except Exception as e:
            self.logger.error(f"Graph query failed: {str(e)}")
        return None

    def _apply_rules(self, query: Dict) -> Optional[Dict]:
        """Apply pre-defined coding patterns"""
        code = query.get('code', '')
        for rule_name, rule in self.rules.items():
            if all(var in code for var in rule['vars']):
                return {
                    'source': 'rule',
                    'rule': rule_name,
                    'template': rule['pattern'].format(**query)
                }
        return None

    async def _query_llm(self, query: Dict) -> Dict:
        """Route to best-suited LLM"""
        llm_pref = query.get('llm', self.context.config.get('default_llm'))
        return await self.context.plugin_manager.execute_llm(
            llm_pref,
            self._build_llm_payload(query)
        )

    def _build_llm_payload(self, query: Dict) -> Dict:
        """Enhance query with context"""
        return {
            **query,
            'context': self.context.get_relevant_context(query),
            'history': self.context.get_interaction_history()
        }


=== core\reasoning\rules.py ===
CODE_PATTERNS = {
    "list_comprehension": {
        "pattern": "[x for x in iterable if condition]",
        "transform": lambda match: {
            "template": match["pattern"],
            "variables": ["iterable", "condition"]
        }
    },
    "context_manager": {
        "pattern": "with expression as var:",
        "transform": lambda match: {
            "solution": f"with {match['expression']} as {match['var']}:"
        }
    }
}

class RuleEngine:
    @staticmethod
    def apply_pattern(code: str) -> Dict|None:
        """Match code against known patterns"""
        for pattern_name, pattern_data in CODE_PATTERNS.items():
            if pattern_data["pattern"] in code:
                return pattern_data["transform"](code)
        return None


=== core\refactoring\refactor_engine.py ===
# core/refactoring/refactor_engine.py
import ast
import re
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class RefactoringType(Enum):
    EXTRACT_FUNCTION = "extract_function"
    RENAME_VARIABLE = "rename_variable"
    SIMPLIFY_CONDITIONAL = "simplify_conditional"
    INTRODUCE_CONSTANT = "introduce_constant"
    REMOVE_DEAD_CODE = "remove_dead_code"
    ADD_TYPE_HINTS = "add_type_hints"

@dataclass
class RefactoringSuggestion:
    type: RefactoringType
    title: str
    description: str
    original_code: str
    suggested_code: str
    confidence: float
    line_range: tuple[int, int]
    effort: str  # "low", "medium", "high"

class RefactoringEngine:
    def __init__(self):
        self.patterns = {
            RefactoringType.EXTRACT_FUNCTION: self._detect_extract_function_opportunities,
            RefactoringType.RENAME_VARIABLE: self._detect_poor_variable_names,
            RefactoringType.SIMPLIFY_CONDITIONAL: self._detect_complex_conditionals,
            RefactoringType.INTRODUCE_CONSTANT: self._detect_magic_numbers,
            RefactoringType.REMOVE_DEAD_CODE: self._detect_dead_code,
            RefactoringType.ADD_TYPE_HINTS: self._detect_missing_type_hints
        }
    
    def analyze_code(self, code: str, language: str) -> List[RefactoringSuggestion]:
        """Analyze code and suggest refactorings"""
        suggestions = []
        
        try:
            if language == "python":
                tree = ast.parse(code)
                suggestions.extend(self._analyze_python_code(tree, code))
            elif language == "javascript":
                # Add JavaScript analysis logic
                pass
        except SyntaxError as e:
            print(f"Syntax error in code: {e}")
        
        return suggestions
    
    def _analyze_python_code(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Analyze Python code for refactoring opportunities"""
        suggestions = []
        
        # Check each refactoring pattern
        for refactoring_type, detector in self.patterns.items():
            new_suggestions = detector(tree, code)
            suggestions.extend(new_suggestions)
        
        # Sort by confidence and effort
        suggestions.sort(key=lambda x: (x.confidence, x.effort), reverse=True)
        return suggestions
    
    def _detect_extract_function_opportunities(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect opportunities to extract functions"""
        suggestions = []
        
        # Look for long functions (> 20 lines)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                func_lines = node.end_lineno - node.lineno
                if func_lines > 20:
                    # Find code blocks that could be extracted
                    extractable_blocks = self._find_extractable_blocks(node)
                    
                    for block in extractable_blocks:
                        suggestions.append(RefactoringSuggestion(
                            type=RefactoringType.EXTRACT_FUNCTION,
                            title=f"Extract function from {node.name}",
                            description=f"Extract {len(block)} lines into a separate function",
                            original_code=self._get_code_lines(code, block[0], block[1]),
                            suggested_code=self._generate_extracted_function(block, node.name),
                            confidence=0.8,
                            line_range=block,
                            effort="medium"
                        ))
        
        return suggestions
    
    def _detect_poor_variable_names(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect poorly named variables"""
        suggestions = []
        
        poor_name_patterns = [
            r'^[a-z]$',  # Single letter names
            r'^[a-z][0-9]+$',  # Names like x1, x2
            r'^temp$',  # Generic temp names
            r'^data$',  # Generic data names
        ]
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):
                var_name = node.id
                
                for pattern in poor_name_patterns:
                    if re.match(pattern, var_name):
                        suggestions.append(RefactoringSuggestion(
                            type=RefactoringType.RENAME_VARIABLE,
                            title=f"Rename variable '{var_name}'",
                            description=f"Variable name '{var_name}' is not descriptive",
                            original_code=var_name,
                            suggested_code=self._suggest_better_name(var_name, node),
                            confidence=0.7,
                            line_range=(node.lineno, node.lineno),
                            effort="low"
                        ))
                        break
        
        return suggestions
    
    def _detect_complex_conditionals(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect complex conditional statements"""
        suggestions = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.If):
                # Check for nested conditionals
                nested_count = self._count_nested_ifs(node)
                if nested_count > 2:
                    suggestions.append(RefactoringSuggestion(
                        type=RefactoringType.SIMPLIFY_CONDITIONAL,
                        title="Simplify complex conditional",
                        description=f"Conditional has {nested_count} levels of nesting",
                        original_code=self._get_code_lines(code, node.lineno, node.end_lineno),
                        suggested_code=self._simplify_conditional(node, code),
                        confidence=0.9,
                        line_range=(node.lineno, node.end_lineno),
                        effort="high"
                    ))
        
        return suggestions
    
    def _detect_magic_numbers(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect magic numbers that should be constants"""
        suggestions = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                # Skip common small numbers
                if node.value in [0, 1, 2, -1, 0.0, 1.0]:
                    continue
                
                # Check if it's used multiple times
                usage_count = self._count_number_usage(tree, node.value)
                if usage_count > 2:
                    suggestions.append(RefactoringSuggestion(
                        type=RefactoringType.INTRODUCE_CONSTANT,
                        title=f"Introduce constant for {node.value}",
                        description=f"Number {node.value} is used {usage_count} times",
                        original_code=str(node.value),
                        suggested_code=f"{self._suggest_constant_name(node.value)} = {node.value}",
                        confidence=0.8,
                        line_range=(node.lineno, node.lineno),
                        effort="low"
                    ))
        
        return suggestions
    
    def _detect_dead_code(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect dead code that can be removed"""
        suggestions = []
        
        # Look for unreachable code after return statements
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                return_statements = [n for n in ast.walk(node) if isinstance(n, ast.Return)]
                
                for return_stmt in return_statements:
                    if return_stmt.lineno < node.end_lineno:
                        # Code after return
                        suggestions.append(RefactoringSuggestion(
                            type=RefactoringType.REMOVE_DEAD_CODE,
                            title="Remove dead code",
                            description=f"Unreachable code after return statement on line {return_stmt.lineno}",
                            original_code=self._get_code_lines(code, return_stmt.lineno + 1, node.end_lineno),
                            suggested_code="",
                            confidence=1.0,
                            line_range=(return_stmt.lineno + 1, node.end_lineno),
                            effort="low"
                        ))
        
        return suggestions
    
    def _detect_missing_type_hints(self, tree: ast.AST, code: str) -> List[RefactoringSuggestion]:
        """Detect missing type hints in Python code"""
        suggestions = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Check function arguments
                missing_args = []
                for arg in node.args.args:
                    if arg.annotation is None:
                        missing_args.append(arg.arg)
                
                # Check return type
                missing_return = node.returns is None
                
                if missing_args or missing_return:
                    suggestions.append(RefactoringSuggestion(
                        type=RefactoringType.ADD_TYPE_HINTS,
                        title=f"Add type hints to {node.name}",
                        description=f"Function {node.name} is missing type hints",
                        original_code=self._get_function_signature(code, node),
                        suggested_code=self._add_type_hints(node, code),
                        confidence=0.6,
                        line_range=(node.lineno, node.lineno),
                        effort="low"
                    ))
        
        return suggestions
    
    # Helper methods for refactoring analysis
    def _find_extractable_blocks(self, func_node: ast.FunctionDef) -> List[tuple[int, int]]:
        """Find blocks of code that could be extracted into functions"""
        # Simplified implementation - in practice, this would be more sophisticated
        blocks = []
        current_block_start = None
        
        for node in ast.walk(func_node):
            if isinstance(node, (ast.For, ast.While, ast.If)):
                if current_block_start is None:
                    current_block_start = node.lineno
            elif isinstance(node, (ast.Return, ast.Break, ast.Continue)):
                if current_block_start is not None:
                    blocks.append((current_block_start, node.lineno))
                    current_block_start = None
        
        return blocks
    
    def _suggest_better_name(self, var_name: str, node: ast.Name) -> str:
        """Suggest a better variable name"""
        # Simple heuristic - in practice, use context analysis
        name_mapping = {
            'x': 'value',
            'y': 'result',
            'i': 'index',
            'j': 'counter',
            'temp': 'temporary',
            'data': 'input_data'
        }
        return name_mapping.get(var_name, f"{var_name}_descriptive")
    
    def _suggest_constant_name(self, value: Any) -> str:
        """Suggest a constant name for a magic number"""
        if isinstance(value, int):
            if value == 100:
                return "PERCENTAGE"
            elif value == 360:
                return "DEGREES_IN_CIRCLE"
            elif value > 1000:
                return "MAX_LIMIT"
        return f"CONSTANT_{value}"
    
    # Additional helper methods would be implemented here...


=== core\security\auth.py ===
# core/security/auth.py
from fastapi import HTTPException, status, Depends
from fastapi.security import APIKeyHeader
from typing import Optional
import secrets
import time
from collections import defaultdict, deque

API_KEY_NAME = "X-API-KEY"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

# Simple in-memory API key storage (use database in production)
VALID_API_KEYS = {"dev-key-123", "prod-key-456"}

# Rate limiting
class RateLimiter:
    def __init__(self, max_requests=100, window_seconds=60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = defaultdict(deque)
    
    def is_allowed(self, api_key: str) -> bool:
        now = time.time()
        key_requests = self.requests[api_key]
        
        # Remove old requests
        while key_requests and key_requests[0] <= now - self.window_seconds:
            key_requests.popleft()
        
        # Check if under limit
        if len(key_requests) >= self.max_requests:
            return False
        
        key_requests.append(now)
        return True

rate_limiter = RateLimiter()

async def get_api_key(api_key: Optional[str] = Depends(api_key_header)):
    if api_key not in VALID_API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API Key"
        )
    
    if not rate_limiter.is_allowed(api_key):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="Rate limit exceeded"
        )
    
    return api_key

# core/security/authorization.py
from enum import Enum
from typing import List, Set

class Permission(Enum):
    QUERY = "query"
    FEEDBACK = "feedback"
    ADMIN = "admin"

class Role:
    def __init__(self, permissions: Set[Permission]):
        self.permissions = permissions

# Role definitions
ROLES = {
    "user": Role({Permission.QUERY}),
    "premium_user": Role({Permission.QUERY, Permission.FEEDBACK}),
    "admin": Role({Permission.QUERY, Permission.FEEDBACK, Permission.ADMIN})
}

# User roles (in production, store in database)
USER_ROLES = {
    "user1": "user",
    "premium_user1": "premium_user",
    "admin1": "admin"
}

async def check_permission(api_key: str, required_permission: Permission):
    """Check if user has required permission"""
    username = USER_ROLES.get(api_key, "user")
    user_role = ROLES.get(username, Role(set()))
    
    if required_permission not in user_role.permissions:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions"
        )
    
    return True


=== core\security\rate_limiter.py ===
{\rtf1}# core/security/rate_limiter.py
import time
import asyncio
from typing import Dict, Optional, Tuple
from collections import defaultdict, deque
from dataclasses import dataclass
from enum import Enum

class RateLimitType(Enum):
    FIXED_WINDOW = "fixed_window"
    SLIDING_WINDOW = "sliding_window"
    TOKEN_BUCKET = "token_bucket"

@dataclass
class RateLimitRule:
    name: str
    limit: int  # Max requests
    window: int  # Time window in seconds
    type: RateLimitType
    burst: int = 1  # Burst capacity (for token bucket)

class AdvancedRateLimiter:
    def __init__(self):
        self.rules: Dict[str, RateLimitRule] = {}
        self.user_requests: Dict[str, Dict[str, deque]] = defaultdict(lambda: defaultdict(deque))
        self.user_tokens: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
        self.last_cleanup = time.time()
    
    def add_rule(self, name: str, limit: int, window: int, 
                 rate_type: RateLimitType = RateLimitType.SLIDING_WINDOW, burst: int = 1):
        """Add a rate limiting rule"""
        self.rules[name] = RateLimitRule(name, limit, window, rate_type, burst)
    
    async def check_rate_limit(self, user_id: str, rule_name: str) -> Tuple[bool, Optional[str]]:
        """Check if user is rate limited"""
        if rule_name not in self.rules:
            return True, None  # No rule = no limit
        
        rule = self.rules[rule_name]
        
        # Cleanup old data periodically
        if time.time() - self.last_cleanup > 60:  # Cleanup every minute
            self._cleanup_old_requests()
            self.last_cleanup = time.time()
        
        if rule.type == RateLimitType.FIXED_WINDOW:
            return await self._check_fixed_window(user_id, rule)
        elif rule.type == RateLimitType.SLIDING_WINDOW:
            return await self._check_sliding_window(user_id, rule)
        elif rule.type == RateLimitType.TOKEN_BUCKET:
            return await self._check_token_bucket(user_id, rule)
        
        return True, None
    
    async def _check_fixed_window(self, user_id: str, rule: RateLimitRule) -> Tuple[bool, Optional[str]]:
        """Check fixed window rate limiting"""
        now = time.time()
        window_start = now - rule.window
        
        # Remove old requests
        user_queue = self.user_requests[user_id][rule.name]
        while user_queue and user_queue[0] < window_start:
            user_queue.popleft()
        
        # Check if limit exceeded
        if len(user_queue) >= rule.limit:
            return False, f"Rate limit exceeded: {rule.limit} requests per {rule.window} seconds"
        
        # Add current request
        user_queue.append(now)
        return True, None
    
    async def _check_sliding_window(self, user_id: str, rule: RateLimitRule) -> Tuple[bool, Optional[str]]:
        """Check sliding window rate limiting"""
        now = time.time()
        window_start = now - rule.window
        
        # Remove old requests
        user_queue = self.user_requests[user_id][rule.name]
        while user_queue and user_queue[0] < window_start:
            user_queue.popleft()
        
        # Check if limit exceeded
        if len(user_queue) >= rule.limit:
            return False, f"Rate limit exceeded: {rule.limit} requests per {rule.window} seconds"
        
        # Add current request
        user_queue.append(now)
        return True, None
    
    async def _check_token_bucket(self, user_id: str, rule: RateLimitRule) -> Tuple[bool, Optional[str]]:
        """Check token bucket rate limiting"""
        now = time.time()
        tokens = self.user_tokens[user_id][rule.name]
        
        # Add tokens based on refill rate
        refill_rate = rule.limit / rule.window
        time_since_last_check = now - self.last_cleanup
        tokens = min(tokens + refill_rate * time_since_last_check, rule.burst)
        
        # Check if tokens available
        if tokens >= 1:
            self.user_tokens[user_id][rule.name] = tokens - 1
            return True, None
        else:
            return False, f"Rate limit exceeded: token bucket empty"
    
    def _cleanup_old_requests(self):
        """Clean up old request data"""
        now = time.time()
        
        for user_id, user_rules in self.user_requests.items():
            for rule_name, requests in user_rules.items():
                rule = self.rules.get(rule_name)
                if rule:
                    window_start = now - rule.window
                    while requests and requests[0] < window_start:
                        requests.popleft()
    
    def get_user_status(self, user_id: str) -> Dict[str, Dict[str, Any]]:
        """Get current rate limit status for a user"""
        status = {}
        
        for rule_name, rule in self.rules.items():
            user_requests = self.user_requests[user_id][rule_name]
            user_tokens = self.user_tokens[user_id][rule_name]
            
            now = time.time()
            window_start = now - rule.window
            
            # Count requests in current window
            recent_requests = sum(1 for req_time in user_requests if req_time >= window_start)
            
            status[rule_name] = {
                "rule_type": rule.type.value,
                "limit": rule.limit,
                "window": rule.window,
                "current_requests": recent_requests,
                "remaining_capacity": rule.limit - recent_requests,
                "tokens": user_tokens if rule.type == RateLimitType.TOKEN_BUCKET else None
            }
        
        return status

# Integration with FastAPI
from fastapi import Request, HTTPException, status
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-KEY")

rate_limiter = AdvancedRateLimiter()

# Configure rate limiting rules
rate_limiter.add_rule("api_requests", 100, 60, RateLimitType.SLIDING_WINDOW)
rate_limiter.add_rule("code_analysis", 10, 60, RateLimitType.TOKEN_BUCKET, burst=5)
rate_limiter.add_rule("multimodal_analysis", 5, 60, RateLimitType.FIXED_WINDOW)

async def get_rate_limit_user(request: Request):
    """Extract user identifier from request"""
    # In a real implementation, this would extract from JWT or API key
    return request.headers.get("X-API-KEY", "anonymous")

async def rate_limit_dependency(request: Request, rule_name: str = "api_requests"):
    """FastAPI dependency for rate limiting"""
    user_id = await get_rate_limit_user(request)
    is_allowed, message = await rate_limiter.check_rate_limit(user_id, rule_name)
    
    if not is_allowed:
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=message,
            headers={"Retry-After": "60"}
        )
    
    return user_id


=== core\self_healing.py ===
from dataclasses import dataclass
from enum import Enum, auto
import time
import asyncio
from typing import Dict, List, Optional
from modules.base_module import BaseModule

class HealthStatus(Enum):
    HEALTHY = auto()
    DEGRADED = auto()
    FAILED = auto()

@dataclass
class ModuleHealth:
    module_id: str
    status: HealthStatus
    last_checked: float
    failure_count: int = 0
    last_error: Optional[str] = None

class SelfHealingController:
    def __init__(self, registry):
        self.registry = registry
        self.health_status: Dict[str, ModuleHealth] = {}
        self._monitor_task = None
        
    async def start_monitoring(self, interval=60):
        """Start periodic health checks"""
        self._monitor_task = asyncio.create_task(self._monitor_loop(interval))
        
    async def _monitor_loop(self, interval):
        while True:
            await self.check_all_modules()
            await asyncio.sleep(interval)
            
    async def check_all_modules(self):
        """Check health of all registered modules"""
        for module_id, module in self.registry._instances.items():
            try:
                health_data = module.health_check()
                status = (
                    HealthStatus.DEGRADED if health_data.get("degraded", False) 
                    else HealthStatus.HEALTHY
                )
                self.health_status[module_id] = ModuleHealth(
                    module_id=module_id,
                    status=status,
                    last_checked=time.time()
                )
            except Exception as e:
                self._handle_module_failure(module_id, str(e))
                
    def _handle_module_failure(self, module_id: str, error: str):
        """Process module failure and initiate recovery"""
        if module_id not in self.health_status:
            self.health_status[module_id] = ModuleHealth(
                module_id=module_id,
                status=HealthStatus.FAILED,
                last_checked=time.time(),
                failure_count=1,
                last_error=error
            )
        else:
            self.health_status[module_id].failure_count += 1
            self.health_status[module_id].last_error = error
            self.health_status[module_id].status = HealthStatus.FAILED
            
        if self.health_status[module_id].failure_count > 3:
            self._attempt_recovery(module_id)
            
    def _attempt_recovery(self, module_id: str):
        """Execute recovery procedures for failed module"""
        module = self.registry._instances[module_id]
        try:
            # Attempt reinitialization
            module.initialize()
            self.health_status[module_id].status = HealthStatus.HEALTHY
            self.health_status[module_id].failure_count = 0
        except Exception as e:
            # If recovery fails, disable module temporarily
            self.health_status[module_id].status = HealthStatus.FAILED
            # TODO: Notify operators
            
    def get_available_modules(self) -> List[str]:
        """List modules currently available for routing"""
        return [
            module_id for module_id, health in self.health_status.items()
            if health.status != HealthStatus.FAILED
        ]


=== core\self_learning\engine.py ===
from typing import Dict, Any
from pathlib import Path
import json
import hashlib
from datetime import datetime
from ..knowledge.graph import KnowledgeGraph

class SelfLearningEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph: KnowledgeGraph = context.graph
        self.learned_rules_path = Path("data/learned_rules.json")
        self._init_storage()

    def _init_storage(self):
        """Ensure learning storage exists"""
        self.learned_rules_path.parent.mkdir(exist_ok=True)
        if not self.learned_rules_path.exists():
            with open(self.learned_rules_path, 'w') as f:
                json.dump({"rules": []}, f)

    def observe_solution(self, problem: str, solution: str, source: str):
        """Record successful solutions"""
        problem_hash = hashlib.sha256(problem.encode()).hexdigest()
        
        # Store in knowledge graph
        self.graph.cache_solution(
            problem=problem,
            solution=solution,
            metadata={
                "source": source,
                "timestamp": datetime.utcnow().isoformat(),
                "usage_count": 0
            }
        )
        
        # Auto-generate rules for pattern-like solutions
        if self._is_pattern_candidate(solution):
            self._extract_rule(problem, solution)

    def _is_pattern_candidate(self, solution: str) -> bool:
        """Check if solution is generalizable"""
        return (solution.count('\n') <= 2 and 
                solution.count('(') < 3 and 
                'for ' in solution or 'with ' in solution)

    def _extract_rule(self, problem: str, solution: str):
        """Convert solutions into reusable rules"""
        # Basic pattern extraction
        vars = {
            'iterable': self._find_between(solution, 'for ', ' in'),
            'var': self._find_between(solution, 'for ', ' in').split()[0]
        } if 'for ' in solution else {
            'expr': self._find_between(solution, 'with ', ' as'),
            'var': self._find_between(solution, 'as ', ':').strip()
        }
        
        new_rule = {
            "template": solution,
            "vars": list(vars.keys()),
            "source_problem": problem,
            "last_used": None,
            "success_rate": 1.0
        }
        
        self._save_rule(new_rule)

    def _save_rule(self, rule: Dict[str, Any]):
        """Persist learned rules"""
        with open(self.learned_rules_path, 'r+') as f:
            data = json.load(f)
            data["rules"].append(rule)
            f.seek(0)
            json.dump(data, f, indent=2)


=== core\self_learning\rule_applier.py ===
import ast
from typing import Dict, Any

class RuleApplier:
    @staticmethod
    def apply_learned_rules(code: str, rules: list) -> Dict[str, Any]:
        """Apply learned rules to code context"""
        try:
            tree = ast.parse(code)
            for rule in sorted(rules, key=lambda x: x['success_rate'], reverse=True):
                if RuleApplier._matches_pattern(tree, rule['template']):
                    return {
                        "solution": rule['template'],
                        "confidence": rule['success_rate'],
                        "source": "learned_rule"
                    }
        except SyntaxError:
            pass
        return {}

    @staticmethod
    def _matches_pattern(tree: ast.AST, template: str) -> bool:
        """Check if code matches rule pattern"""
        try:
            template_tree = ast.parse(template)
            return ast.dump(tree) == ast.dump(template_tree)
        except:
            return False


=== core\service.py ===
from fastapi import FastAPI, APIRouter, HTTPException, WebSocket
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
import uvicorn
import asyncio
from typing import Dict, Any
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field
from shared.schemas import FeedbackRating, FeedbackCorrection  # Your new schemas

# Core imports
from core.integrations.manager import IntegrationManager
from core.reasoning.engine import HybridEngine
from core.orchestrator import Orchestrator
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.visualization import KnowledgeVisualizer
from core.versioning import KnowledgeVersioner

# Module system
from modules.registry import ModuleRegistry
from shared.schemas import Query

class AIService:
    def __init__(self, config: Dict[str, Any]):
        self.load_balancer = LoadBalancer(self.monitor)
        asyncio.create_task(self._update_weights_loop())
        self.config = config
        self.integration_manager = IntegrationManager(config.get("plugins", {}))
        self.reasoning = HybridEngine(self.context)
        
        self.app = FastAPI(
            title="AI Code Assistant",
            version="0.6.0",  # Bumped version
            docs_url="/api-docs"
        )
        
        # Core systems
        self.registry = ModuleRegistry()
        self.context = ContextManager()
        self.healing = SelfHealingController(self.registry)
        self.orchestrator = Orchestrator(
            registry=self.registry,
            healing=self.healing,
            context=self.context,
            reasoning=self.reasoning  # New dependency
        )
        self.visualizer = KnowledgeVisualizer(self.context.graph)
        self.versioner = KnowledgeVersioner(self.context.graph)
        
        self._setup()
        
        from core.feedback.processor import FeedbackProcessor  # Lazy import
        self.feedback_processor = FeedbackProcessor(self.context)
        
    async def _update_weights_loop(self):
        while True:
            await asyncio.sleep(self.config["load_balancing"]["update_interval"])
            self.load_balancer.update_weights()
    
    async def process_query(self, query: Dict) -> Dict:
        """Enhanced processing pipeline"""
        return await self.reasoning.process(query)

    def _setup(self):
        """Initialize all components"""
        # Setup filesystem
        Path("static").mkdir(exist_ok=True)
        Path("templates").mkdir(exist_ok=True)
        
        # Initialize modules
        self.registry.discover_modules()
        for module in self.registry._instances.values():
            module.context = self.context
            module.initialize()
            
        # Start background services
        asyncio.create_task(self.healing.start_monitoring())
        
        # Setup routes
        self._setup_routes()
        self._mount_static()

    def _setup_routes(self):
        @self.app.post("/process")
        async def process(query: Query):
            """Main processing endpoint with hybrid reasoning"""
            try:
                return await self.orchestrator.route_query(query)
            except Exception as e:
                raise HTTPException(status_code=503, detail=str(e))
                
        # Knowledge endpoints
        knowledge_router = APIRouter(prefix="/knowledge")
        
        @knowledge_router.get("")
        async def get_knowledge(concept: str = None):
            if concept:
                return self.context.graph.find_semantic_matches(concept)
            return {
                "stats": {
                    "nodes": len(self.context.graph.graph.nodes()),
                    "edges": len(self.context.graph.graph.edges()),
                    "interactions": len(self.context.interaction_log)
                }
            }
        
        # Specialized endpoints
        @self.app.post("/debug")
        async def debug_code(query: Query):
            try:
                return await self.registry.get_module("debug").process(query)
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
                
        @self.app.get("/health")
        async def health_check():
            """Simplified health check"""
            return {
                "status": "healthy",
                "services": {
                    name: plugin.is_ready()
                    for name, plugin in self.integration_manager.plugins.items()
                }
            }

        self.app.include_router(knowledge_router)
        
        @self.app.get("/cost-monitoring")
        async def get_cost_metrics():
            return {
                "current": service.cost_monitor.current_spend,
                "forecast": service.cost_monitor.get_spend_forecast(),
                "budget": service.cost_monitor.config["monthly_budget"]
            }
            
        # ===== ADD FEEDBACK ROUTES HERE =====
        @self.app.post("/feedback/rate", tags=["Feedback"])
        async def record_rating(feedback: FeedbackRating):
            """Record explicit user ratings (1-5 stars)"""
            self.feedback_processor.process_feedback({
                'type': 'positive' if feedback.rating >= 3 else 'negative',
                'rating': feedback.rating / 5,  # Normalize to 0-1
                'query_node': feedback.query_hash,
                'response_node': feedback.response_hash,
                'user_comment': feedback.comment,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "rating_recorded"}

        @self.app.post("/feedback/correct", tags=["Feedback"])
        async def record_correction(feedback: FeedbackCorrection):
            """Handle factual corrections from users"""
            self.feedback_processor.process_feedback({
                'type': 'correction',
                'target_node': feedback.node_id,
                'corrected_info': feedback.corrected_content,
                'severity': feedback.severity,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "correction_applied"}

    def _mount_static(self):
        self.app.mount("/static", StaticFiles(directory="static"), name="static")
        
        @self.app.get("/")
        async def serve_ui():
            return FileResponse("templates/index.html")

    async def start_service(self, host: str = "0.0.0.0", port: int = 8000):
        """Corrected instance method"""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info"
        )
        server = uvicorn.Server(config)
        await server.serve()


=== core\signature_help.py ===
import re
from typing import Dict, List, Optional
from shared.schemas import SignatureHelp

class SignatureProvider:
    def __init__(self):
        self.patterns = {
            "python": r"def\s+(\w+)\s*\((.*?)\)",
            "javascript": r"function\s+(\w+)\s*\((.*?)\)",
            "csharp": r"public\s+\w+\s+(\w+)\s*\((.*?)\)"
        }

    def get_signature_help(self, code: str, language: str, cursor_pos: int) -> Optional[SignatureHelp]:
        """Extract function signature at cursor position"""
        matches = self._find_function_defs(code, language)
        current_func = self._get_function_at_pos(matches, cursor_pos)
        
        if current_func:
            params = self._parse_parameters(current_func[1])
            return SignatureHelp(
                name=current_func[0],
                parameters=params,
                active_parameter=self._get_active_param(current_func[1], cursor_pos)
            )
        return None

    def _find_function_defs(self, code: str, language: str) -> List[tuple]:
        """Find all function definitions in code"""
        pattern = self.patterns.get(language, self.patterns["python"])
        return re.findall(pattern, code, re.DOTALL)

    def _get_function_at_pos(self, functions: List[tuple], cursor_pos: int) -> Optional[tuple]:
        """Find which function contains the cursor position"""
        # Simplified - in reality would need AST parsing
        for func in functions:
            # Check if cursor is within function bounds
            if func[2] <= cursor_pos <= func[3]:  # (start_pos, end_pos)
                return func
        return None

    def _parse_parameters(self, param_str: str) -> List[Dict[str, str]]:
        """Parse parameter string into structured format"""
        params = []
        for p in param_str.split(','):
            p = p.strip()
            if p:
                parts = p.split()
                params.append({
                    "name": parts[-1],
                    "type": parts[0] if len(parts) > 1 else "any"
                })
        return params

    def _get_active_param(self, param_str: str, cursor_pos: int) -> int:
        """Determine which parameter is active based on cursor position"""
        if not param_str:
            return 0
        commas = [m.start() for m in re.finditer(',', param_str)]
        for i, pos in enumerate(commas):
            if cursor_pos <= pos:
                return i
        return len(commas)


=== core\state_manager.py ===
from typing import Dict, Any
from shared.schemas import Query, Response

class SessionState:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.context = {}
        self.history = []
        
    def update(self, query: Query, response: Response):
        self.history.append((query, response))
        self._update_context(query, response)
        
    def _update_context(self, query: Query, response: Response):
        """Extract and store relevant context"""
        self.context.update({
            "last_module": response.metadata.get("module"),
            "last_type": query.content_type
        })

class StateManager:
    def __init__(self):
        self.sessions: Dict[str, SessionState] = {}
        
    def get_session(self, session_id: str) -> SessionState:
        if session_id not in self.sessions:
            self.sessions[session_id] = SessionState(session_id)
        return self.sessions[session_id]


=== core\testing\test_generator.py ===
# core/testing/test_generator.py
import ast
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import subprocess
import tempfile
import os

class TestType(Enum):
    UNIT = "unit"
    INTEGRATION = "integration"
    FUNCTIONAL = "functional"

class TestFramework(Enum):
    PYTEST = "pytest"
    UNITTEST = "unittest"
    JEST = "jest"
    MOCHA = "mocha"

@dataclass
class TestCase:
    name: str
    type: TestType
    description: str
    code: str
    dependencies: List[str]
    setup: str
    teardown: str

@dataclass
class TestSuite:
    name: str
    framework: TestFramework
    test_cases: List[TestCase]
    imports: List[str]
    fixtures: List[str]

class TestGenerator:
    def __init__(self):
        self.code_analyzer = AdvancedCodeAnalyzer()
        self.test_templates = self._load_test_templates()
    
    def generate_tests(self, code: str, language: str, test_type: TestType = TestType.UNIT) -> TestSuite:
        """Generate test cases for given code"""
        if language == "python":
            return self._generate_python_tests(code, test_type)
        elif language == "javascript":
            return self._generate_javascript_tests(code, test_type)
        else:
            raise ValueError(f"Unsupported language: {language}")
    
    def _generate_python_tests(self, code: str, test_type: TestType) -> TestSuite:
        """Generate Python test cases"""
        # Parse the code to extract functions and classes
        try:
            tree = ast.parse(code)
        except SyntaxError:
            raise ValueError("Invalid Python code")
        
        test_cases = []
        
        # Extract functions and classes to test
        functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        
        # Generate test cases for functions
        for func in functions:
            test_case = self._generate_function_test(func, code, test_type)
            if test_case:
                test_cases.append(test_case)
        
        # Generate test cases for classes
        for cls in classes:
            class_test_cases = self._generate_class_tests(cls, code, test_type)
            test_cases.extend(class_test_cases)
        
        return TestSuite(
            name=f"test_{test_type.value}",
            framework=TestFramework.PYTEST,
            test_cases=test_cases,
            imports=["pytest", "unittest.mock"],
            fixtures=[]
        )
    
    def _generate_function_test(self, func: ast.FunctionDef, code: str, test_type: TestType) -> Optional[TestCase]:
        """Generate test case for a function"""
        func_name = func.name
        
        # Analyze function to determine test approach
        func_analysis = self._analyze_function(func, code)
        
        # Generate test based on function analysis
        if test_type == TestType.UNIT:
            test_code = self._generate_unit_test(func_name, func_analysis)
        elif test_type == TestType.INTEGRATION:
            test_code = self._generate_integration_test(func_name, func_analysis)
        else:
            test_code = self._generate_functional_test(func_name, func_analysis)
        
        return TestCase(
            name=f"test_{func_name}",
            type=test_type,
            description=f"Test for {func_name} function",
            code=test_code,
            dependencies=func_analysis["dependencies"],
            setup=func_analysis["setup"],
            teardown=func_analysis["teardown"]
        )
    
    def _analyze_function(self, func: ast.FunctionDef, code: str) -> Dict[str, Any]:
        """Analyze function to determine testing approach"""
        analysis = {
            "name": func.name,
            "args": [arg.arg for arg in func.args.args],
            "return_type": self._get_return_type(func),
            "dependencies": [],
            "setup": "",
            "teardown": "",
            "side_effects": False,
            "external_calls": []
        }
        
        # Check for external calls and dependencies
        for node in ast.walk(func):
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    analysis["external_calls"].append(node.func.id)
                elif isinstance(node.func, ast.Attribute):
                    analysis["external_calls"].append(f"{node.func.value.id}.{node.func.attr}")
        
        # Check for side effects
        side_effect_patterns = [
            r'\.write\(',
            r'\.read\(',
            r'\.append\(',
            r'\.remove\(',
            r'\.update\(',
            r'\.delete\(',
            r'print\(',
            r'input\('
        ]
        
        func_code = ast.get_source_segment(code, func)
        for pattern in side_effect_patterns:
            if re.search(pattern, func_code):
                analysis["side_effects"] = True
                break
        
        # Determine dependencies
        if analysis["external_calls"]:
            analysis["dependencies"] = list(set(analysis["external_calls"]))
        
        # Generate setup/teardown if needed
        if analysis["side_effects"]:
            analysis["setup"] = self._generate_setup_code(analysis)
            analysis["teardown"] = self._generate_teardown_code(analysis)
        
        return analysis
    
    def _get_return_type(self, func: ast.FunctionDef) -> str:
        """Extract return type from function"""
        if func.returns:
            return ast.unparse(func.returns)
        
        # Try to infer return type from return statements
        for node in ast.walk(func):
            if isinstance(node, ast.Return) and node.value:
                if isinstance(node.value, ast.Constant):
                    return type(node.value.value).__name__
                elif isinstance(node.value, ast.Name):
                    return "Any"
                elif isinstance(node.value, ast.List):
                    return "List"
                elif isinstance(node.value, ast.Dict):
                    return "Dict"
        
        return "None"
    
    def _generate_unit_test(self, func_name: str, analysis: Dict[str, Any]) -> str:
        """Generate unit test code"""
        test_code = f"""def test_{func_name}():
    # Arrange
    {self._generate_test_setup(analysis)}
    
    # Act
    result = {func_name}({self._generate_test_args(analysis)})
    
    # Assert
    {self._generate_test_assertions(analysis)}
"""
        return test_code
    
    def _generate_integration_test(self, func_name: str, analysis: Dict[str, Any]) -> str:
        """Generate integration test code"""
        test_code = f"""def test_{func_name}_integration():
    # Arrange
    {self._generate_test_setup(analysis)}
    
    # Act
    result = {func_name}({self._generate_test_args(analysis)})
    
    # Assert
    {self._generate_integration_assertions(analysis)}
"""
        return test_code
    
    def _generate_functional_test(self, func_name: str, analysis: Dict[str, Any]) -> str:
        """Generate functional test code"""
        test_code = f"""def test_{func_name}_functional():
    # Arrange
    {self._generate_test_setup(analysis)}
    
    # Act
    result = {func_name}({self._generate_test_args(analysis)})
    
    # Assert
    {self._generate_functional_assertions(analysis)}
"""
        return test_code
    
    def _generate_test_setup(self, analysis: Dict[str, Any]) -> str:
        """Generate test setup code"""
        setup_lines = []
        
        if analysis["setup"]:
            setup_lines.append(analysis["setup"])
        
        # Add mock setup for external dependencies
        for dep in analysis["dependencies"]:
            setup_lines.append(f"mock_{dep} = Mock()")
            setup_lines.append(f"mock_{dep}.return_value = None")
        
        return "\n    ".join(setup_lines) if setup_lines else "pass"
    
    def _generate_test_args(self, analysis: Dict[str, Any]) -> str:
        """Generate test arguments"""
        args = []
        
        for arg in analysis["args"]:
            if arg in ["self", "cls"]:
                continue
            
            # Generate sample values based on argument name
            if "file" in arg.lower():
                args.append('"test_file.txt"')
            elif "path" in arg.lower():
                args.append '"/test/path"'
            elif "url" in arg.lower():
                args.append('"https://example.com"')
            elif "count" in arg.lower() or "num" in arg.lower():
                args.append("10")
            elif "flag" in arg.lower() or "is_" in arg.lower():
                args.append("True")
            else:
                args.append('"test_value"')
        
        return ", ".join(args) if args else ""
    
    def _generate_test_assertions(self, analysis: Dict[str, Any]) -> str:
        """Generate test assertions"""
        assertions = []
        
        return_type = analysis["return_type"]
        
        if return_type == "None":
            assertions.append("# No return value to assert")
        elif return_type in ["int", "float"]:
            assertions.append("assert isinstance(result, (int, float))")
            assertions.append("assert result >= 0")
        elif return_type == "str":
            assertions.append("assert isinstance(result, str)")
            assertions.append("assert len(result) > 0")
        elif return_type == "bool":
            assertions.append("assert isinstance(result, bool)")
        elif return_type == "List":
            assertions.append("assert isinstance(result, list)")
            assertions.append("assert len(result) >= 0")
        elif return_type == "Dict":
            assertions.append("assert isinstance(result, dict)")
            assertions.append("assert len(result) >= 0")
        else:
            assertions.append("assert result is not None")
        
        return "\n    ".join(assertions)
    
    def _generate_integration_assertions(self, analysis: Dict[str, Any]) -> str:
        """Generate integration test assertions"""
        assertions = []
        
        assertions.append("# Integration test assertions")
        assertions.append("assert result is not None")
        
        if analysis["side_effects"]:
            assertions.append("# Verify side effects")
        
        return "\n    ".join(assertions)
    
    def _generate_functional_assertions(self, analysis: Dict[str, Any]) -> str:
        """Generate functional test assertions"""
        assertions = []
        
        assertions.append("# Functional test assertions")
        assertions.append("assert result is not None")
        assertions.append("# Verify expected behavior")
        
        return "\n    ".join(assertions)
    
    def _generate_setup_code(self, analysis: Dict[str, Any]) -> str:
        """Generate setup code for tests with side effects"""
        setup_lines = []
        
        if any(dep in analysis["external_calls"] for dep in ["open", "write", "read"]):
            setup_lines.append("test_file = 'test_temp.txt'")
            setup_lines.append("with open(test_file, 'w') as f:")
            setup_lines.append("    f.write('test content')")
        
        return "\n    ".join(setup_lines)
    
    def _generate_teardown_code(self, analysis: Dict[str, Any]) -> str:
        """Generate teardown code for tests with side effects"""
        teardown_lines = []
        
        if any(dep in analysis["external_calls"] for dep in ["open", "write", "read"]):
            teardown_lines.append("if os.path.exists('test_temp.txt'):")
            teardown_lines.append("    os.remove('test_temp.txt')")
        
        return "\n    ".join(teardown_lines)
    
    def _generate_class_tests(self, cls: ast.ClassDef, code: str, test_type: TestType) -> List[TestCase]:
        """Generate test cases for a class"""
        test_cases = []
        
        # Test class initialization
        test_cases.append(self._generate_class_init_test(cls, code, test_type))
        
        # Test class methods
        methods = [node for node in ast.walk(cls) if isinstance(node, ast.FunctionDef)]
        for method in methods:
            if not method.name.startswith('_'):  # Skip private methods
                test_case = self._generate_method_test(cls, method, code, test_type)
                if test_case:
                    test_cases.append(test_case)
        
        return test_cases
    
    def _generate_class_init_test(self, cls: ast.ClassDef, code: str, test_type: TestType) -> TestCase:
        """Generate test for class initialization"""
        class_name = cls.name
        
        # Get constructor arguments
        init_method = None
        for node in cls.body:
            if isinstance(node, ast.FunctionDef) and node.name == "__init__":
                init_method = node
                break
        
        args = []
        if init_method:
            args = [arg.arg for arg in init_method.args.args if arg.arg != "self"]
        
        test_code = f"""def test_{class_name}_init():
    # Arrange
    {self._generate_test_args_for_class(args)}
    
    # Act
    instance = {class_name}({', '.join(args)})
    
    # Assert
    assert instance is not None
    assert isinstance(instance, {class_name})
"""
        
        return TestCase(
            name=f"test_{class_name}_init",
            type=test_type,
            description=f"Test {class_name} class initialization",
            code=test_code,
            dependencies=[],
            setup="",
            teardown=""
        )
    
    def _generate_method_test(self, cls: ast.ClassDef, method: ast.FunctionDef, code: str, test_type: TestType) -> TestCase:
        """Generate test for class method"""
        class_name = cls.name
        method_name = method.name
        
        # Analyze method
        method_analysis = self._analyze_function(method, code)
        
        # Generate test code
        if test_type == TestType.UNIT:
            test_code = f"""def test_{class_name}_{method_name}():
    # Arrange
    instance = {class_name}()
    {self._generate_test_setup(method_analysis)}
    
    # Act
    result = instance.{method_name}({self._generate_test_args(method_analysis)})
    
    # Assert
    {self._generate_test_assertions(method_analysis)}
"""
        else:
            test_code = f"""def test_{class_name}_{method_name}_{test_type.value}():
    # Arrange
    instance = {class_name}()
    {self._generate_test_setup(method_analysis)}
    
    # Act
    result = instance.{method_name}({self._generate_test_args(method_analysis)})
    
    # Assert
    {self._generate_integration_assertions(method_analysis)}
"""
        
        return TestCase(
            name=f"test_{class_name}_{method_name}",
            type=test_type,
            description=f"Test {class_name}.{method_name} method",
            code=test_code,
            dependencies=method_analysis["dependencies"],
            setup=method_analysis["setup"],
            teardown=method_analysis["teardown"]
        )
    
    def _generate_test_args_for_class(self, args: List[str]) -> str:
        """Generate test arguments for class initialization"""
        arg_values = []
        
        for arg in args:
            if "file" in arg.lower():
                arg_values.append('"test_file.txt"')
            elif "path" in arg.lower():
                arg_values.append '"/test/path"'
            elif "url" in arg.lower():
                arg_values.append('"https://example.com"')
            elif "config" in arg.lower():
                arg_values.append("{{}}")
            else:
                arg_values.append('"test_value"')
        
        return "\n    ".join([f"{arg} = {value}" for arg, value in zip(args, arg_values)])
    
    def run_generated_tests(self, test_suite: TestSuite) -> Dict[str, Any]:
        """Run the generated tests and return results"""
        # Create temporary test file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(self._generate_test_file(test_suite))
            test_file_path = f.name
        
        try:
            # Run tests using pytest
            result = subprocess.run(
                ["pytest", test_file_path, "-v"],
                capture_output=True,
                text=True
            )
            
            # Parse results
            test_results = {
                "passed": 0,
                "failed": 0,
                "errors": 0,
                "output": result.stdout,
                "errors_output": result.stderr
            }
            
            # Parse pytest output
            for line in result.stdout.split('\n'):
                if "PASSED" in line:
                    test_results["passed"] += 1
                elif "FAILED" in line:
                    test_results["failed"] += 1
                elif "ERROR" in line:
                    test_results["errors"] += 1
            
            return test_results
        
        finally:
            # Clean up temporary file
            os.unlink(test_file_path)
    
    def _generate_test_file(self, test_suite: TestSuite) -> str:
        """Generate complete test file"""
        test_file_content = f"""# Generated tests for {test_suite.name}
import pytest
import unittest.mock
from unittest.mock import Mock
import os
import sys

# Add the source directory to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import the module to test
# from your_module import {', '.join([f.name for f in test_suite.test_cases if 'test_' in f.name])}

"""
        
        # Add test cases
        for test_case in test_suite.test_cases:
            test_file_content += f"\n{test_case.code}\n\n"
        
        return test_file_content
    
    def _load_test_templates(self) -> Dict[str, str]:
        """Load test templates for different languages and frameworks"""
        return {
            "python_pytest_unit": """
def test_{function_name}():
    # Arrange
    {setup}
    
    # Act
    result = {function_name}({args})
    
    # Assert
    {assertions}
""",
            "javascript_jest_unit": """
test('{function_name}', () => {{
    // Arrange
    {setup}
    
    // Act
    const result = {function_name}({args});
    
    // Assert
    {assertions}
}});
"""
        }


=== core\ux\enhanced_error_handler.py ===
# core/ux/enhanced_error_handler.py
from fastapi import Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from typing import Dict, Any, Optional
import traceback
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class EnhancedErrorHandler:
    def __init__(self):
        self.error_templates = {
            "validation_error": {
                "title": "Invalid Request",
                "message": "The request contains invalid data.",
                "suggestions": [
                    "Check your request format",
                    "Ensure all required fields are provided",
                    "Verify data types match expected formats"
                ]
            },
            "rate_limit_exceeded": {
                "title": "Too Many Requests",
                "message": "You've exceeded the rate limit.",
                "suggestions": [
                    "Wait a moment before trying again",
                    "Consider upgrading your plan for higher limits",
                    "Batch multiple requests into one"
                ]
            },
            "internal_error": {
                "title": "Internal Server Error",
                "message": "Something went wrong on our end.",
                "suggestions": [
                    "Please try again later",
                    "Contact support if the problem persists",
                    "Check our status page for system updates"
                ]
            }
        }
    
    async def handle_error(self, request: Request, error: Exception) -> JSONResponse:
        """Handle errors with enhanced user feedback"""
        error_id = str(id(request))
        timestamp = datetime.now().isoformat()
        
        # Log the error
        logger.error(f"Error {error_id}: {str(error)}")
        logger.error(traceback.format_exc())
        
        # Determine error type
        error_type = self._classify_error(error)
        template = self.error_templates.get(error_type, self.error_templates["internal_error"])
        
        # Create error response
        error_response = {
            "error": {
                "id": error_id,
                "type": error_type,
                "title": template["title"],
                "message": template["message"],
                "timestamp": timestamp,
                "path": str(request.url),
                "method": request.method,
                "suggestions": template["suggestions"]
            }
        }
        
        # Add debugging info for developers
        if isinstance(error, RequestValidationError):
            error_response["error"]["details"] = {
                "validation_errors": error.errors(),
                "body": error.body
            }
        
        # Set appropriate status code
        status_code = self._get_status_code(error)
        
        return JSONResponse(
            status_code=status_code,
            content=error_response,
            headers={"X-Error-ID": error_id}
        )
    
    def _classify_error(self, error: Exception) -> str:
        """Classify error type for appropriate handling"""
        if isinstance(error, RequestValidationError):
            return "validation_error"
        elif isinstance(error, HTTPException) and error.status_code == 429:
            return "rate_limit_exceeded"
        else:
            return "internal_error"
    
    def _get_status_code(self, error: Exception) -> int:
        """Get appropriate HTTP status code"""
        if isinstance(error, HTTPException):
            return error.status_code
        elif isinstance(error, RequestValidationError):
            return 422
        else:
            return 500

# Integration with FastAPI app
error_handler = EnhancedErrorHandler()

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return await error_handler.handle_error(request, exc)

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    return await error_handler.handle_error(request, exc)


=== core\validation\quality_gates.py ===
from typing import Dict, Any
import re
from shared.schemas import Response

class QualityValidator:
    def __init__(self, config: Dict[str, Any]):
        self.standards = config.get("quality_standards", {})
        self.compiled_rules = {
            "code_safety": re.compile(r"(eval\(|system\(|os\.popen)"),
            "min_complexity": float(self.standards.get("min_complexity", 0.3))
        }

    def validate(self, response: Response) -> Dict[str, Any]:
        """Run all quality checks"""
        checks = {
            "safety": self._check_code_safety(response.content),
            "complexity": self._check_complexity(response.content),
            "formatting": self._check_formatting(response.content)
        }
        
        return {
            "passed": all(checks.values()),
            "checks": checks,
            "original_response": response
        }

    def _check_code_safety(self, content: str) -> bool:
        """Block dangerous code patterns"""
        if "code" not in content:
            return True
        return not self.compiled_rules["code_safety"].search(content["code"])

    def _check_complexity(self, content: str) -> bool:
        """Ensure sufficient solution quality"""
        complexity = self._calculate_complexity(content)
        return complexity >= self.compiled_rules["min_complexity"]

    def _calculate_complexity(self, text: str) -> float:
        """Simple complexity heuristic (0-1 scale)"""
        lines = text.split('\n')
        return min(
            (len([l for l in lines if l.strip()]) * 0.1) +
            (len(re.findall(r"\b(for|while|def|class)\b", text)) * 0.3),
            1.0
        )

    def _check_formatting(self, content: str) -> bool:
        """Validate basic structure"""
        return bool(
            isinstance(content, (str, dict)) and 
            (not isinstance(content, dict) or "answer" in content)
        )


=== deploy\docker\docker-compose.yml ===
# deploy/docker/docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:password@db:5432/open_llm
    depends_on:
      - redis
      - db
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: open_llm
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
  prometheus_data:
  grafana_data:


=== docs\DEVELOPER_GUIDE.md ===
# docs/DEVELOPER_GUIDE.md

# Open LLM Code Assistant - Developer Guide

## Architecture Overview

The Open LLM Code Assistant is built with a modular architecture that separates concerns into distinct layers:

### Core Components

1. **Orchestration Layer** (`core/orchestrator.py`)
   - Main query processing pipeline
   - Routes requests to appropriate modules
   - Handles quality validation and response enhancement

2. **Integration Layer** (`core/integrations/`)
   - Plugin system for LLM providers
   - Supports multiple backends (Ollama, vLLM, HuggingFace, etc.)
   - Handles batching and rate limiting

3. **Context Management** (`core/context.py`)
   - Maintains knowledge graph
   - Tracks user interactions
   - Provides contextual information for queries

4. **Module System** (`modules/`)
   - Specialized processing modules
   - Language-specific functionality
   - Extensible plugin architecture

## Development Setup

### Prerequisites
- Python 3.8+
- Redis (for caching)
- GPU (for optimal performance with local models)

### Installation
```bash
git clone https://github.com/bozozeclown/open_llm.git
cd open_llm
pip install -r requirements.txt


=== modules\base_module.py ===
from abc import ABC, abstractmethod
from typing import List, Optional
from enum import Enum
from shared.schemas import Query, Response
from core.orchestrator import Capability

class BaseModule(ABC):
    MODULE_ID: str
    VERSION: str
    CAPABILITIES: List[Capability]
    PRIORITY: int = 0
    
    def __init__(self):
        self.context = None  # Will be set by service
        self._usage_count = 0
        
    @classmethod
    def get_metadata(cls) -> dict:
        return {
            "id": cls.MODULE_ID,
            "version": cls.VERSION,
            "capabilities": [cap.value for cap in cls.CAPABILITIES],
            "priority": cls.PRIORITY
        }
    
    async def initialize(self):
        """Initialize with module-specific knowledge"""
        self._load_domain_knowledge()
        self._ready = True
        
    @abstractmethod
    def _load_domain_knowledge(self):
        """Preload module-specific knowledge"""
        pass
        
    @abstractmethod
    async def process(self, query: Query) -> Response:
        """Process query using contextual knowledge"""
        pass
        
    def health_check(self) -> dict:
        """Report health including knowledge metrics"""
        return {
            "status": "ready" if self._ready else "loading",
            "version": self.VERSION,
            "usage": self._usage_count,
            "knowledge": self._get_knowledge_stats()
        }
        
    def _get_knowledge_stats(self) -> dict:
        """Get module-specific knowledge metrics"""
        if not self.context:
            return {}
            
        return {
            "nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n].get("module") == self.MODULE_ID
            ]),
            "relationships": len([
                e for e in self.context.graph.graph.edges()
                if self.context.graph.graph.edges[e].get("module") == self.MODULE_ID
            ])
        }


=== modules\module_ai.py ===
from modules.base_module import BaseModule
from core.integrations.manager import IntegrationManager

class AIModule(BaseModule):
    MODULE_ID = "ai_integration"
    CAPABILITIES = ["text_generation"]
    
    def __init__(self):
        self.integrations = {}
    
    async def initialize(self):
        # Initialize configured integrations
        self.integrations["ollama"] = IntegrationManager.get_integration("ollama")
        # Add others from config
        
    async def process(self, query: Query) -> Response:
        integration = self.integrations.get(query.metadata.get("integration"))
        if not integration:
            return Response.error("Integration not configured")
        
        try:
            result = integration.generate(
                query.content,
                **query.metadata.get("params", {})
            )
            return Response(content=result)
        except Exception as e:
            return Response.error(f"Generation failed: {str(e)}")


=== modules\module_completion.py ===
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.completion import CodeCompleter

class CompletionModule(BaseModule):
    MODULE_ID = "completion"
    CAPABILITIES = ["code_completion"]

    async def initialize(self):
        self.completer = CodeCompleter()

    async def process(self, query: Query) -> Response:
        completions = self.completer.generate_completions({
            "context": query.context.get("code", ""),
            "cursor_context": query.content
        })
        return Response(
            content="\n---\n".join(completions["completions"]),
            metadata={
                "type": "completion",
                "language": query.context.get("language", "unknown")
            }
        )


=== modules\module_debug.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.debugger import CodeDebugger

class DebugModule(BaseModule):
    MODULE_ID = "debug"
    CAPABILITIES = ["error_diagnosis", "fix_suggestion"]
    
    async def initialize(self):
        self.debugger = CodeDebugger(self.context.graph)
        
    async def process(self, query: Query) -> Response:
        if not query.context.get("error"):
            return Response(content="No error provided", metadata={})
            
        frames = self.debugger.analyze_traceback(
            query.context["code"],
            query.context["error"]
        )
        suggestions = self.debugger.suggest_fixes(frames)
        
        return Response(
            content=self._format_report(frames, suggestions),
            metadata={
                "frames": [f.__dict__ for f in frames],
                "suggestions": suggestions
            }
        )
        
    def _format_report(self, frames, suggestions) -> str:
        report = []
        for frame in frames:
            report.append(f"File {frame.file}, line {frame.line}:")
            report.append(f"Context:\n{frame.context}")
            report.append(f"Error: {frame.error}")
            if frame.line in suggestions:
                report.append("Suggestions:")
                report.extend(f"- {sug}" for sug in suggestions[frame.line])
            report.append("")
        return '\n'.join(report)


=== modules\module_generic.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query

class GenericCodeModule(BaseModule):
    MODULE_ID = "code_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all code requests"""
        return Response(
            content=f"Generic code processing: {query.content[:200]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True,
                "warning": "Primary module unavailable"
            },
            metrics={"generic_processing": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_code_processing"]
        }

class GenericChatModule(BaseModule):
    MODULE_ID = "chat_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all requests"""
        return Response(
            content=f"Generic response: {query.content[:150]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True
            },
            metrics={"generic_response": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_text_processing"]
        }


=== modules\module_python.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.orchestrator import Capability

class PythonModule(BaseModule):
    MODULE_ID = "python"
    VERSION = "0.2.0"
    CAPABILITIES = [
        Capability.CODE_COMPLETION,
        Capability.DEBUGGING,
        Capability.DOCSTRING
    ]
    PRIORITY = 10
    
    async def initialize(self):
        self._ready = True
        # Initialize with Python-specific knowledge
        self._init_python_knowledge()
        
    def _init_python_knowledge(self):
        """Preload Python-specific concepts"""
        python_concepts = [
            ("list", "mutable sequence"),
            ("dict", "key-value mapping"),
            ("generator", "iterator creator"),
            ("decorator", "function wrapper")
        ]
        
        for concept, desc in python_concepts:
            self.context.graph.add_entity(
                content=concept,
                type="python_concept",
                metadata={
                    "description": desc,
                    "language": "python"
                }
            )
        
    async def process(self, query: Query) -> Response:
        """Process Python queries with knowledge context"""
        # Extract context from query metadata
        context = query.context.get("knowledge_graph", {})
        
        # Generate response using contextual knowledge
        response_content = self._generate_response(query.content, context)
        
        return Response(
            content=response_content,
            metadata={
                "module": self.MODULE_ID,
                "capabilities": [cap.value for cap in self.CAPABILITIES],
                "context_used": bool(context)
            },
            metrics={"python_processing": 0.42}
        )
        
    def _generate_response(self, content: str, context: dict) -> str:
        """Generate response using available knowledge"""
        # Simplified response generation
        if "def " in content:
            return f"Python function suggestion based on {len(context.get('nodes', []))} related concepts..."
        return f"Python code solution referencing {context.get('edges', [])[:2]}..."
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "knowledge_nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n]['type'] == "python_concept"
            ])
        }
    
    async def process(self, query: Query) -> Response:
        """Enhanced processing with visualization support"""
        # Generate standard response
        response = await super().process(query)
        
        # Add visualization if requested
        if "visualize" in query.tags:
            graph_data = self._extract_relevant_subgraph(query.content)
            response.metadata["visualization"] = {
                "type": "knowledge_subgraph",
                "data": graph_data
            }
            
        return response
        
    def _extract_relevant_subgraph(self, content: str) -> dict:
        """Create a subgraph relevant to the query"""
        matches = self.context.graph.find_semantic_matches(content)
        if not matches:
            return {}
            
        central_node = matches[0]["node_id"]
        subgraph = nx.ego_graph(self.context.graph.graph, central_node, radius=2)
        
        return {
            "central_concept": self.context.graph.graph.nodes[central_node],
            "related": [
                {
                    "id": n,
                    "content": self.context.graph.graph.nodes[n]["content"],
                    "type": self.context.graph.graph.nodes[n]["type"],
                    "relations": [
                        {
                            "target": e[1],
                            "type": e[2]["type"],
                            "weight": e[2].get("weight", 1.0)
                        }
                        for e in subgraph.edges(n, data=True)
                    ]
                }
                for n in subgraph.nodes() if n != central_node
            ]
        }


=== modules\module_signature.py ===
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.signature_help import SignatureProvider

class SignatureModule(BaseModule):
    MODULE_ID = "signature"
    CAPABILITIES = ["signature_help"]

    async def initialize(self):
        self.provider = SignatureProvider()

    async def process(self, query: Query) -> Response:
        help_data = self.provider.get_signature_help(
            code=query.context.get("code", ""),
            language=query.context.get("language", "python"),
            cursor_pos=query.context.get("cursor_pos", 0)
        )
        return Response(
            content=help_data if help_data else "No signature found",
            metadata={"type": "signature_help"}
        )


=== modules\registry.py ===
import importlib
import inspect
from pathlib import Path
from typing import Dict, Type
from .base_module import BaseModule
from core.orchestrator import CapabilityRouter

class ModuleRegistry:
    def __init__(self):
        self._modules: Dict[str, Type[BaseModule]] = {}
        self._instances: Dict[str, BaseModule] = {}
        self.router = CapabilityRouter()
        
    def discover_modules(self, package="modules"):
        """Automatically discover and register all modules"""
        modules_dir = Path(__file__).parent
        
        for module_file in modules_dir.glob("module_*.py"):
            module_name = module_file.stem
            module = importlib.import_module(f"{package}.{module_name}")
            
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) and 
                    issubclass(obj, BaseModule) and 
                    obj != BaseModule):
                    self.register_module(obj)
    
    def register_module(self, module_class: Type[BaseModule]):
        """Register a single module class"""
        instance = module_class()
        self._modules[module_class.MODULE_ID] = module_class
        self._instances[module_class.MODULE_ID] = instance
        self.router.register_module(
            instance,
            module_class.CAPABILITIES,
            module_class.PRIORITY
        )
        return instance
        
    def get_module(self, module_id: str) -> BaseModule:
        return self._instances.get(module_id)


=== monitoring\alert_rules.yml ===
# monitoring/alert_rules.yml
groups:
  - name: open_llm_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(llm_requests_total{status="failed"}[5m]) / rate(llm_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | printf "%.2f" }} for the last 5 minutes"

      - alert: HighLatency
        expr: histogram_quantile(0.95, llm_response_latency_seconds_bucket) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response latency"
          description: "95th percentile latency is {{ $value }} seconds"

      - alert: LowCacheHitRate
        expr: cache_hit_ratio < 0.5
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit ratio is {{ $value | printf "%.2f" }}"


=== monitoring\dashboard.json ===
{
  "metrics": [
    {
      "title": "Requests/Min",
      "query": "rate(llm_requests_total[1m])",
      "type": "line"
    },
    {
      "title": "Latency (99p)",
      "query": "histogram_quantile(0.99, sum by(le)(rate(llm_response_latency_seconds_bucket[1m])))",
      "unit": "s"
    }
  ]
}


=== monitoring\prometheus.yml ===
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'open_llm'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'postgres'
    static_configs:
      - targets: ['db:5432']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093


=== package.json ===
{
  "name": "llm-code-assistant-ui",
  "version": "1.0.0",
  "scripts": {
    "build": "webpack --mode production",
    "dev": "webpack --watch --mode development",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "d3": "^7.8.5",
    "typescript": "^5.3.3"
  },
  "devDependencies": {
    "@types/d3": "^7.4.2",
    "css-loader": "^6.8.1",
    "mini-css-extract-plugin": "^2.7.6",
    "sass": "^1.69.5",
    "sass-loader": "^13.3.2",
    "ts-loader": "^9.5.1",
    "webpack": "^5.89.0",
    "webpack-cli": "^5.1.4"
  }
}


=== README.md ===
# Open LLM Code Assistant

An AI-powered coding assistant with hybrid reasoning, self-learning capabilities, and multi-LLM orchestration.

## ✅ Completed Features

### Core Architecture
- **Hybrid Reasoning Engine** - Combines rule-based patterns, knowledge graphs, and LLMs
- **Multi-LLM Integration** - Supports Ollama, vLLM, HuggingFace, Grok, and more
- **Adaptive Routing** - Dynamic load balancing and SLA-aware prioritization
- **Self-Learning** - Improves from user feedback and corrections
- **Quality Gates** - Automated validation of all responses
- **Predictive Caching** - Anticipates and pre-computes likely queries

### Advanced Capabilities
- **Multi-Modal Code Analysis** - Extract and analyze code from images/screenshots
- **Advanced Refactoring Engine** - Intelligent code improvement suggestions
- **Real-time Collaboration** - Live coding sessions with multiple users
- **VS Code Extension** - Seamless IDE integration
- **Comprehensive Analytics Dashboard** - Real-time metrics and insights
- **ML Model Management** - Automated model updates and versioning

### Security & Reliability
- **Authentication & Authorization** - API key-based access control
- **Rate Limiting** - Advanced throttling with multiple strategies
- **Circuit Breakers** - Resilient error handling and failover
- **Health Monitoring** - Comprehensive system health checks
- **Performance Optimization** - Database optimization and caching

### Testing & Quality
- **Comprehensive Test Suite** - Unit, integration, and performance tests
- **Database Optimization** - Efficient indexing and query optimization
- **Error Handling** - Enhanced user experience with detailed feedback
- **Production Deployment** - Docker containerization with monitoring

## 🚀 Installation

### Prerequisites
- Python 3.8+
- Redis (for caching)
- PostgreSQL (for analytics)
- Docker (optional, for containerized deployment)
- GPU (optional, for optimal performance with local models)

### Quick Start
```bash
git clone https://github.com/bozozeclown/open_llm.git
cd open_llm
pip install -r requirements.txt
```

### Configuration
1. Copy example configuration:
```bash
cp configs/integration.example.yaml configs/integration.yaml
```

2. Edit `configs/integration.yaml` to enable your preferred LLM providers:
```yaml
plugins:
  ollama:
    enabled: true
    config:
      base_url: "http://localhost:11434"
      default_model: "codellama"
  vllm:
    enabled: true
    config:
      model: "codellama/CodeLlama-7b-hf"
```

3. Set environment variables:
```bash
export GROQ_API_KEY="your_groq_api_key"
export HF_API_KEY="your_huggingface_api_key"
export TEXTGEN_API_KEY="your_textgen_api_key"
```

### Docker Deployment
```bash
# Build and run with Docker Compose
docker-compose up -d

# Access the application
# Web Interface: http://localhost:8000
# Analytics Dashboard: http://localhost:8000/analytics/dashboard
# Grafana: http://localhost:3000
```

## 📖 Usage

### Web Interface
Start the service:
```bash
python -m core.service
```
Access the web interface at `http://localhost:8000`

### API Usage
```python
from client import OpenLLMClient

client = OpenLLMClient()

# Basic code completion
response = client.query("How to reverse a list in Python?")
print(response.content)

# Code refactoring suggestions
suggestions = client.analyze_refactoring("your_code_here", "python")

# Multi-modal analysis
analysis = client.analyze_image("path/to/code/image.png")

# Real-time collaboration
session = client.create_session("My Coding Session", "print('Hello World')", "python")
```

### VS Code Extension
1. Install the Open LLM Code Assistant extension from the VS Code marketplace
2. Configure your API endpoint in VS Code settings:
```json
{
  "open-llm.apiUrl": "http://localhost:8000",
  "open-llm.apiKey": "your_api_key"
}
```

### Keyboard Shortcuts
- `Ctrl+Shift+C` (Windows/Linux) / `Cmd+Shift+C` (Mac) - Get code suggestion
- `Ctrl+Shift+R` (Windows/Linux) / `Cmd+Shift+R` (Mac) - Analyze refactoring opportunities

## 📊 Analytics Dashboard

Access the comprehensive analytics dashboard at `http://localhost:8000/analytics/dashboard` to monitor:

- **Usage Statistics**: Request trends, active users, success rates
- **Performance Metrics**: Response times, latency distribution
- **User Analytics**: Activity patterns, top users
- **Code Quality Trends**: Language distribution, refactoring patterns

## 🔧 Configuration

### Environment Variables
```bash
# API Keys
GROQ_API_KEY="your_groq_api_key"
HF_API_KEY="your_huggingface_api_key"
TEXTGEN_API_KEY="your_textgen_api_key"

# Database
DATABASE_URL="postgresql://user:password@localhost:5432/open_llm"
REDIS_URL="redis://localhost:6379"

# Security
SECRET_KEY="your_secret_key_here"
JWT_SECRET="your_jwt_secret_here"

# Monitoring
PROMETHEUS_ENABLED=true
GRAFANA_ENABLED=true
```

### Model Management
```python
from core.ml.model_manager import ModelManager

manager = ModelManager()

# Download and load models
await manager.download_model(ModelType.MULTIMODAL)
await manager.load_model(ModelType.MULTIMODAL)

# Check model status
model_info = manager.get_model_info(ModelType.MULTIMODAL)
print(f"Model status: {model_info.status}")
```

## 🧪 Testing

Run the comprehensive test suite:
```bash
# Run all tests
pytest tests/

# Run specific test categories
pytest tests/unit/
pytest tests/integration/
pytest tests/performance/

# Run with coverage
pytest --cov=core tests/
```

## 📈 Performance

The system is optimized for:
- **High Throughput**: 100+ requests per second
- **Low Latency**: <2s average response time
- **Memory Efficiency**: Optimized database queries and caching
- **Scalability**: Horizontal scaling with Docker and load balancing

## 🛠️ Development

### Project Structure
```
open_llm/
├── core/                    # Core application logic
│   ├── orchestrator.py      # Main query orchestrator
│   ├── integrations/        # LLM provider integrations
│   ├── multimodal/          # Multi-modal analysis
│   ├── refactoring/         # Code refactoring engine
│   ├── collaboration/       # Real-time collaboration
│   ├── analytics/           # Analytics dashboard
│   ├── ml/                  # Machine learning models
│   └── security/            # Authentication & rate limiting
├── modules/                 # Specialized processing modules
├── tests/                   # Test suite
├── deploy/                  # Deployment configuration
├── vscode-extension/         # VS Code extension
└── monitoring/             # Monitoring and alerting
```

### Contributing
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run pre-commit hooks
pre-commit install

# Start development server with hot reload
uvicorn core.service:app --reload
```

## 📋 TO DO

### ✅ Completed (Previous Phases)
- [x] Implement core architecture and orchestration
- [x] Add multi-LLM integration support
- [x] Implement self-learning and feedback processing
- [x] Add quality gates and validation
- [x] Implement caching and performance optimization
- [x] Add comprehensive error handling
- [x] Implement security layer (authentication, authorization)
- [x] Add testing infrastructure
- [x] Implement Docker containerization
- [x] Add monitoring and alerting
- [x] Implement multi-modal code analysis
- [x] Add advanced refactoring suggestions
- [x] Implement real-time collaboration features
- [x] Create VS Code extension
- [x] Build comprehensive analytics dashboard
- [x] Implement ML model management system
- [x] Add performance testing and optimization
- [x] Implement advanced rate limiting
- [x] Add database optimization

### 🚧 In Progress (Current Phase)
- [ ] Add mobile app support (React Native)
- [ ] Implement offline mode capabilities
- [ ] Add voice command support
- [ ] Create CLI tool for command-line usage

### 📋 Next Phase (Advanced Features)
- [ ] **Enterprise Features**
  - [ ] Add SSO integration (OAuth2, SAML)
  - [ ] Implement team management and permissions
  - [ ] Add audit logging and compliance features
  - [ ] Create enterprise deployment templates

- [ ] **Advanced AI Capabilities**
  - [ ] Implement code generation from natural language specifications
  - [ ] Add automated test generation
  - [ ] Implement bug prediction and prevention
  - [ ] Add code documentation generation

- [ ] **Ecosystem Integration**
  - [ ] Integrate with GitHub/GitLab for seamless workflow
  - [ ] Add Jira integration for issue tracking
  - [ ] Implement Slack/Teams bot integration
  - [ ] Create browser extension for web-based IDEs

- [ ] **Performance Enhancements**
  - [ ] Implement distributed caching cluster
  - [ ] Add horizontal scaling with Kubernetes
  - [ ] Implement edge caching for global users
  - [ ] Add request queuing for high-load scenarios

- [ ] **User Experience**
  - [ ] Add dark mode to web interface
  - [ ] Implement keyboard shortcuts customization
  - [ ] Add code snippet library
  - [ ] Create interactive tutorials and onboarding

### 🎯 Future Roadmap
- **Q1 2025**: Enterprise features and mobile app
- **Q2 2025**: Advanced AI capabilities and ecosystem integration
- **Q3 2025**: Performance enhancements and user experience improvements
- **Q4 2025**: Community features and plugin marketplace

## 🤝 Community

- **Documentation**: [Wiki](https://github.com/bozozeclown/open_llm/wiki)
- **Issues**: [GitHub Issues](https://github.com/bozozeclown/open_llm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/bozozeclown/open_llm/discussions)
- **Discord**: [Community Server](https://discord.gg/5VEMNdsyYs)

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 👏 Acknowledgments

- **Open Source Community**: For the amazing libraries and tools that make this project possible
- **Contributors**: Everyone who has helped shape this project
- **Early Adopters**: For providing valuable feedback and suggestions

---

**Built with ❤️ by the Open LLM community**


=== requirements.txt ===
torch>=2.0.1
transformers>=4.30.0
accelerate>=0.20.0
datasets>=2.12.0
peft>=0.4.0
wandb>=0.15.0
fastapi>=0.95.0
uvicorn>=0.22.0


=== shared\config\init.py ===
import yaml
from pathlib import Path
from typing import Any, Dict

class ConfigManager:
    _config: Dict[str, Any] = {}
    
    @classmethod
    def load_configs(cls, config_dir: str = "shared/config"):
        config_path = Path(config_dir)
        
        for config_file in config_path.glob("*.yaml"):
            with open(config_file) as f:
                cls._config[config_file.stem] = yaml.safe_load(f)
                
    @classmethod
    def get(cls, key: str, default: Any = None) -> Any:
        keys = key.split(".")
        value = cls._config
        
        for k in keys:
            value = value.get(k)
            if value is None:
                return default
                
        return value


=== shared\config\loader.py ===
# shared/config/loader.py
import watchdog.events
import yaml

class ConfigWatcher(watchdog.events.FileSystemEventHandler):
    def __init__(self, callback):
        self.callback = callback
        
    def on_modified(self, event):
        if event.src_path.endswith('.yaml'):
            self.callback(event.src_path)

# Enhanced ConfigManager
class ConfigManager:
    def __init__(self):
        self._callbacks = []
        self.load_configs()
        
    def register_callback(self, callback):
        self._callbacks.append(callback)
        
    def _notify_changes(self, changed_file):
        for callback in self._callbacks:
            callback(self, changed_file)
    
    def load_config():
    """Safe config reload that preserves existing connections"""
    new_config = yaml.safe_load(open("configs/integration.yaml"))
    for key in current_config:
        if key in new_config:
            current_config[key].update(new_config[key])


=== shared\knowledge\graph.py ===
from dataclasses import dataclass
from typing import Dict, List, Set, Optional
import networkx as nx
from enum import Enum
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
import spacy

class EntityType(Enum):
    CONCEPT = "concept"
    CODE = "code"
    API = "api"
    LIBRARY = "library"
    ERROR = "error"
    PATTERN = "pattern"

@dataclass
class KnowledgeNode:
    id: str
    type: EntityType
    content: str
    metadata: dict
    embeddings: Optional[np.ndarray] = None

class KnowledgeGraph:
    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.nlp = spacy.load("en_core_web_sm")
        self._setup_indices()
        
    def _setup_indices(self):
        """Initialize data structures for efficient lookup"""
        self.content_index = {}  # content -> node_id
        self.embedding_index = []  # List of (node_id, embedding)
        
    def _generate_id(self, content: str, type: EntityType) -> str:
        """Create deterministic node ID"""
        return hashlib.sha256(f"{type.value}:{content}".encode()).hexdigest()
        
    def add_entity(self, content: str, type: EntityType, metadata: dict = None) -> str:
        """Add or update an entity with enhanced NLP processing"""
        # Preprocess content
        doc = self.nlp(content)
        normalized_content = " ".join([token.lemma_ for token in doc if not token.is_stop])
        
        node_id = self._generate_id(normalized_content, type)
        embedding = self.encoder.encode(normalized_content)
        
        if node_id not in self.graph:
            self.graph.add_node(node_id, 
                type=type,
                content=content,
                normalized=normalized_content,
                metadata=metadata or {},
                embedding=embedding
            )
            self.content_index[normalized_content] = node_id
            self.embedding_index.append((node_id, embedding))
        else:
            # Update existing node
            self.graph.nodes[node_id]['metadata'].update(metadata or {})
            
        return node_id
        
    def add_relation(self, source_id: str, target_id: str, relation_type: str, weight: float = 1.0):
        """Create weighted relationship between entities"""
        if source_id in self.graph and target_id in self.graph:
            self.graph.add_edge(source_id, target_id, 
                type=relation_type,
                weight=weight
            )
            
    def find_semantic_matches(self, query: str, threshold: float = 0.7) -> List[dict]:
        """Find knowledge nodes semantically similar to query"""
        query_embedding = self.encoder.encode(query)
        matches = []
        
        for node_id, emb in self.embedding_index:
            similarity = np.dot(query_embedding, emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(emb)
            )
            if similarity > threshold:
                matches.append({
                    "node_id": node_id,
                    "similarity": float(similarity),
                    **self.graph.nodes[node_id]
                })
                
        return sorted(matches, key=lambda x: x["similarity"], reverse=True)
        
    def expand_from_text(self, text: str, source: str = "user"):
        """Automatically extract and add knowledge from text"""
        doc = self.nlp(text)
        
        # Extract entities and noun phrases
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        noun_chunks = [(chunk.text, "NOUN_PHRASE") for chunk in doc.noun_chunks]
        
        # Add to knowledge graph
        nodes = []
        for content, label in entities + noun_chunks:
            node_id = self.add_entity(
                content=content,
                type=self._map_spacy_label(label),
                metadata={"source": source}
            )
            nodes.append(node_id)
            
        # Create relationships based on syntactic dependencies
        for sent in doc.sents:
            for token in sent:
                if token.dep_ in ("dobj", "nsubj", "attr"):
                    source = self._get_node_for_token(token.head)
                    target = self._get_node_for_token(token)
                    if source and target:
                        self.add_relation(source, target, token.dep_)
    
    def _map_spacy_label(self, label: str) -> EntityType:
        """Map Spacy labels to our entity types"""
        mapping = {
            "PERSON": "concept",
            "ORG": "concept",
            "GPE": "concept",
            "PRODUCT": "api",
            "NOUN_PHRASE": "concept"
        }
        return EntityType(mapping.get(label, "concept"))
        
    def _get_node_for_token(self, token) -> Optional[str]:
        """Find or create node for a Spacy token"""
        text = token.lemma_
        return self.content_index.get(text)
        
    def get_statistics(self) -> dict:
        """Return comprehensive graph statistics"""
        centrality = nx.degree_centrality(self.graph)
        top_nodes = sorted(
            [(n, c) for n, c in centrality.items()],
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            "basic": {
                "nodes": len(self.graph.nodes()),
                "edges": len(self.graph.edges()),
                "components": nx.number_weakly_connected_components(self.graph)
            },
            "centrality": {
                "top_concepts": [
                    {"id": n[0], "content": self.graph.nodes[n[0]]["content"], "score": n[1]}
                    for n in top_nodes
                ]
            },
            "types": {
                nt: sum(1 for n in self.graph.nodes() if self.graph.nodes[n]["type"] == nt)
                for nt in set(nx.get_node_attributes(self.graph, "type").values())
            }
        }

    def export_gexf(self, path: str):
        """Export graph to GEXF format for external tools"""
        nx.write_gexf(self.graph, path)
        
    def find_code_patterns(self, pattern_type: str) -> list:
        """Enhanced pattern matching for common code structures"""
        return self._query_graph(
            f"""
            MATCH (n:CodePattern {{type: $pattern_type}})
            RETURN n
            """,
            {"pattern_type": pattern_type}
        )
    
    def cache_solution(self, problem_hash: str, solution: Dict):
        """Store successful solutions for future reuse"""
        self._store_node("Solution", {
            "hash": problem_hash,
            "solution": solution
        })
        
    def find_similar(self, code_snippet: str, threshold: float = 0.8):
        """Find similar code patterns using vector similarity"""
        query_embedding = self.embedder.encode(code_snippet)
        results = []
        
        for node in self.graph.nodes(data=True):
            if 'embedding' in node[1]:
                similarity = cosine_similarity(
                    query_embedding,
                    node[1]['embedding']
                )
                if similarity > threshold:
                    results.append({
                        'node': node[0],
                        'similarity': similarity,
                        'solution': node[1].get('solution')
                    })
        
        return sorted(results, key=lambda x: x['similarity'], reverse=True)

    def cache_solution(self, problem: str, solution: str):
        """Store successful solutions with embeddings"""
        embedding = self.embedder.encode(problem)
        self.graph.add_node(problem, solution=solution, embedding=embedding)



=== shared\schemas.py ===
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

class CompletionRequest(BaseModel):
    context: str  # Full file content
    cursor_context: str  # Line/fragment near cursor
    
class SignatureHelp(BaseModel):
    name: str
    parameters: List[Dict[str, str]]
    active_parameter: int

class SignatureRequest(BaseModel):
    code: str
    language: str
    cursor_pos: int
    
class HealthStatus(BaseModel):
    service: str
    status: Literal["online", "degraded", "offline"]
    models: List[str] = []
    latency: Optional[float]

class IntegrationConfig(BaseModel):
    priority: int
    timeout: int = 30
    
class Query(BaseModel):
    """Enhanced query class with routing support"""
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    # Add these new methods
    def with_additional_context(self, reasoning_data: Dict) -> 'Query':
        """
        Create a new query instance with reasoning context
        Usage:
            enriched_query = original_query.with_additional_context(
                {"source": "graph", "confidence": 0.9}
            )
        """
        return self.copy(
            update={
                "metadata": {
                    **self.metadata,
                    "reasoning": reasoning_data
                }
            }
        )
    
    @property
    def preferred_provider(self) -> Optional[str]:
        """Get preferred LLM provider if specified"""
        return self.metadata.get('preferred_provider')
    
    @preferred_provider.setter
    def preferred_provider(self, provider: str):
        """Set preferred LLM provider"""
        self.metadata['preferred_provider'] = provider
        
class FeedbackRating(BaseModel):
    query_hash: str
    response_hash: str
    rating: float = Field(..., ge=0, le=5)
    comment: Optional[str]
    
class FeedbackCorrection(BaseModel):
    node_id: str
    corrected_content: str
    severity: Literal["low", "medium", "high"] = "medium"


=== static\css\debugger.css ===
.debug-panel {
    border: 1px solid #e1e4e8;
    border-radius: 6px;
    padding: 16px;
    margin-top: 20px;
    background: #f6f8fa;
}

.debug-frame {
    margin-bottom: 20px;
    padding: 15px;
    background: white;
    border-radius: 4px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.debug-frame pre {
    background: #f0f0f0;
    padding: 8px;
    border-radius: 3px;
    overflow-x: auto;
}

.var {
    display: inline-block;
    background: #e1f5fe;
    padding: 2px 6px;
    border-radius: 3px;
    margin-right: 5px;
    font-family: monospace;
}

.suggestions ul {
    margin: 5px 0 0 20px;
    padding: 0;
}

.suggestions li {
    margin: 5px 0;
}


=== static\css\graph.css ===
/* Layout */
#graph-explorer-container {
    width: 100%;
    height: 800px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.controls {
    padding: 12px;
    background: #f5f5f5;
    margin-bottom: 12px;
    border-radius: 4px;
    display: flex;
    gap: 12px;
    align-items: center;
}

.controls button {
    padding: 6px 12px;
    background-color: #2962FF;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-size: 14px;
}

.controls button:hover {
    background-color: #1E4DCC;
}

.controls label {
    display: flex;
    align-items: center;
    gap: 6px;
    font-size: 14px;
}

/* Graph Canvas */
#graph-canvas {
    width: 100%;
    height: calc(100% - 120px);
    border: 1px solid #ddd;
    border-radius: 4px;
    background: white;
}

/* Graph Elements */
.graph-node {
    cursor: pointer;
    stroke: white;
    stroke-width: 1.5px;
}

.graph-node:hover {
    stroke-width: 2px;
}

.graph-link {
    stroke: #888;
    stroke-opacity: 0.6;
}

.node-label {
    pointer-events: none;
    user-select: none;
    font-family: 'Segoe UI', sans-serif;
}

/* Analytics Panel */
#graph-analytics {
    margin-top: 12px;
    padding: 12px;
    background: #f5f5f5;
    border-radius: 4px;
}

#graph-analytics h3 {
    margin: 0 0 8px 0;
    color: #2962FF;
    font-size: 16px;
}

.metric {
    margin-bottom: 8px;
}

.metric-label {
    font-weight: bold;
    color: #FF6D00;
    margin-right: 8px;
}

.metric-values {
    display: inline-flex;
    gap: 12px;
}


=== static\css\signature.css ===
.signature-tooltip {
    position: fixed;
    background: #2d2d2d;
    color: white;
    padding: 8px 12px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 14px;
    z-index: 1000;
    box-shadow: 0 2px 8px rgba(0,0,0,0.2);
    max-width: 500px;
}

.signature-title {
    color: #569cd6;
}

.active-param {
    color: #9cdcfe;
    font-weight: bold;
}

.signature-tooltip span {
    margin: 0 2px;
}


=== static\js\completion.js ===
class CompletionUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.setupListeners();
    }

    setupListeners() {
        this.editor.addEventListener("keydown", async (e) => {
            if (e.key === "Tab" || (e.key === " " && e.ctrlKey)) {
                e.preventDefault();
                const completions = await this.fetchCompletions();
                this.showCompletions(completions);
            }
        });
    }

    async fetchCompletions() {
        const response = await fetch("/completion", {
            method: "POST",
            body: JSON.stringify({
                content: this.getCursorContext(),
                context: {
                    code: this.editor.value,
                    language: "python"  # Dynamic in real impl
                }
            })
        });
        return await response.json();
    }

    getCursorContext() {
        const cursorPos = this.editor.selectionStart;
        return this.editor.value.substring(
            Math.max(0, cursorPos - 50), 
            cursorPos
        );
    }

    showCompletions(completions) {
        // Render as dropdown or inline suggestions
        console.log("Suggestions:", completions);
    }
}


=== static\js\debugger.js ===
class DebuggerUI {
    constructor() {
        this.container = document.getElementById('debug-container');
        this.setupUI();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="debug-panel">
                <h3>Debug Assistant</h3>
                <div class="debug-frames"></div>
                <button id="analyze-btn">Analyze Error</button>
            </div>
        `;
        
        document.getElementById('analyze-btn').addEventListener('click', () => this.analyzeError());
    }

    async analyzeError() {
        const code = document.getElementById('code-editor').value;
        const error = document.getElementById('error-output').value;
        
        const response = await fetch('/debug', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                content: "debug_request",
                context: { code, error }
            })
        });
        
        const result = await response.json();
        this.displayResults(result);
    }

    displayResults(debugData) {
        const framesContainer = document.querySelector('.debug-frames');
        framesContainer.innerHTML = debugData.metadata.frames.map(frame => `
            <div class="debug-frame">
                <h4>${frame.file}:${frame.line}</h4>
                <pre>${frame.context}</pre>
                <div class="variables">${this.formatVariables(frame.variables)}</div>
                ${this.formatSuggestions(debugData.metadata.suggestions[frame.line] || [])}
            </div>
        `).join('');
    }

    formatVariables(vars) {
        return Object.entries(vars).map(([k, v]) => 
            `<span class="var">${k}=${v}</span>`
        ).join(' ');
    }

    formatSuggestions(suggestions) {
        if (!suggestions.length) return '';
        return `
            <div class="suggestions">
                <h5>Suggestions:</h5>
                <ul>${suggestions.map(s => `<li>${s}</li>`).join('')}</ul>
            </div>
        `;
    }
}


=== static\js\graph-explorer.js ===
// Configuration
const CONFIG = {
    nodeColors: {
        concept: '#FF6D00',
        code: '#2962FF',
        api: '#00C853',
        error: '#D50000',
        default: '#666666'
    },
    apiBaseUrl: '/knowledge/graph'
};

class GraphExplorer {
    constructor(containerId) {
        this.container = document.getElementById(containerId);
        this.simulation = null;
        this.currentGraph = { nodes: [], edges: [] };
        this.init();
    }

    async init() {
        this.setupUI();
        await this.loadGraph();
        this.setupEventListeners();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="controls">
                <button id="refresh-graph">Refresh</button>
                <label>
                    Depth: <input type="range" id="graph-depth" min="1" max="3" value="2">
                </label>
                <label>
                    Physics: <input type="checkbox" id="toggle-physics" checked>
                </label>
                <button id="analyze-btn">Run Analysis</button>
            </div>
            <div id="graph-canvas"></div>
            <div id="graph-analytics"></div>
        `;
    }

    async loadGraph(depth = 2, physics = true) {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/json?depth=${depth}`);
            this.currentGraph = await response.json();
            this.renderGraph(physics);
        } catch (error) {
            console.error('Failed to load graph:', error);
        }
    }

    renderGraph(enablePhysics) {
        const canvas = document.getElementById('graph-canvas');
        canvas.innerHTML = '';
        
        const width = canvas.clientWidth;
        const height = canvas.clientHeight;
        
        const svg = d3.select(canvas)
            .append('svg')
            .attr('width', width)
            .attr('height', height);
        
        // Create simulation
        this.simulation = d3.forceSimulation()
            .force('link', d3.forceLink().id(d => d.id))
            .force('charge', d3.forceManyBody().strength(-100))
            .force('center', d3.forceCenter(width / 2, height / 2));
        
        // Draw links
        const link = svg.append('g')
            .selectAll('line')
            .data(this.currentGraph.edges)
            .enter().append('line')
            .attr('class', 'graph-link')
            .attr('stroke-width', d => Math.sqrt(d.value));
        
        // Draw nodes
        const node = svg.append('g')
            .selectAll('circle')
            .data(this.currentGraph.nodes)
            .enter().append('circle')
            .attr('class', 'graph-node')
            .attr('r', 8)
            .attr('fill', d => CONFIG.nodeColors[d.type] || CONFIG.nodeColors.default)
            .call(d3.drag()
                .on('start', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0.3).restart();
                    d.fx = d.x;
                    d.fy = d.y;
                })
                .on('drag', (event, d) => {
                    d.fx = event.x;
                    d.fy = event.y;
                })
                .on('end', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0);
                    d.fx = null;
                    d.fy = null;
                }));
        
        // Add labels
        const label = svg.append('g')
            .selectAll('text')
            .data(this.currentGraph.nodes)
            .enter().append('text')
            .attr('class', 'node-label')
            .text(d => d.label)
            .attr('font-size', 10)
            .attr('dx', 10)
            .attr('dy', 4);
        
        // Update positions
        this.simulation.nodes(this.currentGraph.nodes)
            .on('tick', () => {
                link.attr('x1', d => d.source.x)
                    .attr('y1', d => d.source.y)
                    .attr('x2', d => d.target.x)
                    .attr('y2', d => d.target.y);
                
                node.attr('cx', d => d.x)
                    .attr('cy', d => d.y);
                
                label.attr('x', d => d.x)
                    .attr('y', d => d.y);
            });
        
        this.simulation.force('link')
            .links(this.currentGraph.edges);
        
        if (!enablePhysics) {
            this.simulation.stop();
        }
    }

    setupEventListeners() {
        document.getElementById('refresh-graph').addEventListener('click', () => {
            const depth = document.getElementById('graph-depth').value;
            const physics = document.getElementById('toggle-physics').checked;
            this.loadGraph(depth, physics);
        });

        document.getElementById('analyze-btn').addEventListener('click', () => {
            this.runAnalytics();
        });
    }

    async runAnalytics() {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/analytics`);
            const analytics = await response.json();
            this.displayAnalytics(analytics);
        } catch (error) {
            console.error('Failed to run analytics:', error);
        }
    }

    displayAnalytics(analytics) {
        const panel = document.getElementById('graph-analytics');
        panel.innerHTML = `
            <h3>Graph Analytics</h3>
            <div class="metric">
                <span class="metric-label">Central Nodes:</span>
                <div class="metric-values">
                    ${analytics.centrality.map(n => `
                        <div>${n.label} (${n.value.toFixed(3)})</div>
                    `).join('')}
                </div>
            </div>
            <div class="metric">
                <span class="metric-label">Communities:</span>
                <div>${analytics.community.count} detected</div>
            </div>
        `;
    }
	
	highlightNode(nodeId, clientId) {
        const color = this.getClientColor(clientId);
        d3.select(`circle[data-id="${nodeId}"]`)
            .transition()
            .attr("stroke", color)
            .attr("stroke-width", 3);
    }
    
    getClientColor(clientId) {
        // Simple deterministic color assignment
        const colors = ["#FF00FF", "#00FFFF", "#FFFF00", "#FF9900"];
        const index = parseInt(clientId.split("-")[1]) % colors.length;
        return colors[index];
    }
    
    refreshNode(nodeId) {
        // Refresh node visualization
        this.loadGraph(this.currentDepth, true);
    }
	
	renderHistoricalGraph(graphData) {
        // Clear current graph
        d3.select("#graph-canvas").selectAll("*").remove();
        
        // Render historical version
        this.currentGraph = graphData;
        this.renderGraph(false); // Disable physics for historical views
        
        // Visual indication
        d3.select("#graph-canvas")
            .append("rect")
            .attr("width", "100%")
            .attr("height", "100%")
            .attr("fill", "rgba(0,0,0,0.1)")
            .attr("class", "historical-overlay");
    }
	
}

// Initialize when loaded
window.addEventListener('DOMContentLoaded', () => {
    new GraphExplorer('graph-explorer-container');
});


=== static\js\signature.js ===
class SignatureUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.tooltip = this._createTooltip();
        this._setupListeners();
    }

    _createTooltip() {
        const tooltip = document.createElement('div');
        tooltip.className = 'signature-tooltip';
        tooltip.style.display = 'none';
        document.body.appendChild(tooltip);
        return tooltip;
    }

    _setupListeners() {
        this.editor.addEventListener('mousemove', this._debounce(async (e) => {
            const pos = this._getCursorPosition(e);
            const signature = await this._fetchSignature(pos);
            if (signature) this._showSignature(signature);
        }, 300));
    }

    async _fetchSignature(cursorPos) {
        const response = await fetch('/signature', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                content: this.editor.value,
                context: {
                    code: this.editor.value,
                    language: 'python',
                    cursor_pos: cursorPos
                }
            })
        });
        return await response.json();
    }

    _showSignature(data) {
        if (!data.name) {
            this.tooltip.style.display = 'none';
            return;
        }

        const params = data.parameters.map((p, i) => 
            `<span class="${i === data.active_parameter ? 'active-param' : ''}">
                ${p.type ? `${p.type} ` : ''}${p.name}
            </span>`
        ).join(', ');

        this.tooltip.innerHTML = `
            <div class="signature-title">${data.name}(${params})</div>
        `;
        this._positionTooltip();
        this.tooltip.style.display = 'block';
    }

    _positionTooltip() {
        // Position near cursor (simplified)
        const rect = this.editor.getBoundingClientRect();
        this.tooltip.style.left = `${rect.left + 20}px`;
        this.tooltip.style.top = `${rect.top - 40}px`;
    }

    _debounce(func, delay) {
        let timeout;
        return (...args) => {
            clearTimeout(timeout);
            timeout = setTimeout(() => func.apply(this, args), delay);
        };
    }
}


=== templates\index.html ===
<!DOCTYPE html>
<html>
<head>
    <title>Knowledge Graph Explorer</title>
    <link rel="stylesheet" href="/static/css/graph.css">
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
    <div id="graph-explorer-container"></div>
    <script src="/static/js/graph-explorer.js"></script>
</body>
</html>


=== tests\conftest.py ===
# tests/conftest.py
import pytest
import asyncio
from pathlib import Path
import sys
import os

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.context import ContextManager
from core.orchestrator import Orchestrator
from core.validation.quality_gates import QualityValidator
from modules.registry import ModuleRegistry
from shared.knowledge.graph import KnowledgeGraph

@pytest.fixture
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def context_manager():
    """Create a test context manager"""
    context = ContextManager()
    yield context
    # Cleanup
    context.graph.graph.clear()

@pytest.fixture
async def module_registry():
    """Create a test module registry"""
    registry = ModuleRegistry()
    registry.discover_modules()
    yield registry

@pytest.fixture
async def quality_validator():
    """Create a test quality validator"""
    config = {
        "quality_standards": {
            "min_complexity": 0.3,
            "required_keys": ["answer", "explanation"],
            "banned_patterns": ["eval(", "system("]
        }
    }
    return QualityValidator(config)

# tests/test_orchestrator.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from shared.schemas import Query, Response
from core.orchestrator import Orchestrator

@pytest.mark.asyncio
async def test_route_query_success(context_manager, module_registry):
    """Test successful query routing"""
    # Mock dependencies
    validator = MagicMock()
    sla_router = MagicMock()
    load_balancer = MagicMock()
    healing_controller = MagicMock()
    reasoning_engine = MagicMock()
    monitoring = MagicMock()
    
    # Configure mocks
    sla_router.select_provider.return_value = {"provider": "test", "tier": "standard"}
    reasoning_engine.process.return_value = {"source": "llm", "result": "test response"}
    
    module = MagicMock()
    module.process.return_value = Response(content="test response")
    module_registry.get_module.return_value = module
    
    validator.validate.return_value = {
        "passed": True,
        "checks": {},
        "original_response": Response(content="test response")
    }
    
    # Create orchestrator
    orchestrator = Orchestrator(
        validator=validator,
        sla_router=sla_router,
        load_balancer=load_balancer,
        registry=module_registry,
        healing_controller=healing_controller,
        context_manager=context_manager,
        reasoning_engine=reasoning_engine,
        monitoring=monitoring
    )
    
    # Test query
    query = Query(content="test query")
    response = await orchestrator.route_query(query)
    
    # Assertions
    assert response.content == "test response"
    assert sla_router.select_provider.called
    assert reasoning_engine.process.called
    assert module.process.called

@pytest.mark.asyncio
async def test_route_query_with_quality_failure(context_manager, module_registry):
    """Test query routing with quality validation failure"""
    # Mock dependencies
    validator = MagicMock()
    sla_router = MagicMock()
    load_balancer = MagicMock()
    healing_controller = MagicMock()
    reasoning_engine = MagicMock()
    monitoring = MagicMock()
    
    # Configure mocks
    sla_router.select_provider.return_value = {"provider": "test", "tier": "standard"}
    reasoning_engine.process.return_value = {"source": "llm", "result": "test response"}
    
    module = MagicMock()
    module.process.return_value = Response(content="test response")
    module_registry.get_module.return_value = module
    
    validator.validate.return_value = {
        "passed": False,
        "checks": {"safety": False},
        "original_response": Response(content="test response")
    }
    
    # Create orchestrator
    orchestrator = Orchestrator(
        validator=validator,
        sla_router=sla_router,
        load_balancer=load_balancer,
        registry=module_registry,
        healing_controller=healing_controller,
        context_manager=context_manager,
        reasoning_engine=reasoning_engine,
        monitoring=monitoring
    )
    
    # Mock the retry method
    orchestrator._retry_with_stricter_llm = AsyncMock(return_value=Response(content="retry response"))
    
    # Test query
    query = Query(content="test query")
    response = await orchestrator.route_query(query)
    
    # Assertions
    assert response.content == "retry response"
    assert orchestrator._retry_with_stricter_llm.called

# tests/test_integrations/test_ollama.py
import pytest
from unittest.mock import Mock, patch
from core.integrations.ollama import Plugin

@pytest.mark.asyncio
async def test_ollama_plugin_initialization():
    """Test Ollama plugin initialization"""
    config = {
        "base_url": "http://localhost:11434",
        "default_model": "llama2",
        "batch_size": 4
    }
    
    plugin = Plugin(config)
    result = plugin.initialize()
    
    assert result is True
    assert plugin._initialized is True
    assert plugin.base_url == "http://localhost:11434"
    assert plugin.default_model == "llama2"

@pytest.mark.asyncio
async def test_ollama_plugin_execution():
    """Test Ollama plugin execution"""
    config = {
        "base_url": "http://localhost:11434",
        "default_model": "llama2",
        "batch_size": 4
    }
    
    plugin = Plugin(config)
    plugin.initialize()
    
    input_data = {"prompt": "test prompt", "max_tokens": 100}
    
    with patch('requests.post') as mock_post:
        mock_response = Mock()
        mock_response.json.return_value = {"response": "test response"}
        mock_response.raise_for_status.return_value = None
        mock_post.return_value = mock_response
        
        result = plugin.execute(input_data)
        
        assert result["response"] == "test response"
        mock_post.assert_called_once()


=== tests\integration\test_multimodal.py ===
# tests/integration/test_multimodal.py
import pytest
import base64
from unittest.mock import Mock, patch
from core.multimodal.image_analyzer import ImageAnalyzer
from PIL import Image
import io

@pytest.fixture
def image_analyzer():
    return ImageAnalyzer()

@pytest.fixture
def sample_code_image():
    # Create a sample image with code
    image = Image.new('RGB', (800, 600), color='white')
    # In a real test, you'd use an actual image with code
    return image

def test_image_analysis_success(image_analyzer, sample_code_image):
    """Test successful image analysis"""
    # Convert image to base64
    buffer = io.BytesIO()
    sample_code_image.save(buffer, format="PNG")
    image_data = base64.b64encode(buffer.getvalue()).decode()
    
    result = image_analyzer.analyze_code_image(image_data)
    
    assert result["success"] is True
    assert "extracted_text" in result
    assert "language" in result
    assert "structured_code" in result
    assert result["confidence"] > 0

@pytest.mark.asyncio
async def test_language_detection(image_analyzer):
    """Test programming language detection"""
    python_code = "def hello_world():\n    print('Hello, World!')"
    js_code = "function helloWorld() {\n    console.log('Hello, World!');\n}"
    
    python_result = image_analyzer._detect_language(python_code)
    js_result = image_analyzer._detect_language(js_code)
    
    assert python_result == "python"
    assert js_result == "javascript"

# tests/integration/test_refactoring.py
import pytest
from core.refactoring.refactor_engine import RefactoringEngine, RefactoringType

@pytest.fixture
def refactoring_engine():
    return RefactoringEngine()

def test_extract_function_detection(refactoring_engine):
    """Test detection of extract function opportunities"""
    long_function_code = """
def complex_function():
    # This function is too long
    data = []
    for i in range(100):
        if i % 2 == 0:
            data.append(i)
    # More code...
    result = process_data(data)
    return result
"""
    
    suggestions = refactoring_engine.analyze_code(long_function_code, "python")
    
    extract_suggestions = [s for s in suggestions if s.type == RefactoringType.EXTRACT_FUNCTION]
    assert len(extract_suggestions) > 0
    assert any("Extract function" in s.title for s in extract_suggestions)

def test_magic_number_detection(refactoring_engine):
    """Test detection of magic numbers"""
    code_with_magic_numbers = """
def calculate_area(radius):
    return 3.14159 * radius * radius

def calculate_total(items):
    total = 0
    for item in items:
        if item.value > 100:
            total += item.value * 1.15  # 15% tax
    return total
"""
    
    suggestions = refactoring_engine.analyze_code(code_with_magic_numbers, "python")
    
    magic_number_suggestions = [s for s in suggestions if s.type == RefactoringType.INTRODUCE_CONSTANT]
    assert len(magic_number_suggestions) > 0
    assert any("3.14159" in s.original_code for s in magic_number_suggestions)

# tests/integration/test_collaboration.py
import pytest
import asyncio
from core.collaboration.session_manager import CollaborationManager, SessionRole, Permission

@pytest.fixture
def collaboration_manager():
    return CollaborationManager()

@pytest.mark.asyncio
async def test_session_creation(collaboration_manager):
    """Test session creation"""
    session = await collaboration_manager.create_session(
        owner_id="user1",
        name="Test Session",
        code="print('Hello, World!')",
        language="python"
    )
    
    assert session.id is not None
    assert session.owner_id == "user1"
    assert session.code == "print('Hello, World!')"
    assert session.owner_id in session.collaborators
    assert session.collaborators[session.owner_id].role == SessionRole.OWNER

@pytest.mark.asyncio
async def test_session_joining(collaboration_manager):
    """Test joining a session"""
    # Create session
    session = await collaboration_manager.create_session(
        owner_id="user1",
        name="Test Session",
        code="print('Hello, World!')",
        language="python"
    )
    
    # Join session
    joined_session = await collaboration_manager.join_session(
        session_id=session.id,
        user_id="user2",
        user_name="User 2"
    )
    
    assert joined_session is not None
    assert "user2" in joined_session.collaborators
    assert joined_session.collaborators["user2"].role == SessionRole.VIEWER

@pytest.mark.asyncio
async def test_code_update_permissions(collaboration_manager):
    """Test code update permissions"""
    # Create session
    session = await collaboration_manager.create_session(
        owner_id="user1",
        name="Test Session",
        code="print('Hello, World!')",
        language="python"
    )
    
    # Join as viewer
    await collaboration_manager.join_session(
        session_id=session.id,
        user_id="user2",
        user_name="User 2"
    )
    
    # Try to update code as viewer (should fail)
    result = await collaboration_manager.update_code(
        session_id=session.id,
        user_id="user2",
        code="print('Updated code')"
    )
    assert result is False
    
    # Update code as owner (should succeed)
    result = await collaboration_manager.update_code(
        session_id=session.id,
        user_id="user1",
        code="print('Updated code')"
    )
    assert result is True
    assert session.code == "print('Updated code')"


=== tests\performance\test_performance.py ===
# tests/performance/test_performance.py
import pytest
import asyncio
import time
import psutil
import statistics
from concurrent.futures import ThreadPoolExecutor
from core.orchestrator import Orchestrator
from shared.schemas import Query

class PerformanceTestSuite:
    def __init__(self):
        self.results = {}
    
    async def test_query_throughput(self, orchestrator, num_queries=100):
        """Test query processing throughput"""
        queries = [
            Query(content=f"How to reverse a list in Python? {i}")
            for i in range(num_queries)
        ]
        
        start_time = time.time()
        tasks = [orchestrator.route_query(query) for query in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        successful_queries = sum(1 for r in results if not isinstance(r, Exception))
        throughput = successful_queries / (end_time - start_time)
        
        self.results["query_throughput"] = {
            "total_queries": num_queries,
            "successful_queries": successful_queries,
            "time_seconds": end_time - start_time,
            "queries_per_second": throughput
        }
        
        return throughput
    
    async def test_concurrent_users(self, orchestrator, num_users=50):
        """Test system performance under concurrent load"""
        async def user_session(user_id):
            queries = [
                Query(content=f"User {user_id} query {i}")
                for i in range(5)
            ]
            
            for query in queries:
                start = time.time()
                try:
                    await orchestrator.route_query(query)
                    yield time.time() - start
                except Exception:
                    yield float('inf')  # Mark failed requests
        
        start_time = time.time()
        all_latencies = []
        
        # Simulate concurrent users
        with ThreadPoolExecutor(max_workers=num_users) as executor:
            loop = asyncio.get_event_loop()
            futures = []
            
            for user_id in range(num_users):
                future = loop.run_in_executor(
                    executor, 
                    lambda uid=user_id: list(asyncio.run(user_session(uid)))
                )
                futures.append(future)
            
            for future in futures:
                user_latencies = future.result()
                all_latencies.extend(user_latencies)
        
        end_time = time.time()
        
        # Calculate metrics
        valid_latencies = [l for l in all_latencies if l != float('inf')]
        avg_latency = statistics.mean(valid_latencies) if valid_latencies else float('inf')
        p95_latency = statistics.quantiles(valid_latencies, n=20)[18] if len(valid_latencies) > 20 else float('inf')
        
        self.results["concurrent_users"] = {
            "num_users": num_users,
            "total_time": end_time - start_time,
            "avg_latency_seconds": avg_latency,
            "p95_latency_seconds": p95_latency,
            "success_rate": len(valid_latencies) / len(all_latencies)
        }
        
        return avg_latency
    
    def test_memory_usage(self, orchestrator, duration=60):
        """Test memory usage over time"""
        process = psutil.Process()
        memory_samples = []
        
        def sample_memory():
            memory_info = process.memory_info()
            return memory_info.rss / 1024 / 1024  # MB
        
        start_time = time.time()
        while time.time() - start_time < duration:
            memory_samples.append(sample_memory())
            time.sleep(1)
        
        self.results["memory_usage"] = {
            "duration_seconds": duration,
            "samples": len(memory_samples),
            "avg_memory_mb": statistics.mean(memory_samples),
            "max_memory_mb": max(memory_samples),
            "min_memory_mb": min(memory_samples)
        }
        
        return statistics.mean(memory_samples)
    
    def get_results(self):
        """Get all performance test results"""
        return self.results

@pytest.mark.asyncio
async def test_performance_suite():
    """Run complete performance test suite"""
    # This would be integrated with your actual orchestrator
    # orchestrator = get_test_orchestrator()
    
    performance_suite = PerformanceTestSuite()
    
    # Run tests
    # await performance_suite.test_query_throughput(orchestrator)
    # await performance_suite.test_concurrent_users(orchestrator)
    # performance_suite.test_memory_usage(orchestrator)
    
    results = performance_suite.get_results()
    
    # Assert performance thresholds
    # assert results["query_throughput"]["queries_per_second"] > 10
    # assert results["concurrent_users"]["avg_latency_seconds"] < 2.0
    # assert results["memory_usage"]["max_memory_mb"] < 1024  # 1GB
    
    print("Performance test results:", results)


=== vscode-extension\package.json ===
// vscode-extension/package.json
{
    "name": "open-llm-code-assistant",
    "displayName": "Open LLM Code Assistant",
    "description": "AI-powered coding assistant with multi-modal analysis and refactoring suggestions",
    "version": "0.1.0",
    "engines": {
        "vscode": "^1.60.0"
    },
    "categories": [
        "Programming Languages",
        "Machine Learning",
        "Other"
    ],
    "activationEvents": [
        "onStartupFinished"
    ],
    "main": "./out/extension.js",
    "scripts": {
        "vscode:prepublish": "npm run compile",
        "compile": "tsc -p ./",
        "watch": "tsc -w"
    },
    "contributes": {
        "commands": [
            {
                "command": "open-llm.getCodeSuggestion",
                "title": "Get Code Suggestion",
                "category": "Open LLM"
            },
            {
                "command": "open-llm.analyzeRefactoring",
                "title": "Analyze Refactoring Opportunities",
                "category": "Open LLM"
            },
            {
                "command": "open-llm.analyzeImage",
                "title": "Analyze Code from Image",
                "category": "Open LLM"
            }
        ],
        "keybindings": [
            {
                "command": "open-llm.getCodeSuggestion",
                "key": "ctrl+shift+c",
                "mac": "cmd+shift+c"
            },
            {
                "command": "open-llm.analyzeRefactoring",
                "key": "ctrl+shift+r",
                "mac": "cmd+shift+r"
            }
        ],
        "configuration": {
            "title": "Open LLM Code Assistant",
            "properties": {
                "open-llm.apiUrl": {
                    "type": "string",
                    "default": "http://localhost:8000",
                    "description": "URL of the Open LLM API server"
                },
                "open-llm.apiKey": {
                    "type": "string",
                    "default": "",
                    "description": "API key for authentication"
                }
            }
        }
    },
    "dependencies": {
        "axios": "^0.24.0",
        "vscode": "^1.1.37"
    },
    "devDependencies": {
        "@types/vscode": "^1.60.0",
        "@types/node": "^16.0.0",
        "typescript": "^4.0.0"
    }
}


=== webpack.config.js ===
const path = require('path');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');

module.exports = {
    entry: {
        app: './static/ts/graph-explorer.ts',
        styles: './static/scss/main.scss'
    },
    output: {
        filename: 'js/[name].bundle.js',
        path: path.resolve(__dirname, 'static/dist')
    },
    resolve: {
        extensions: ['.ts', '.js', '.scss']
    },
    module: {
        rules: [
            {
                test: /\.ts$/,
                use: 'ts-loader',
                exclude: /node_modules/
            },
            {
                test: /\.scss$/,
                use: [
                    MiniCssExtractPlugin.loader,
                    'css-loader',
                    'sass-loader'
                ]
            }
        ]
    },
    plugins: [
        new MiniCssExtractPlugin({
            filename: 'css/[name].css'
        })
    ],
    externals: {
        d3: 'd3'
    }
};


