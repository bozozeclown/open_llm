=== .env ===
GROQ_API_KEY="your_key"
HF_API_KEY="your_key" 
TEXTGEN_API_KEY="your_key"


=== .gitignore ===


=== configs\base.yaml ===
project:
  name: "AI-code-assistant"
  version: "0.0.1"
  
paths:
  data_raw: "./data/raw"
  data_processed: "./data/processed"
  model_checkpoints: "./models"
  
languages:
  priority: ["python", "csharp", "c", "javascript", "typescript", "html", "css"]
  
quality_standards:
  min_complexity: 0.4  # 0-1 scale
  required_keys: ["answer", "explanation"]  # For structured responses
  banned_patterns:
    - "eval("
    - "system("
    - "os.popen"


=== configs\integration.yaml ===
integrations:
  # Plugin configurations
  plugins:
    ollama:
      enabled: true
      config:
        base_url: "http://localhost:11434"
        default_model: "codellama"
        timeout: 30
        batch_size: 4  # Added batching support

    vllm:
      enabled: true
      config:
        model: "codellama/CodeLlama-7b-hf"
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.9
        max_batch_size: 2048  # Tokens

    textgen:
      enabled: true
      config:
        base_url: "http://localhost:5000"
        api_key: "${TEXTGEN_API_KEY}"  # From environment
        batch_size: 4
        timeout: 45

    huggingface:
      enabled: false  # Disabled by default
      config:
        api_key: "${HF_API_KEY}"
        model_name: "codellama/CodeLlama-7b-hf"
        device: "auto"
        quantize: false
        batch_size: 2

    grok:
      enabled: true
      config:
        api_key: "${GROQ_API_KEY}"
        rate_limit: 5  # Requests per minute
        timeout: 15

    lmstudio:
      enabled: false  # Disabled by default
      config:
        base_url: "http://localhost:1234"
        timeout: 60
        batch_support: false

  # Global integration settings
  settings:
    default_timeout: 30  # Fallback timeout
    priority_order:  # Execution priority
      - "vllm"
      - "ollama" 
      - "grok"
      - "huggingface"
      - "textgen"
      - "lmstudio"

    # Batch processing defaults
    batching:
      enabled: true
      max_batch_size: 8
      max_wait_ms: 50

    # Monitoring
    health_check_interval: 60  # Seconds
  
  load_balancing:
    update_interval: 10  # Seconds
    min_requests: 20     # Minimum data points before activating
    priority_bump: 2.0   # Weight multiplier for high-priority queries


=== configs\model.yaml ===


=== configs\predictions.yaml ===
# configs/prediction.yaml
cache:
  warmers: 2
  max_predictions: 3
  min_confidence: 0.7


=== configs\sla_tiers.yaml ===
tiers:
  critical:
    min_accuracy: 0.96
    max_latency: 1.2
    allowed_providers: ["gpt-4", "claude-2"]
    cost_multiplier: 2.5
    
  standard:
    min_accuracy: 0.88  
    max_latency: 2.5
    allowed_providers: ["gpt-3.5", "claude-instant"]
    
  economy:
    min_accuracy: 0.75
    max_latency: 7.0
    allowed_providers: ["llama2", "local"]


=== core\analysis.py ===
# core/analysis.py
import re
from enum import Enum

class ContentType(Enum):
    CODE_PYTHON = "code_python"
    CODE_CSHARP = "code_csharp"
    MATH_SYMBOLIC = "math_symbolic"
    TEXT_QUERY = "text_query"

class ContentAnalyzer:
    CODE_PATTERNS = {
        ContentType.CODE_PYTHON: [
            r'def\s+\w+\(.*\):',
            r'import\s+\w+'
        ],
        ContentType.CODE_CSHARP: [
            r'public\s+(class|interface)\s+\w+',
            r'using\s+\w+;'
        ]
    }
    
    def analyze(self, text: str) -> ContentType:
        for content_type, patterns in self.CODE_PATTERNS.items():
            if any(re.search(p, text) for p in patterns):
                return content_type
        return ContentType.TEXT_QUERY


=== core\completion.py ===
from transformers import pipeline
from typing import List, Dict
from shared.schemas import CompletionRequest

class CodeCompleter:
    def __init__(self, model_name="deepseek-coder-6.7b"):
        self.completion_pipeline = pipeline(
            "text-generation",
            model=model_name,
            device="cuda"  # Use "cpu" if no GPU
        )

    def generate_completions(self, request: CompletionRequest) -> Dict[str, List[str]]:
        """Generate code suggestions with context awareness"""
        prompt = self._build_prompt(request.context, request.cursor_context)
        outputs = self.completion_pipeline(
            prompt,
            num_return_sequences=3,
            max_new_tokens=50,
            temperature=0.7,
            stop_sequences=["\n\n"]
        )
        return {"completions": [o["generated_text"] for o in outputs]}

    def _build_prompt(self, context: str, cursor_context: str) -> str:
        """Structured prompt for code completion"""
        return f"""# Code Context:
{context}

# Cursor Position:
{cursor_context}

# Suggested Completion:"""


=== core\context.py ===
from shared.knowledge.graph import KnowledgeGraph
from shared.schemas import Query, Response
from typing import Dict, Any, List, Optional
import numpy as np
import hashlib
from datetime import datetime

class ContextManager:
    def __init__(self):
        self.graph = KnowledgeGraph()
        self._setup_foundational_knowledge()
        self.interaction_log = []
        self.routing_history = []  # NEW: Track routing decisions
        
    def _setup_foundational_knowledge(self):
        """Initialize with programming fundamentals"""
        foundations = [
            ("variable", "named storage location", ["storage", "memory"]),
            ("function", "reusable code block", ["abstraction", "parameters"]),
            ("loop", "iteration construct", ["repetition", "termination"]),
            ("class", "object blueprint", ["inheritance", "encapsulation"])
        ]
        
        for concept, desc, tags in foundations:
            node_id = self.graph.add_entity(
                content=concept,
                type="concept",
                metadata={
                    "description": desc,
                    "tags": tags,
                    "source": "system"
                }
            )
    
    # MODIFIED: Enhanced with routing metadata        
    def process_interaction(
        self, 
        query: Query, 
        response: Response,
        metadata: Optional[Dict] = None
    ):
        """
        Learn from user interactions with routing context
        Args:
            metadata: {
                "sla_tier": str,       # critical/standard/economy
                "reasoning_source": str, # graph/rule/llm
                "provider": str         # gpt-4/llama2/etc
            }
        """
        # Generate unique interaction ID
        interaction_id = hashlib.sha256(
            f"{datetime.now().isoformat()}:{query.content}".encode()
        ).hexdigest()
        
        # Enhanced logging (NEW)
        log_entry = {
            "id": interaction_id,
            "query": query.content,
            "response": response.content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        self.interaction_log.append(log_entry)
        
        # Track routing decisions separately (NEW)
        if metadata:
            self.routing_history.append({
                "timestamp": datetime.now().isoformat(),
                "query_hash": hashlib.sha256(query.content.encode()).hexdigest()[:8],
                **metadata
            })
        
        # Extract knowledge from both query and response
        self.graph.expand_from_text(
            query.content, 
            source="query",
            metadata={"sla_tier": metadata.get("sla_tier")} if metadata else None  # NEW
        )
        
        self.graph.expand_from_text(
            response.content,
            source="response",
            metadata={"provider": metadata.get("provider")} if metadata else None  # NEW
        )
        
        # Create relationship between query and response concepts
        query_nodes = self._extract_key_nodes(query.content)
        response_nodes = self._extract_key_nodes(response.content)
        
        for q_node in query_nodes:
            for r_node in response_nodes:
                self.graph.add_relation(
                    q_node, 
                    r_node, 
                    "elicits",
                    metadata=metadata  # NEW: Attach routing info to edges
                )
    
    # NEW METHOD
    def get_routing_context(self, query_content: str) -> Dict[str, Any]:
        """
        Get context specifically for routing decisions
        Returns:
            {
                "is_production": bool,
                "similar_past_queries": List[Dict],
                "preferred_llm": Optional[str],
                "complexity_score": float
            }
        """
        # Existing semantic matching
        matches = self.graph.find_semantic_matches(query_content)
        
        # NEW: Calculate complexity
        complexity = min(len(query_content.split()) / 10, 1.0)  # 0-1 scale
        
        return {
            "is_production": any(
                "production" in node["content"].lower() 
                for node in matches[:3]
            ),
            "similar_past_queries": [
                {
                    "query": self.graph.graph.nodes[m["node_id"]]["content"],
                    "source": self.graph.graph.nodes[m["node_id"]].get("source"),
                    "success": self._get_interaction_success(m["node_id"])
                }
                for m in matches[:3]
            ],
            "complexity_score": complexity,
            "preferred_llm": self._detect_preferred_provider(query_content)
        }
    
    # NEW HELPER METHODS
    def _get_interaction_success(self, node_id: str) -> bool:
        """Check if previous interactions with this node were successful"""
        edges = list(self.graph.graph.edges(node_id, data=True))
        return any(
            e[2].get("metadata", {}).get("success", True)
            for e in edges
        )
    
    def _detect_preferred_provider(self, query: str) -> Optional[str]:
        """Detect if query suggests a preferred provider"""
        query_lower = query.lower()
        if "openai" in query_lower:
            return "gpt-4"
        elif "local" in query_lower:
            return "llama2"
        return None
    
    # EXISTING METHODS (unchanged)
    def _extract_key_nodes(self, text: str) -> List[str]:
        """Identify most important nodes in text"""
        matches = self.graph.find_semantic_matches(text)
        return [m["node_id"] for m in matches[:3]]  # Top 3 matches
        
    def get_context(self, text: str) -> Dict[str, Any]:
        """Get relevant context for given text"""
        matches = self.graph.find_semantic_matches(text)
        context_nodes = set()
        
        # Get related nodes for each match
        for match in matches[:5]:  # Top 5 matches
            neighbors = list(self.graph.graph.neighbors(match["node_id"]))
            context_nodes.update(neighbors)
            
        return {
            "matches": matches,
            "related": [
                {"id": n, **self.graph.graph.nodes[n]}
                for n in context_nodes
            ]
        }


=== core\debugger.py ===
from typing import Dict, List
from dataclasses import dataclass
import ast
import traceback

@dataclass
class DebugFrame:
    file: str
    line: int
    context: str
    variables: Dict[str, str]
    error: str

class CodeDebugger:
    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph
        self.error_patterns = self._load_error_patterns()

    def analyze_traceback(self, code: str, error: str) -> List[DebugFrame]:
        """Convert traceback into structured debug frames"""
        frames = []
        tb = traceback.extract_tb(error.__traceback__)
        
        for frame in tb:
            context = self._get_code_context(code, frame.lineno)
            frames.append(DebugFrame(
                file=frame.filename,
                line=frame.lineno,
                context=context,
                variables=self._extract_variables(frame.locals),
                error=str(error)
            ))
        
        return frames

    def suggest_fixes(self, frames: List[DebugFrame]) -> Dict[str, List[str]]:
        """Generate fix suggestions using knowledge graph"""
        suggestions = {}
        for frame in frames:
            error_key = self._match_error_pattern(frame.error)
            if error_key in self.error_patterns:
                suggestions[frame.line] = self._enhance_suggestions(
                    self.error_patterns[error_key],
                    frame.context
                )
        return suggestions

    def _get_code_context(self, code: str, line: int, window=3) -> str:
        lines = code.split('\n')
        start = max(0, line - window - 1)
        end = min(len(lines), line + window)
        return '\n'.join(lines[start:end])

    def _extract_variables(self, locals_dict: Dict) -> Dict[str, str]:
        return {k: repr(v)[:100] for k, v in locals_dict.items() if not k.startswith('_')}

    def _match_error_pattern(self, error: str) -> str:
        for pattern in self.error_patterns:
            if pattern in error:
                return pattern
        return "unknown"

    def _enhance_suggestions(self, base_suggestions: List[str], context: str) -> List[str]:
        enhanced = []
        for suggestion in base_suggestions:
            # Augment with knowledge graph matches
            related = self.kg.find_semantic_matches(context + " " + suggestion)[:2]
            if related:
                enhanced.append(f"{suggestion} (Related: {', '.join(r['content'] for r in related)})")
            else:
                enhanced.append(suggestion)
        return enhanced

    def _load_error_patterns(self) -> Dict[str, List[str]]:
        return {
            "IndexError": [
                "Check list length before accessing",
                "Verify array bounds"
            ],
            "KeyError": [
                "Check if key exists in dictionary",
                "Use dict.get() for safe access"
            ],
            # ... other patterns
        }


=== core\feedback\processor.py ===
from datetime import datetime
from typing import Dict, Any
import numpy as np
from shared.knowledge.graph import KnowledgeGraph

class FeedbackProcessor:
    def __init__(self, context_manager):
        self.context = context_manager
        self.feedback_weights = {
            'explicit_rating': 0.7,
            'implicit_engagement': 0.3,
            'correction': 1.0
        }

    def process_feedback(self, feedback: Dict[str, Any]):
        """Handle both explicit and implicit feedback"""
        # Store raw feedback
        self._log_feedback(feedback)

        # Update knowledge graph
        if feedback['type'] == 'correction':
            self._apply_correction(feedback)
        else:
            self._update_edge_weights(feedback)

    def _apply_correction(self, feedback):
        """Direct knowledge corrections"""
        self.context.graph.update_node(
            node_id=feedback['target_node'],
            new_content=feedback['corrected_info'],
            metadata={'last_corrected': datetime.now()}
        )

    def _update_edge_weights(self, feedback):
        """Adjust relationship strengths"""
        current_weight = self.context.graph.get_edge_weight(
            feedback['query_node'],
            feedback['response_node']
        )
        
        new_weight = current_weight * (1 + self._calculate_feedback_impact(feedback))
        self.context.graph.update_edge(
            source=feedback['query_node'],
            target=feedback['response_node'],
            weight=min(new_weight, 1.0)  # Cap at 1.0
        )

    def _calculate_feedback_impact(self, feedback) -> float:
        """Calculate weighted feedback impact"""
        base_score = (
            feedback.get('rating', 0.5) * self.feedback_weights['explicit_rating'] +
            feedback.get('engagement', 0.2) * self.feedback_weights['implicit_engagement']
        )
        return base_score * (2 if feedback['type'] == 'positive' else -1)


=== core\health.py ===
from typing import Dict
import requests

class HealthChecker:
    @staticmethod
    def check_endpoint(url: str) -> Dict[str, bool]:
        try:
            resp = requests.get(f"{url}/health", timeout=3)
            return {
                "online": resp.status_code == 200,
                "models_loaded": resp.json().get("models_loaded", 0)
            }
        except:
            return {"online": False}
            
    def check_ollama(base_url="http://localhost:11434"):
        try:
            resp = requests.get(f"{base_url}/api/tags", timeout=3)
            return {
                "status": "online",
                "models": [m['name'] for m in resp.json().get('models', [])]
            }
        except Exception as e:
            return {"status": "error", "details": str(e)}




=== core\integrations\__init__.py ===
from importlib import import_module
from pathlib import Path
from typing import Dict, Type
from ..plugin import PluginBase

_PLUGINS: Dict[str, Type[PluginBase]] = {}

def _discover_plugins():
    package_dir = Path(__file__).parent
    for _, module_name, _ in pkgutil.iter_modules([str(package_dir)]):
        if module_name in ("__init__", "manager"):
            continue
        
        try:
            module = import_module(f".{module_name}", package=__package__)
            if (plugin_class := getattr(module, "Plugin", None)) and \
               issubclass(plugin_class, PluginBase):
                _PLUGINS[module_name] = plugin_class
        except (ImportError, TypeError) as e:
            import warnings
            warnings.warn(f"Failed to load {module_name}: {str(e)}")

def get_plugin(name: str) -> Type[PluginBase]:
    """Get plugin class by name (e.g., 'ollama')"""
    return _PLUGINS[name]  # Will raise KeyError if not found

def available_plugins() -> Dict[str, Type[PluginBase]]:
    """Return copy of registered plugins"""
    return _PLUGINS.copy()

# Initialize on import
import pkgutil
_discover_plugins()

__all__ = ['get_plugin', 'available_plugins']


=== core\integrations\grok.py ===
import requests
import time
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
from datetime import datetime

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="grok",
            version="0.3.0",
            required_config={
                "api_key": str,
                "rate_limit": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Grok AI API integration with batching"
        )

    def initialize(self):
        self.api_key = self.config["api_key"]
        self.rate_limit = self.config.get("rate_limit", 5)
        self.timeout = self.config.get("timeout", 10)
        self.last_calls = []
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        if not self._check_rate_limit():
            return {"error": "Rate limit exceeded"}

        try:
            self.last_calls.append(time.time())
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            "https://api.grok.ai/v1/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 150)
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _check_rate_limit(self):
        now = time.time()
        self.last_calls = [t for t in self.last_calls if t > now - 60]
        return len(self.last_calls) < self.rate_limit

    def health_check(self):
        return {
            "ready": self._initialized,
            "rate_limit": f"{len(self.last_calls)}/{self.rate_limit}",
            "last_call": self.last_calls[-1] if self.last_calls else None
        }


=== core\integrations\huggingface.py ===
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import torch
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="huggingface",
            version="0.5.0",
            required_config={
                "model_name": str,
                "device": str,
                "quantize": bool,
                "batch_size": int
            },
            dependencies=["transformers>=4.30.0", "torch"],
            description="HuggingFace Transformers with batching"
        )

    def initialize(self):
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config["model_name"],
                device_map="auto" if self.config["device"] == "auto" else None
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model_name"]
            )
            if self.config.get("quantize", False):
                self.model = torch.quantization.quantize_dynamic(
                    self.model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
            self.batch_size = self.config.get("batch_size", 4)
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"Initialization failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(input_data["prompt"], return_tensors="pt").to(self.config["device"])
        outputs = self.model.generate(**inputs)
        return {"response": self.tokenizer.decode(outputs[0])}

    def _batch_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(
            input_data["prompt"],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.config["device"])
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=input_data.get("max_tokens", 50),
            num_return_sequences=1,
            batch_size=self.batch_size
        )
        
        return {
            "responses": [
                self.tokenizer.decode(output, skip_special_tokens=True)
                for output in outputs
            ]
        }

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("huggingface").observe(time.time() - start_time)


=== core\integrations\lmstudio.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="lmstudio",
            version="0.4.0",
            required_config={
                "base_url": str,
                "timeout": int,
                "batch_support": bool
            },
            dependencies=["requests"],
            description="LM Studio local server with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.timeout = self.config.get("timeout", 60)
        self._batch_support = self.config.get("batch_support", False)
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self._batch_support

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/v1/completions",
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 200),
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("lmstudio").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/v1/models", timeout=5)
            base_status["status"] = "online" if resp.ok else "offline"
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\manager.py ===
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any
from core.monitoring.service import Monitoring

class Plugin(PluginBase):
    def __init__(self, config: Dict[str, Any], monitor: Monitoring):
        super().__init__(config)
        self.monitor = monitor

    def get_metadata(self):
        return PluginMetadata(
            name="manager",
            version="0.2.0",
            description="Enhanced plugin management with monitoring",
            required_config={}
        )

    @self.monitor.track_request('plugin_manager')
    def execute(self, command: Dict[str, Any]) -> Dict[str, Any]:
        """Monitored plugin management"""
        try:
            if command["action"] == "list_plugins":
                return self._list_plugins()
            elif command["action"] == "plugin_status":
                return self._plugin_status(command["plugin"])
            else:
                return {"error": "Unknown action"}
        except Exception as e:
            self.monitor.REQUEST_COUNT.labels('manager', 'failed').inc()
            raise

    def _list_plugins(self) -> Dict[str, Any]:
        return {
            "plugins": list(self.context.plugin_manager.plugins.keys()),
            "stats": {
                "ready": sum(1 for p in self.context.plugin_manager.plugins.values() if p.is_ready()),
                "total": len(self.context.plugin_manager.plugins)
            }
        }

    def _plugin_status(self, plugin_name: str) -> Dict[str, Any]:
        plugin = self.context.plugin_manager.get_plugin(plugin_name)
        return {
            "exists": plugin is not None,
            "ready": plugin.is_ready() if plugin else False,
            "metadata": plugin.metadata if plugin else None
        }


=== core\integrations\ollama.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="ollama",
            version="0.5.0",
            required_config={
                "base_url": str,
                "default_model": str,
                "batch_size": int
            },
            dependencies=["requests"],
            description="Ollama with experimental batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.default_model = self.config.get("default_model", "llama2")
        self.batch_size = self.config.get("batch_size", 1)  # Default to no batching
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": input_data.get("model", self.default_model),
                "prompt": input_data["prompt"],
                "stream": False,
                **input_data.get("options", {})
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        # Note: Ollama doesn't natively support batching, so we parallelize
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.batch_size) as executor:
            results = list(executor.map(
                lambda p: self._single_execute({"prompt": p}),
                input_data["prompt"]
            ))
        return {"responses": results}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("ollama").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/tags", timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "models": [m["name"] for m in resp.json().get("models", [])]
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\textgen.py ===
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="textgen",
            version="0.4.0",
            required_config={
                "base_url": str,
                "api_key": str,
                "batch_size": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Text Generation WebUI API with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.api_key = self.config["api_key"]
        self.batch_size = self.config.get("batch_size", 1)
        self.timeout = self.config.get("timeout", 30)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start_time)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/v1/generate",
            headers=self.headers,
            json={
                "prompt": input_data["prompt"],
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        """Execute multiple prompts as a batch"""
        responses = []
        prompts = input_data["prompt"]
        
        # Process in chunks of batch_size
        for i in range(0, len(prompts), self.batch_size):
            chunk = prompts[i:i + self.batch_size]
            response = requests.post(
                f"{self.base_url}/api/v1/generate_batch",  # Note: Your API must support this endpoint
                headers=self.headers,
                json={
                    "prompts": chunk,
                    **input_data.get("parameters", {})
                },
                timeout=self.timeout
            )
            response.raise_for_status()
            responses.extend(response.json()["results"])
            
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("textgen").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/v1/model", 
                             headers=self.headers,
                             timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "model": resp.json().get("model_name", "unknown")
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status


=== core\integrations\vllm.py ===
from vllm import LLM, SamplingParams
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="vllm",
            version="0.4.0",
            required_config={
                "model": str,
                "tensor_parallel_size": int,
                "gpu_memory_utilization": float,
                "max_batch_size": int
            },
            dependencies=["vllm>=0.2.0"],
            description="High-performance batched inference"
        )

    def initialize(self):
        try:
            self.llm = LLM(
                model=self.config["model"],
                tensor_parallel_size=self.config.get("tensor_parallel_size", 1),
                gpu_memory_utilization=self.config.get("gpu_memory_utilization", 0.9),
                max_num_batched_tokens=self.config.get("max_batch_size", 2560)
            )
            self.default_params = SamplingParams(
                temperature=0.8,
                top_p=0.95
            )
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"vLLM init failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            params = self.default_params.copy()
            if "parameters" in input_data:
                params = SamplingParams(**input_data["parameters"])
            
            if isinstance(input_data["prompt"], list):
                outputs = self.llm.generate(input_data["prompt"], params)
                return {
                    "responses": [o.outputs[0].text for o in outputs]
                }
            else:
                output = self.llm.generate([input_data["prompt"]], params)
                return {"response": output[0].outputs[0].text}
        finally:
            self._log_latency(start)

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("vllm").observe(time.time() - start_time)

    def health_check(self):
        status = super().health_check()
        status["gpu_utilization"] = self._get_gpu_stats()
        status["batch_capacity"] = self.llm.llm_engine.scheduler_config.max_num_batched_tokens
        return status


=== core\interface.py ===


=== core\monitoring\service.py ===
import time
from prometheus_client import start_http_server, Counter, Gauge, Histogram

class Monitoring:
    def __init__(self, port=9090):
        # Metrics Definitions
        self.REQUEST_COUNT = Counter(
            'llm_requests_total',
            'Total API requests',
            ['module', 'status']
        )
        
        self.LATENCY = Histogram(
            'llm_response_latency_seconds',
            'Response latency distribution',
            ['provider']
        )
        
        self.CACHE_HITS = Gauge(
            'cache_hit_ratio',
            'Current cache hit percentage'
        )
        
        start_http_server(port)

    def track_request(self, module: str):
        """Decorator to monitor request metrics"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.time()
                try:
                    result = func(*args, **kwargs)
                    self.REQUEST_COUNT.labels(module, 'success').inc()
                    return result
                except Exception:
                    self.REQUEST_COUNT.labels(module, 'failed').inc()
                    raise
                finally:
                    self.LATENCY.labels(module).observe(time.time() - start)
            return wrapper
        return decorator

    def update_cache_metrics(self, hits: int, misses: int):
        """Update cache performance metrics"""
        self.CACHE_HITS.set(hits / max(hits + misses, 1))


=== core\orchestration\budget_router.py ===
from typing import Dict, Literal
from ..performance.cost import CostMonitor

class BudgetRouter:
    def __init__(self, cost_monitor: CostMonitor):
        self.cost = cost_monitor

    def select_llm(self, query: Dict) -> Literal['premium', 'standard', 'local']:
        """Choose LLM tier based on budget and query criticality"""
        forecast = self.cost.get_spend_forecast()
        criticality = query.get("criticality", 0.5)
        
        if forecast["burn_rate"] > forecast["budget_remaining"] / 7:  # Weekly burn
            return 'local'
        elif criticality > 0.8 and forecast["budget_remaining"] > 50:
            return 'premium'
        else:
            return 'standard'


=== core\orchestration\load_balancer.py ===
from typing import Dict, List
import numpy as np
from collections import deque
from ..performance.tracker import PerformanceTracker

class LoadBalancer:
    def __init__(self, tracker: PerformanceTracker):
        self.tracker = tracker
        self.weights = {}  # Provider -> weight
        self.history = deque(maxlen=100)  # Tracks last 100 routing decisions

    def update_weights(self):
        """Calculate new weights based on performance"""
        metrics = self.tracker.get_provider_metrics()
        total = sum(m['requests_per_second'] / (m['avg_latency'] + 1e-6) for m in metrics.values())
        
        self.weights = {
            provider: (m['requests_per_second'] / (m['avg_latency'] + 1e-6)) / total
            for provider, m in metrics.items()
        }

    def select_provider(self, query: Dict) -> str:
        """Select provider using weighted random choice"""
        providers = list(self.weights.keys())
        weights = list(self.weights.values())
        choice = np.random.choice(providers, p=weights)
        self.history.append((query['content'][:50], choice))
        return choice


=== core\orchestration\sla_router.py ===
from typing import Dict, Literal
from dataclasses import dataclass
from ..performance.cost import CostMonitor
from ..performance.tracker import PerformanceTracker
import numpy as np

@dataclass
class SLATier:
    name: str
    min_accuracy: float
    max_latency: float  # seconds
    allowed_providers: list
    cost_multiplier: float = 1.0

class SLARouter:
    def __init__(self, cost_monitor: CostMonitor, perf_tracker: PerformanceTracker):
        self.cost = cost_monitor
        self.performance = perf_tracker
        
        # Define service tiers
        self.tiers = {
            "critical": SLATier(
                name="critical",
                min_accuracy=0.95,
                max_latency=1.5,
                allowed_providers=["gpt-4", "claude-2", "vllm"],
                cost_multiplier=2.0
            ),
            "standard": SLATier(
                name="standard",
                min_accuracy=0.85,
                max_latency=3.0,
                allowed_providers=["gpt-3.5", "claude-instant", "llama2"]
            ),
            "economy": SLATier(
                name="economy",
                min_accuracy=0.70,
                max_latency=5.0,
                allowed_providers=["llama2", "local"]
            )
        }

    def select_provider(self, query: Dict) -> Dict[str, str]:
        """Select optimal provider based on SLA and budget"""
        # Determine SLA tier
        tier = self._determine_tier(query)
        
        # Get eligible providers
        candidates = [
            p for p in self.performance.get_available_providers()
            if p in tier.allowed_providers
        ]
        
        # Rank by performance/cost tradeoff
        ranked = sorted(
            candidates,
            key=lambda p: self._score_provider(p, tier)
        )
        
        return {
            "provider": ranked[0],
            "tier": tier.name,
            "reason": f"Best match for {tier.name} SLA"
        }

    def _determine_tier(self, query: Dict) -> SLATier:
        """Auto-select SLA tier based on query properties"""
        if query.get("user_priority") == "high":
            return self.tiers["critical"]
        
        # Auto-detect critical queries
        if ("error" in query.get("intent", "") or 
            "production" in query.get("context", "")):
            return self.tiers["critical"]
            
        # Budget-aware fallback
        budget_status = self.cost.get_spend_forecast()
        if budget_status["burn_rate"] > budget_status["budget_remaining"] / 10:
            return self.tiers["economy"]
            
        return self.tiers["standard"]

    def _score_provider(self, provider: str, tier: SLATier) -> float:
        """Score providers (0-1) based on SLA fit"""
        metrics = self.performance.get_provider_metrics(provider)
        
        # Normalized performance score (higher better)
        accuracy_score = metrics["accuracy"] / tier.min_accuracy
        latency_score = tier.max_latency / max(metrics["latency"], 0.1)
        
        # Cost penalty (lower better)
        cost_rate = self.cost._get_rate(provider.split('-')[0], provider)
        cost_penalty = cost_rate["input"] * tier.cost_multiplier
        
        return np.mean([accuracy_score, latency_score]) / cost_penalty


=== core\orchestrator.py ===
from typing import Dict, List
from shared.schemas import Query, Response
from modules.base_module import BaseModule
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.validation.quality_gates import QualityValidator
from core.orchestration.sla_router import SLARouter
from core.orchestration.load_balancer import LoadBalancer
from core.reasoning.engine import HybridEngine
from core.prediction.warmer import CacheWarmer
from core.monitoring.service import Monitoring
from core.processing.batcher import AdaptiveBatcher
import logging
import asyncio
import numpy as np

class Orchestrator:
    def __init__(
        self,
        validator: QualityValidator,
        sla_router: SLARouter,
        load_balancer: LoadBalancer,
        registry,
        healing_controller: SelfHealingController,
        context_manager: ContextManager,
        reasoning_engine: HybridEngine,
        monitoring: Monitoring
    ):
        self.validator = validator
        self.sla_router = sla_router
        self.load_balancer = load_balancer
        self.registry = registry
        self.healing = healing_controller
        self.context = context_manager
        self.reasoning = reasoning_engine
        self.monitor = monitoring
        self.logger = logging.getLogger("orchestrator")
        self.cache_warmer = CacheWarmer(self, self.context.cache_predictor)
        self.batcher = AdaptiveBatcher(
            max_batch_size=self.context.config.get("batching.max_size", 8),
            max_wait_ms=self.context.config.get("batching.max_wait_ms", 50)
        )
        self._setup_fallback_strategies()
        asyncio.create_task(self.batcher.background_flush())
        asyncio.create_task(self._update_balancer_weights())

    def _setup_fallback_strategies(self):
        self.fallback_map = {
            "python": "code_generic",
            "csharp": "code_generic",
            "math": "math_basic",
            "chat": "generic"
        }

    async def _update_balancer_weights(self):
        """Periodically update load balancer weights"""
        while True:
            await asyncio.sleep(
                self.context.config.get("load_balancing.update_interval", 10)
            )
            if len(self.load_balancer.history) >= self.context.config.get("load_balancing.min_requests", 20):
                self.load_balancer.update_weights()

    @self.monitor.track_request('orchestrator')
    async def route_query(self, query: Query) -> Response:
        """Enhanced query processing pipeline"""
        try:
            # 1. Get context and routing info
            pre_context = self.context.get_context(query.content)
            
            # Dynamic provider selection (NEW)
            if query.metadata.get("priority", 0) > 0:
                # High-priority uses SLA routing
                routing_decision = self.sla_router.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "user_priority": query.metadata.get("priority", "normal")
                })
                provider = routing_decision["provider"]
            else:
                # Normal traffic uses load balancing
                provider = self.load_balancer.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "priority": query.metadata.get("priority", 0)
                })
                routing_decision = {"provider": provider, "tier": "balanced"}
            
            query.provider = provider

            # 2. Hybrid reasoning
            reasoning_result = await self.reasoning.process({
                "query": query.content,
                "context": pre_context,
                "llm_preference": provider
            })

            # 3. Module processing with quality gates
            module = self._select_module(
                query,
                pre_context,
                reasoning_source=reasoning_result.get("source")
            )
            enriched_query = query.with_additional_context(reasoning_result)
            
            # Process with batching if enabled
            if query.metadata.get("allow_batching", True):
                batch = await self.batcher.add_query(
                    enriched_query.model_dump(),
                    priority=query.metadata.get("priority", 0)
                )
                if len(batch) > 1:
                    return await self._batch_process(batch)

            raw_response = await module.process(enriched_query)

            # 4. Validate and enhance response
            validation = self.validator.validate(raw_response)
            if not validation["passed"]:
                return await self._handle_quality_failure(enriched_query, validation)

            final_response = self._augment_response(
                validation["original_response"],
                pre_context,
                reasoning_metadata={
                    "sla_tier": routing_decision["tier"],
                    "provider": provider,
                    "reasoning_path": reasoning_result["source"]
                }
            )

            # 5. Learn and cache
            self.context.process_interaction(
                query,
                final_response,
                metadata={
                    "sla_tier": routing_decision["tier"],
                    "reasoning_source": reasoning_result["source"],
                    "provider": provider
                }
            )
            asyncio.create_task(self.cache_warmer.warm_cache(query.content))

            return final_response

        except Exception as e:
            self.logger.error(f"Routing failed: {str(e)}")
            return await self._handle_failure(query, e)

    async def _batch_process(self, batch: List[Dict]) -> Response:
        """Process batched queries through LLM"""
        try:
            # Get first provider that supports batching
            provider = next(
                p for p in {
                    query.get("provider") for query in batch
                } 
                if (plugin := self.context.plugin_manager.get_plugin(p)) 
                and plugin.supports_batching
            )
            
            llm = self.context.plugin_manager.get_plugin(provider)
            combined = [q["content"] for q in batch]
            responses = await llm.batch_complete(combined)
            
            # Return only the response for our original query
            original_query = batch[0]["content"]
            return next(
                Response(content=r) 
                for q, r in zip(combined, responses)
                if q == original_query
            )
        except Exception as e:
            self.logger.warning(f"Batch processing failed: {str(e)}")
            return await self.route_query(Query(**batch[0]))

    async def _handle_quality_failure(self, query: Query, validation: Dict) -> Response:
        """Process failed quality checks"""
        self.logger.warning(f"Quality check failed: {validation['checks']}")
        return await self._retry_with_stricter_llm(query)

    def _select_module(self, query: Query, context: dict, reasoning_source: str = None) -> BaseModule:
        """Enhanced module selection"""
        if reasoning_source == "graph":
            return self.registry.get_module("knowledge")
        
        if any(match["type"] == "code" for match in context["matches"]):
            lang = self._detect_language(context["matches"])
            return self.registry.get_module(f"code_{lang}")
            
        return self.registry.get_module("chat")
        
    def _detect_language(self, matches: List[dict]) -> str:
        """Detect programming language from knowledge matches"""
        lang_keywords = {
            "python": ["def", "import", "lambda"],
            "csharp": ["var", "using", "namespace"]
        }
        
        for match in matches:
            content = match.get("content", "").lower()
            for lang, keywords in lang_keywords.items():
                if any(kw in content for kw in keywords):
                    return lang
        return "generic"
        
    def _augment_response(self, response: Response, context: dict, reasoning_metadata: dict = None) -> Response:
        """Enhance response with contextual knowledge"""
        if not response.metadata:
            response.metadata = {}
            
        response.metadata.update({
            "context": {
                "matched_concepts": [
                    {"id": m["node_id"], "content": m["content"]}
                    for m in context["matches"][:3]
                ],
                "related_concepts": [
                    {"id": n["id"], "content": n["content"]}
                    for n in context["related"][:5]
                ]
            },
            "processing": reasoning_metadata or {}
        })
        return response
        
    async def _handle_failure(self, query: Query, error: Exception) -> Response:
        """Handle routing failures with fallback logic"""
        module_id = query.content_type.split("_")[-1]
        fallback_id = self.fallback_map.get(module_id, "generic")
        
        if fallback := self.registry.get_module(fallback_id):
            response = await fallback.process(query)
            self.context.process_interaction(query, response)
            return response
            
        raise RuntimeError("All fallback strategies failed")

    async def _retry_with_stricter_llm(self, query: Query) -> Response:
        """Fallback strategy for quality failures"""
        query.metadata["require_quality"] = True
        return await self.route_query(query)


=== core\performance\cost.py ===
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, Literal, Optional
import warnings

Provider = Literal['openai', 'anthropic', 'ollama', 'huggingface']

class CostMonitor:
    def __init__(self, config: Dict):
        self.config = config
        self.cost_db = Path("data/cost_tracking.json")
        self._init_db()
        self.current_spend = 0.0
        self._load_current_period()

    def _init_db(self):
        """Initialize cost database with default structure"""
        if not self.cost_db.exists():
            with open(self.cost_db, 'w') as f:
                json.dump({
                    "monthly_budget": self.config.get("monthly_budget", 100.0),
                    "periods": [],
                    "provider_rates": {
                        "openai": {"gpt-4": 0.03, "gpt-3.5": 0.002},
                        "anthropic": {"claude-2": 0.0465, "claude-instant": 0.0163},
                        "ollama": {"llama2": 0.0, "mistral": 0.0},
                        "huggingface": {"default": 0.0}
                    }
                }, f)

    def _load_current_period(self):
        """Load or create current billing period"""
        with open(self.cost_db, 'r') as f:
            data = json.load(f)
        
        current_date = datetime.now().strftime("%Y-%m")
        if not data["periods"] or data["periods"][-1]["period"] != current_date:
            data["periods"].append({
                "period": current_date,
                "total_spend": 0.0,
                "breakdown": {p: 0.0 for p in data["provider_rates"].keys()}
            })
        
        self.current_period = data["periods"][-1]
        self.current_spend = self.current_period["total_spend"]

    def record_llm_call(
        self,
        provider: Provider,
        model: str,
        input_tokens: int,
        output_tokens: int
    ):
        """Calculate and record API call costs"""
        rate = self._get_rate(provider, model)
        cost = (input_tokens * rate["input"] + output_tokens * rate["output"]) / 1000
        
        with open(self.cost_db, 'r+') as f:
            data = json.load(f)
            current = data["periods"][-1]
            current["total_spend"] += cost
            current["breakdown"][provider] += cost
            self.current_spend = current["total_spend"]
            
            # Check budget threshold (80% warning)
            if current["total_spend"] > data["monthly_budget"] * 0.8:
                warnings.warn(
                    f"Approaching budget limit: {current['total_spend']:.2f}/{data['monthly_budget']}",
                    RuntimeWarning
                )
            
            f.seek(0)
            json.dump(data, f, indent=2)

    def _get_rate(self, provider: Provider, model: str) -> Dict[str, float]:
        """Get current token rates for a provider/model"""
        with open(self.cost_db, 'r') as f:
            rates = json.load(f)["provider_rates"]
            provider_rates = rates.get(provider, {})
            return {
                "input": provider_rates.get(model, provider_rates.get("default", 0.0)),
                "output": provider_rates.get(model, provider_rates.get("default", 0.0))
            }

    def get_spend_forecast(self) -> Dict:
        """Predict end-of-period spend"""
        now = datetime.now()
        days_in_month = (now.replace(month=now.month+1, day=1) - timedelta(days=1)).day
        days_elapsed = now.day
        daily_avg = self.current_spend / days_elapsed
        
        return {
            "current_spend": self.current_spend,
            "projected_spend": daily_avg * days_in_month,
            "budget_remaining": self.config["monthly_budget"] - self.current_spend,
            "burn_rate": daily_avg
        }


=== core\performance\hashing.py ===
import hashlib
import json
from typing import Dict, Any

class QueryHasher:
    @staticmethod
    def hash_query(query: Dict[str, Any]) -> str:
        """Create consistent hash for similar queries"""
        normalized = {
            "code": query.get("code", "").strip(),
            "intent": query.get("intent", ""),
            "context": sorted(query.get("context", []))
        }
        return hashlib.sha256(
            json.dumps(normalized, sort_keys=True).encode()
        ).hexdigest()


=== core\performance\tracker.py ===
from datetime import datetime
from pathlib import Path
import json
import statistics
from typing import Dict, List, Literal

SolutionSource = Literal['graph', 'rule', 'llm', 'learned_rule']

class PerformanceTracker:
    def __init__(self):
        self.metrics_path = Path("data/performance_metrics.json")
        self._init_storage()
        self.session_metrics: List[Dict] = []

    def _init_storage(self):
        self.metrics_path.parent.mkdir(exist_ok=True)
        if not self.metrics_path.exists():
            with open(self.metrics_path, 'w') as f:
                json.dump({"sessions": []}, f)

    def record_metric(
        self,
        source: SolutionSource,
        latency: float,
        success: bool,
        query_hash: str
    ):
        """Record performance metrics for each solution"""
        metric = {
            "timestamp": datetime.utcnow().isoformat(),
            "source": source,
            "latency_ms": latency * 1000,
            "success": success,
            "query_hash": query_hash[:8]  # Truncated for privacy
        }
        self.session_metrics.append(metric)

    def get_recommended_source(self, query_hash: str) -> SolutionSource:
        """Determine optimal solution source based on history"""
        history = self._load_history()
        
        # Check for identical past queries
        if query_hash:
            for m in reversed(history):
                if m['query_hash'] == query_hash:
                    if m['success']:
                        return m['source']
                    break

        # Calculate source effectiveness
        success_rates = {}
        latencies = {}
        
        for source in ['graph', 'rule', 'llm', 'learned_rule']:
            source_metrics = [m for m in history if m['source'] == source]
            if source_metrics:
                success_rates[source] = sum(
                    1 for m in source_metrics if m['success']
                ) / len(source_metrics)
                latencies[source] = statistics.median(
                    [m['latency_ms'] for m in source_metrics]
                )

        # Prioritize by success then speed
        if success_rates:
            best_source = max(
                success_rates.keys(),
                key=lambda k: (success_rates[k], -latencies[k])
            )
            return best_source
        return 'llm'  # Default fallback

    def _load_history(self) -> List[Dict]:
        """Load historical metrics"""
        with open(self.metrics_path, 'r') as f:
            data = json.load(f)
            return data['sessions'] + self.session_metrics
            
    def get_provider_metrics(self) -> Dict[str, Dict]:
        """Calculate real-time performance metrics"""
        history = self._load_history()
        window = [m for m in history if m['timestamp'] > time.time() - 60]  # Last 60s
        
        metrics = {}
        for provider in set(m['source'] for m in window):
            provider_metrics = [m for m in window if m['source'] == provider]
            metrics[provider] = {
                'requests_per_second': len(provider_metrics) / 60,
                'avg_latency': np.mean([m['latency_ms'] for m in provider_metrics]) / 1000,
                'error_rate': sum(1 for m in provider_metrics if not m['success']) / len(provider_metrics)
            }
        return metrics

    def get_available_providers(self) -> List[str]:
        """List all currently enabled providers"""
        return ["gpt-4", "gpt-3.5", "claude-2", "llama2", "local"]  # From config


=== core\plugin.py ===
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Type
from dataclasses import dataclass
import importlib
import logging
from pathlib import Path

# ---------- Core Definitions ----------
@dataclass
class PluginMetadata:
    name: str
    version: str
    author: str = "Unknown"
    compatible_versions: str = ">=0.1.0"
    required_config: Dict[str, Any] = None
    dependencies: List[str] = None
    description: str = ""

class PluginBase(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metadata = self.get_metadata()
        self._initialized = False
        self.logger = logging.getLogger(f"plugin.{self.metadata.name}")
        self._validate_config()

    # ---------- Required Interface ----------
    @abstractmethod
    def get_metadata(self) -> PluginMetadata:
        """Return plugin metadata"""
        pass

    @abstractmethod
    def initialize(self) -> bool:
        """Initialize plugin resources"""
        pass

    @abstractmethod
    def execute(self, input_data: Any) -> Any:
        """Main execution method"""
        pass

    # ---------- Core Functionality ----------
    def is_ready(self) -> bool:
        """Check if plugin is operational"""
        return self._initialized

    def cleanup(self):
        """Release all resources"""
        self._initialized = False
        self.logger.info(f"Cleanup completed for {self.metadata.name}")

    # ---------- Advanced Features ----------
    def _validate_config(self):
        """Validate configuration against metadata requirements"""
        if self.metadata.required_config:
            for field, expected_type in self.metadata.required_config.items():
                if field not in self.config:
                    raise ValueError(f"Missing config field: {field}")
                if not isinstance(self.config[field], expected_type):
                    raise TypeError(
                        f"Config field {field} requires {expected_type}, "
                        f"got {type(self.config[field])}"
                    )

    def health_check(self) -> Dict[str, Any]:
        """Detailed health report"""
        return {
            "name": self.metadata.name,
            "ready": self.is_ready(),
            "config_keys": list(self.config.keys()),
            "dependencies": self.metadata.dependencies or []
        }

    # ---------- Context Manager Support ----------
    def __enter__(self):
        if not self._initialized:
            self._initialized = self.initialize()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

# ---------- Plugin Manager ----------
class PluginManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.plugins: Dict[str, PluginBase] = {}
        self._discover_plugins()

    def _discover_plugins(self):
        """Discover and initialize all available plugins"""
        plugin_dir = Path(__file__).parent / "integrations"
        for py_file in plugin_dir.glob("*.py"):
            if py_file.stem == "__init__":
                continue
            
            try:
                module = importlib.import_module(
                    f"core.integrations.{py_file.stem}"
                )
                if hasattr(module, "Plugin"):
                    plugin_class = getattr(module, "Plugin")
                    if issubclass(plugin_class, PluginBase):
                        self._load_plugin(plugin_class)
            except Exception as e:
                logging.error(f"Failed to load {py_file.stem}: {str(e)}")

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Initialize and register a plugin"""
        plugin_name = plugin_class.__name__.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            plugin = plugin_class(plugin_config)
            if plugin.initialize():
                self.plugins[plugin_name] = plugin
                logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")

    def get_plugin(self, name: str) -> Optional[PluginBase]:
        """Retrieve a loaded plugin by name"""
        return self.plugins.get(name.lower())

    def list_plugins(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all plugins"""
        return {
            name: {
                "metadata": plugin.metadata,
                "ready": plugin.is_ready()
            }
            for name, plugin in self.plugins.items()
        }
    
    def reload_plugin(self, name: str) -> bool:
        """Hot-reload a plugin by name"""
        if name not in self.plugins:
            return False

        plugin = self.plugins[name]
        plugin.cleanup()
        
        try:
            module = importlib.import_module(f"core.integrations.{name}")
            importlib.reload(module)
            plugin_class = getattr(module, "Plugin")
            self._load_plugin(plugin_class)
            return True
        except Exception as e:
            logging.error(f"Failed to reload {name}: {str(e)}")
            return False

    def _resolve_dependencies(self, metadata: PluginMetadata) -> bool:
        """Install missing dependencies automatically"""
        if not metadata.dependencies:
            return True

        missing = []
        for dep in metadata.dependencies:
            try:
                req = requirements.Requirement(dep)
                importlib.import_module(req.name)
            except (ImportError, requirements.InvalidRequirement):
                missing.append(dep)

        if missing:
            logging.info(f"Installing dependencies: {', '.join(missing)}")
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", *missing],
                    stdout=subprocess.DEVNULL
                )
                return True
            except subprocess.CalledProcessError:
                logging.error(f"Failed to install dependencies: {missing}")
                return False
        return True

    def _check_version_compatibility(self, metadata: PluginMetadata) -> bool:
        """Verify plugin matches core version requirements"""
        try:
            core_req = requirements.Requirement(f"open_llm{metadata.compatible_versions}")
            current_version = requirements.Requirement(f"open_llm=={self.config['version']}")
            return current_version.specifier in core_req.specifier
        except requirements.InvalidRequirement:
            logging.warning(f"Invalid version spec: {metadata.compatible_versions}")
            return False

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Enhanced plugin loading with new features"""
        metadata = plugin_class({}).get_metadata()
        
        if not self._check_version_compatibility(metadata):
            logging.error(f"Version mismatch for {metadata.name}")
            return

        if not self._resolve_dependencies(metadata):
            logging.error(f"Missing dependencies for {metadata.name}")
            return

        plugin_name = metadata.name.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            with plugin_class(plugin_config) as plugin:
                if plugin.is_ready():
                    self.plugins[plugin_name] = plugin
                    logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")


=== core\prediction\cache.py ===
# core/prediction/cache.py
from typing import List, Dict
import numpy as np
from collections import deque

class CachePredictor:
    def __init__(self, context_manager, max_predictions=5):
        self.context = context_manager
        self.query_buffer = deque(maxlen=10)
        self.predictions = []
        
    def analyze_query_stream(self, new_query: str) -> List[str]:
        """Predict next 3 likely questions"""
        self.query_buffer.append(new_query)
        
        # 1. Get similar historical sequences
        similar_flows = self._find_similar_flows()
        
        # 2. Generate predictions (simplified example)
        return [
            "How to debug this?",
            "Better implementation?",
            "Related documentation"
        ][:max_predictions]

    def _find_similar_flows(self) -> List[Dict]:
        """Find similar query patterns in history"""
        # Implementation using your KnowledgeGraph
        return self.context.graph.find_similar_sequences(list(self.query_buffer))


=== core\prediction\warmer.py ===
# core/prediction/warmer.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

class CacheWarmer:
    def __init__(self, orchestrator, cache_predictor):
        self.orchestrator = orchestrator
        self.predictor = cache_predictor
        self.executor = ThreadPoolExecutor(2)

    async def warm_cache(self, current_query: str):
        """Pre-generate responses for predicted queries"""
        predicted = self.predictor.analyze_query_stream(current_query)
        
        # Run in background thread
        await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self._generate_responses,
            predicted
        )

    def _generate_responses(self, queries: List[str]):
        for query in queries:
            self.orchestrator.route_query(Query(content=query))


=== core\processing\batcher.py ===
# core/processing/batcher.py
from typing import List, Dict
import heapq
from dataclasses import dataclass, field
from sortedcontainers import SortedList

@dataclass(order=True)
class BatchItem:
    priority: int
    query: Dict = field(compare=False)
    created_at: float = field(default_factory=time.time, compare=False)

class AdaptiveBatcher:
    def __init__(self, max_batch_size=8, max_wait_ms=50):
        self.max_batch_size = max_batch_size
        self.max_wait = max_wait_ms / 1000
        self.pending = SortedList(key=lambda x: -x.priority)
        self.semaphore = asyncio.Semaphore(0)

    async def add_query(self, query: Dict, priority: int = 0) -> List[Dict]:
        """Add query to current batch, return completed batches if ready"""
        heapq.heappush(self.pending, BatchItem(priority, query))
        
        if len(self.pending) >= self.max_batch_size:
            return self._release_batch()
        
        await asyncio.wait_for(
            self.semaphore.acquire(),
            timeout=self.max_wait
        )
        return self._release_batch()

    def _release_batch(self) -> List[Dict]:
        """Extract queries for processing"""
        batch = [item.query for item in 
                heapq.nsmallest(self.max_batch_size, self.pending)]
        del self.pending[:len(batch)]
        return batch

    async def background_flush(self):
        """Periodically flush partial batches"""
        while True:
            await asyncio.sleep(self.max_wait)
            if self.pending:
                self.semaphore.release()


=== core\reasoning\engine.py ===
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from ..knowledge.graph import KnowledgeGraph
from ..context import ContextManager
import logging

class HybridEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph = context.graph
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.logger = logging.getLogger("reasoning.engine")
        self._init_rules()
        self.learning = SelfLearningEngine(context)
        self.performance = PerformanceTracker()
        self.query_hasher = QueryHasher()

    def _init_rules(self):
        """Load rule-based patterns"""
        self.rules = {
            'list_comp': {
                'pattern': '[x for x in {iterable} if {condition}]',
                'vars': ['iterable', 'condition']
            },
            'context_mgr': {
                'pattern': 'with {expr} as {var}:',
                'vars': ['expr', 'var']
            }
        }

    async def process(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """Main reasoning pipeline"""
        # Stage 1: Local Graph Check
        if graph_result := await self._check_graph(query):
            return graph_result

        # Stage 2: Rule Application
        if rule_result := self._apply_rules(query):
            return rule_result

        # Stage 3: LLM Fallback
        return await self._query_llm(query)
        
        result = await self._process_query(query)
        
        # Self-learning hook
        if result.get('success', True):
            self.learning.observe_solution(
                problem=query.get('code', ''),
                solution=str(result),
                source=result.get('source', 'llm')
            )
        
        return result
        
        query_hash = self.query_hasher.hash_query(query)
        recommended_source = self.performance.get_recommended_source(query_hash)
        
        # Route based on performance
        if recommended_source == 'graph':
            result = await self._check_graph(query)
        elif recommended_source == 'rule':
            result = self._apply_rules(query)
        else:
            result = await self._query_llm(query)

        # Record metrics
        self.performance.record_metric(
            source=result.get('source', 'llm'),
            latency=result['latency'],
            success=result.get('success', True),
            query_hash=query_hash
        )
        
        return result

    async def _check_graph(self, query: Dict) -> Optional[Dict]:
        """Check knowledge graph for solutions"""
        try:
            if 'code_context' in query:
                matches = self.graph.find_similar(
                    query['code_context'],
                    threshold=0.7
                )
                if matches:
                    return {'source': 'graph', 'result': matches[0]['solution']}
        except Exception as e:
            self.logger.error(f"Graph query failed: {str(e)}")
        return None

    def _apply_rules(self, query: Dict) -> Optional[Dict]:
        """Apply pre-defined coding patterns"""
        code = query.get('code', '')
        for rule_name, rule in self.rules.items():
            if all(var in code for var in rule['vars']):
                return {
                    'source': 'rule',
                    'rule': rule_name,
                    'template': rule['pattern'].format(**query)
                }
        return None

    async def _query_llm(self, query: Dict) -> Dict:
        """Route to best-suited LLM"""
        llm_pref = query.get('llm', self.context.config.get('default_llm'))
        return await self.context.plugin_manager.execute_llm(
            llm_pref,
            self._build_llm_payload(query)
        )

    def _build_llm_payload(self, query: Dict) -> Dict:
        """Enhance query with context"""
        return {
            **query,
            'context': self.context.get_relevant_context(query),
            'history': self.context.get_interaction_history()
        }


=== core\reasoning\rules.py ===
CODE_PATTERNS = {
    "list_comprehension": {
        "pattern": "[x for x in iterable if condition]",
        "transform": lambda match: {
            "template": match["pattern"],
            "variables": ["iterable", "condition"]
        }
    },
    "context_manager": {
        "pattern": "with expression as var:",
        "transform": lambda match: {
            "solution": f"with {match['expression']} as {match['var']}:"
        }
    }
}

class RuleEngine:
    @staticmethod
    def apply_pattern(code: str) -> Dict|None:
        """Match code against known patterns"""
        for pattern_name, pattern_data in CODE_PATTERNS.items():
            if pattern_data["pattern"] in code:
                return pattern_data["transform"](code)
        return None


=== core\self_healing.py ===
from dataclasses import dataclass
from enum import Enum, auto
import time
import asyncio
from typing import Dict, List, Optional
from modules.base_module import BaseModule

class HealthStatus(Enum):
    HEALTHY = auto()
    DEGRADED = auto()
    FAILED = auto()

@dataclass
class ModuleHealth:
    module_id: str
    status: HealthStatus
    last_checked: float
    failure_count: int = 0
    last_error: Optional[str] = None

class SelfHealingController:
    def __init__(self, registry):
        self.registry = registry
        self.health_status: Dict[str, ModuleHealth] = {}
        self._monitor_task = None
        
    async def start_monitoring(self, interval=60):
        """Start periodic health checks"""
        self._monitor_task = asyncio.create_task(self._monitor_loop(interval))
        
    async def _monitor_loop(self, interval):
        while True:
            await self.check_all_modules()
            await asyncio.sleep(interval)
            
    async def check_all_modules(self):
        """Check health of all registered modules"""
        for module_id, module in self.registry._instances.items():
            try:
                health_data = module.health_check()
                status = (
                    HealthStatus.DEGRADED if health_data.get("degraded", False) 
                    else HealthStatus.HEALTHY
                )
                self.health_status[module_id] = ModuleHealth(
                    module_id=module_id,
                    status=status,
                    last_checked=time.time()
                )
            except Exception as e:
                self._handle_module_failure(module_id, str(e))
                
    def _handle_module_failure(self, module_id: str, error: str):
        """Process module failure and initiate recovery"""
        if module_id not in self.health_status:
            self.health_status[module_id] = ModuleHealth(
                module_id=module_id,
                status=HealthStatus.FAILED,
                last_checked=time.time(),
                failure_count=1,
                last_error=error
            )
        else:
            self.health_status[module_id].failure_count += 1
            self.health_status[module_id].last_error = error
            self.health_status[module_id].status = HealthStatus.FAILED
            
        if self.health_status[module_id].failure_count > 3:
            self._attempt_recovery(module_id)
            
    def _attempt_recovery(self, module_id: str):
        """Execute recovery procedures for failed module"""
        module = self.registry._instances[module_id]
        try:
            # Attempt reinitialization
            module.initialize()
            self.health_status[module_id].status = HealthStatus.HEALTHY
            self.health_status[module_id].failure_count = 0
        except Exception as e:
            # If recovery fails, disable module temporarily
            self.health_status[module_id].status = HealthStatus.FAILED
            # TODO: Notify operators
            
    def get_available_modules(self) -> List[str]:
        """List modules currently available for routing"""
        return [
            module_id for module_id, health in self.health_status.items()
            if health.status != HealthStatus.FAILED
        ]


=== core\self_learning\engine.py ===
from typing import Dict, Any
from pathlib import Path
import json
import hashlib
from datetime import datetime
from ..knowledge.graph import KnowledgeGraph

class SelfLearningEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph: KnowledgeGraph = context.graph
        self.learned_rules_path = Path("data/learned_rules.json")
        self._init_storage()

    def _init_storage(self):
        """Ensure learning storage exists"""
        self.learned_rules_path.parent.mkdir(exist_ok=True)
        if not self.learned_rules_path.exists():
            with open(self.learned_rules_path, 'w') as f:
                json.dump({"rules": []}, f)

    def observe_solution(self, problem: str, solution: str, source: str):
        """Record successful solutions"""
        problem_hash = hashlib.sha256(problem.encode()).hexdigest()
        
        # Store in knowledge graph
        self.graph.cache_solution(
            problem=problem,
            solution=solution,
            metadata={
                "source": source,
                "timestamp": datetime.utcnow().isoformat(),
                "usage_count": 0
            }
        )
        
        # Auto-generate rules for pattern-like solutions
        if self._is_pattern_candidate(solution):
            self._extract_rule(problem, solution)

    def _is_pattern_candidate(self, solution: str) -> bool:
        """Check if solution is generalizable"""
        return (solution.count('\n') <= 2 and 
                solution.count('(') < 3 and 
                'for ' in solution or 'with ' in solution)

    def _extract_rule(self, problem: str, solution: str):
        """Convert solutions into reusable rules"""
        # Basic pattern extraction
        vars = {
            'iterable': self._find_between(solution, 'for ', ' in'),
            'var': self._find_between(solution, 'for ', ' in').split()[0]
        } if 'for ' in solution else {
            'expr': self._find_between(solution, 'with ', ' as'),
            'var': self._find_between(solution, 'as ', ':').strip()
        }
        
        new_rule = {
            "template": solution,
            "vars": list(vars.keys()),
            "source_problem": problem,
            "last_used": None,
            "success_rate": 1.0
        }
        
        self._save_rule(new_rule)

    def _save_rule(self, rule: Dict[str, Any]):
        """Persist learned rules"""
        with open(self.learned_rules_path, 'r+') as f:
            data = json.load(f)
            data["rules"].append(rule)
            f.seek(0)
            json.dump(data, f, indent=2)


=== core\self_learning\rule_applier.py ===
import ast
from typing import Dict, Any

class RuleApplier:
    @staticmethod
    def apply_learned_rules(code: str, rules: list) -> Dict[str, Any]:
        """Apply learned rules to code context"""
        try:
            tree = ast.parse(code)
            for rule in sorted(rules, key=lambda x: x['success_rate'], reverse=True):
                if RuleApplier._matches_pattern(tree, rule['template']):
                    return {
                        "solution": rule['template'],
                        "confidence": rule['success_rate'],
                        "source": "learned_rule"
                    }
        except SyntaxError:
            pass
        return {}

    @staticmethod
    def _matches_pattern(tree: ast.AST, template: str) -> bool:
        """Check if code matches rule pattern"""
        try:
            template_tree = ast.parse(template)
            return ast.dump(tree) == ast.dump(template_tree)
        except:
            return False


=== core\service.py ===
from fastapi import FastAPI, APIRouter, HTTPException, WebSocket
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
import uvicorn
import asyncio
from typing import Dict, Any
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field
from shared.schemas import FeedbackRating, FeedbackCorrection  # Your new schemas

# Core imports
from core.integrations.manager import IntegrationManager
from core.reasoning.engine import HybridEngine
from core.orchestrator import Orchestrator
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.visualization import KnowledgeVisualizer
from core.versioning import KnowledgeVersioner

# Module system
from modules.registry import ModuleRegistry
from shared.schemas import Query

class AIService:
    def __init__(self, config: Dict[str, Any]):
        self.load_balancer = LoadBalancer(self.monitor)
        asyncio.create_task(self._update_weights_loop())
        self.config = config
        self.integration_manager = IntegrationManager(config.get("plugins", {}))
        self.reasoning = HybridEngine(self.context)
        
        self.app = FastAPI(
            title="AI Code Assistant",
            version="0.6.0",  # Bumped version
            docs_url="/api-docs"
        )
        
        # Core systems
        self.registry = ModuleRegistry()
        self.context = ContextManager()
        self.healing = SelfHealingController(self.registry)
        self.orchestrator = Orchestrator(
            registry=self.registry,
            healing=self.healing,
            context=self.context,
            reasoning=self.reasoning  # New dependency
        )
        self.visualizer = KnowledgeVisualizer(self.context.graph)
        self.versioner = KnowledgeVersioner(self.context.graph)
        
        self._setup()
        
        from core.feedback.processor import FeedbackProcessor  # Lazy import
        self.feedback_processor = FeedbackProcessor(self.context)
        
    async def _update_weights_loop(self):
        while True:
            await asyncio.sleep(self.config["load_balancing"]["update_interval"])
            self.load_balancer.update_weights()
    
    async def process_query(self, query: Dict) -> Dict:
        """Enhanced processing pipeline"""
        return await self.reasoning.process(query)

    def _setup(self):
        """Initialize all components"""
        # Setup filesystem
        Path("static").mkdir(exist_ok=True)
        Path("templates").mkdir(exist_ok=True)
        
        # Initialize modules
        self.registry.discover_modules()
        for module in self.registry._instances.values():
            module.context = self.context
            module.initialize()
            
        # Start background services
        asyncio.create_task(self.healing.start_monitoring())
        
        # Setup routes
        self._setup_routes()
        self._mount_static()

    def _setup_routes(self):
        @self.app.post("/process")
        async def process(query: Query):
            """Main processing endpoint with hybrid reasoning"""
            try:
                return await self.orchestrator.route_query(query)
            except Exception as e:
                raise HTTPException(status_code=503, detail=str(e))
                
        # Knowledge endpoints
        knowledge_router = APIRouter(prefix="/knowledge")
        
        @knowledge_router.get("")
        async def get_knowledge(concept: str = None):
            if concept:
                return self.context.graph.find_semantic_matches(concept)
            return {
                "stats": {
                    "nodes": len(self.context.graph.graph.nodes()),
                    "edges": len(self.context.graph.graph.edges()),
                    "interactions": len(self.context.interaction_log)
                }
            }
        
        # Specialized endpoints
        @self.app.post("/debug")
        async def debug_code(query: Query):
            try:
                return await self.registry.get_module("debug").process(query)
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
                
        @self.app.get("/health")
        async def health_check():
            """Simplified health check"""
            return {
                "status": "healthy",
                "services": {
                    name: plugin.is_ready()
                    for name, plugin in self.integration_manager.plugins.items()
                }
            }

        self.app.include_router(knowledge_router)
        
        @self.app.get("/cost-monitoring")
        async def get_cost_metrics():
            return {
                "current": service.cost_monitor.current_spend,
                "forecast": service.cost_monitor.get_spend_forecast(),
                "budget": service.cost_monitor.config["monthly_budget"]
            }
            
        # ===== ADD FEEDBACK ROUTES HERE =====
        @self.app.post("/feedback/rate", tags=["Feedback"])
        async def record_rating(feedback: FeedbackRating):
            """Record explicit user ratings (1-5 stars)"""
            self.feedback_processor.process_feedback({
                'type': 'positive' if feedback.rating >= 3 else 'negative',
                'rating': feedback.rating / 5,  # Normalize to 0-1
                'query_node': feedback.query_hash,
                'response_node': feedback.response_hash,
                'user_comment': feedback.comment,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "rating_recorded"}

        @self.app.post("/feedback/correct", tags=["Feedback"])
        async def record_correction(feedback: FeedbackCorrection):
            """Handle factual corrections from users"""
            self.feedback_processor.process_feedback({
                'type': 'correction',
                'target_node': feedback.node_id,
                'corrected_info': feedback.corrected_content,
                'severity': feedback.severity,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "correction_applied"}

    def _mount_static(self):
        self.app.mount("/static", StaticFiles(directory="static"), name="static")
        
        @self.app.get("/")
        async def serve_ui():
            return FileResponse("templates/index.html")

    async def start_service(self, host: str = "0.0.0.0", port: int = 8000):
        """Corrected instance method"""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info"
        )
        server = uvicorn.Server(config)
        await server.serve()


=== core\signature_help.py ===
import re
from typing import Dict, List, Optional
from shared.schemas import SignatureHelp

class SignatureProvider:
    def __init__(self):
        self.patterns = {
            "python": r"def\s+(\w+)\s*\((.*?)\)",
            "javascript": r"function\s+(\w+)\s*\((.*?)\)",
            "csharp": r"public\s+\w+\s+(\w+)\s*\((.*?)\)"
        }

    def get_signature_help(self, code: str, language: str, cursor_pos: int) -> Optional[SignatureHelp]:
        """Extract function signature at cursor position"""
        matches = self._find_function_defs(code, language)
        current_func = self._get_function_at_pos(matches, cursor_pos)
        
        if current_func:
            params = self._parse_parameters(current_func[1])
            return SignatureHelp(
                name=current_func[0],
                parameters=params,
                active_parameter=self._get_active_param(current_func[1], cursor_pos)
            )
        return None

    def _find_function_defs(self, code: str, language: str) -> List[tuple]:
        """Find all function definitions in code"""
        pattern = self.patterns.get(language, self.patterns["python"])
        return re.findall(pattern, code, re.DOTALL)

    def _get_function_at_pos(self, functions: List[tuple], cursor_pos: int) -> Optional[tuple]:
        """Find which function contains the cursor position"""
        # Simplified - in reality would need AST parsing
        for func in functions:
            # Check if cursor is within function bounds
            if func[2] <= cursor_pos <= func[3]:  # (start_pos, end_pos)
                return func
        return None

    def _parse_parameters(self, param_str: str) -> List[Dict[str, str]]:
        """Parse parameter string into structured format"""
        params = []
        for p in param_str.split(','):
            p = p.strip()
            if p:
                parts = p.split()
                params.append({
                    "name": parts[-1],
                    "type": parts[0] if len(parts) > 1 else "any"
                })
        return params

    def _get_active_param(self, param_str: str, cursor_pos: int) -> int:
        """Determine which parameter is active based on cursor position"""
        if not param_str:
            return 0
        commas = [m.start() for m in re.finditer(',', param_str)]
        for i, pos in enumerate(commas):
            if cursor_pos <= pos:
                return i
        return len(commas)


=== core\state_manager.py ===
from typing import Dict, Any
from shared.schemas import Query, Response

class SessionState:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.context = {}
        self.history = []
        
    def update(self, query: Query, response: Response):
        self.history.append((query, response))
        self._update_context(query, response)
        
    def _update_context(self, query: Query, response: Response):
        """Extract and store relevant context"""
        self.context.update({
            "last_module": response.metadata.get("module"),
            "last_type": query.content_type
        })

class StateManager:
    def __init__(self):
        self.sessions: Dict[str, SessionState] = {}
        
    def get_session(self, session_id: str) -> SessionState:
        if session_id not in self.sessions:
            self.sessions[session_id] = SessionState(session_id)
        return self.sessions[session_id]


=== core\validation\quality_gates.py ===
from typing import Dict, Any
import re
from shared.schemas import Response

class QualityValidator:
    def __init__(self, config: Dict[str, Any]):
        self.standards = config.get("quality_standards", {})
        self.compiled_rules = {
            "code_safety": re.compile(r"(eval\(|system\(|os\.popen)"),
            "min_complexity": float(self.standards.get("min_complexity", 0.3))
        }

    def validate(self, response: Response) -> Dict[str, Any]:
        """Run all quality checks"""
        checks = {
            "safety": self._check_code_safety(response.content),
            "complexity": self._check_complexity(response.content),
            "formatting": self._check_formatting(response.content)
        }
        
        return {
            "passed": all(checks.values()),
            "checks": checks,
            "original_response": response
        }

    def _check_code_safety(self, content: str) -> bool:
        """Block dangerous code patterns"""
        if "code" not in content:
            return True
        return not self.compiled_rules["code_safety"].search(content["code"])

    def _check_complexity(self, content: str) -> bool:
        """Ensure sufficient solution quality"""
        complexity = self._calculate_complexity(content)
        return complexity >= self.compiled_rules["min_complexity"]

    def _calculate_complexity(self, text: str) -> float:
        """Simple complexity heuristic (0-1 scale)"""
        lines = text.split('\n')
        return min(
            (len([l for l in lines if l.strip()]) * 0.1) +
            (len(re.findall(r"\b(for|while|def|class)\b", text)) * 0.3),
            1.0
        )

    def _check_formatting(self, content: str) -> bool:
        """Validate basic structure"""
        return bool(
            isinstance(content, (str, dict)) and 
            (not isinstance(content, dict) or "answer" in content)
        )


=== modules\base_module.py ===
from abc import ABC, abstractmethod
from typing import List, Optional
from enum import Enum
from shared.schemas import Query, Response
from core.orchestrator import Capability

class BaseModule(ABC):
    MODULE_ID: str
    VERSION: str
    CAPABILITIES: List[Capability]
    PRIORITY: int = 0
    
    def __init__(self):
        self.context = None  # Will be set by service
        self._usage_count = 0
        
    @classmethod
    def get_metadata(cls) -> dict:
        return {
            "id": cls.MODULE_ID,
            "version": cls.VERSION,
            "capabilities": [cap.value for cap in cls.CAPABILITIES],
            "priority": cls.PRIORITY
        }
    
    async def initialize(self):
        """Initialize with module-specific knowledge"""
        self._load_domain_knowledge()
        self._ready = True
        
    @abstractmethod
    def _load_domain_knowledge(self):
        """Preload module-specific knowledge"""
        pass
        
    @abstractmethod
    async def process(self, query: Query) -> Response:
        """Process query using contextual knowledge"""
        pass
        
    def health_check(self) -> dict:
        """Report health including knowledge metrics"""
        return {
            "status": "ready" if self._ready else "loading",
            "version": self.VERSION,
            "usage": self._usage_count,
            "knowledge": self._get_knowledge_stats()
        }
        
    def _get_knowledge_stats(self) -> dict:
        """Get module-specific knowledge metrics"""
        if not self.context:
            return {}
            
        return {
            "nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n].get("module") == self.MODULE_ID
            ]),
            "relationships": len([
                e for e in self.context.graph.graph.edges()
                if self.context.graph.graph.edges[e].get("module") == self.MODULE_ID
            ])
        }


=== modules\module_ai.py ===
from modules.base_module import BaseModule
from core.integrations.manager import IntegrationManager

class AIModule(BaseModule):
    MODULE_ID = "ai_integration"
    CAPABILITIES = ["text_generation"]
    
    def __init__(self):
        self.integrations = {}
    
    async def initialize(self):
        # Initialize configured integrations
        self.integrations["ollama"] = IntegrationManager.get_integration("ollama")
        # Add others from config
        
    async def process(self, query: Query) -> Response:
        integration = self.integrations.get(query.metadata.get("integration"))
        if not integration:
            return Response.error("Integration not configured")
        
        try:
            result = integration.generate(
                query.content,
                **query.metadata.get("params", {})
            )
            return Response(content=result)
        except Exception as e:
            return Response.error(f"Generation failed: {str(e)}")


=== modules\module_completion.py ===
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.completion import CodeCompleter

class CompletionModule(BaseModule):
    MODULE_ID = "completion"
    CAPABILITIES = ["code_completion"]

    async def initialize(self):
        self.completer = CodeCompleter()

    async def process(self, query: Query) -> Response:
        completions = self.completer.generate_completions({
            "context": query.context.get("code", ""),
            "cursor_context": query.content
        })
        return Response(
            content="\n---\n".join(completions["completions"]),
            metadata={
                "type": "completion",
                "language": query.context.get("language", "unknown")
            }
        )


=== modules\module_debug.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.debugger import CodeDebugger

class DebugModule(BaseModule):
    MODULE_ID = "debug"
    CAPABILITIES = ["error_diagnosis", "fix_suggestion"]
    
    async def initialize(self):
        self.debugger = CodeDebugger(self.context.graph)
        
    async def process(self, query: Query) -> Response:
        if not query.context.get("error"):
            return Response(content="No error provided", metadata={})
            
        frames = self.debugger.analyze_traceback(
            query.context["code"],
            query.context["error"]
        )
        suggestions = self.debugger.suggest_fixes(frames)
        
        return Response(
            content=self._format_report(frames, suggestions),
            metadata={
                "frames": [f.__dict__ for f in frames],
                "suggestions": suggestions
            }
        )
        
    def _format_report(self, frames, suggestions) -> str:
        report = []
        for frame in frames:
            report.append(f"File {frame.file}, line {frame.line}:")
            report.append(f"Context:\n{frame.context}")
            report.append(f"Error: {frame.error}")
            if frame.line in suggestions:
                report.append("Suggestions:")
                report.extend(f"- {sug}" for sug in suggestions[frame.line])
            report.append("")
        return '\n'.join(report)


=== modules\module_generic.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query

class GenericCodeModule(BaseModule):
    MODULE_ID = "code_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all code requests"""
        return Response(
            content=f"Generic code processing: {query.content[:200]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True,
                "warning": "Primary module unavailable"
            },
            metrics={"generic_processing": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_code_processing"]
        }

class GenericChatModule(BaseModule):
    MODULE_ID = "chat_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all requests"""
        return Response(
            content=f"Generic response: {query.content[:150]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True
            },
            metrics={"generic_response": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_text_processing"]
        }


=== modules\module_python.py ===
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.orchestrator import Capability

class PythonModule(BaseModule):
    MODULE_ID = "python"
    VERSION = "0.2.0"
    CAPABILITIES = [
        Capability.CODE_COMPLETION,
        Capability.DEBUGGING,
        Capability.DOCSTRING
    ]
    PRIORITY = 10
    
    async def initialize(self):
        self._ready = True
        # Initialize with Python-specific knowledge
        self._init_python_knowledge()
        
    def _init_python_knowledge(self):
        """Preload Python-specific concepts"""
        python_concepts = [
            ("list", "mutable sequence"),
            ("dict", "key-value mapping"),
            ("generator", "iterator creator"),
            ("decorator", "function wrapper")
        ]
        
        for concept, desc in python_concepts:
            self.context.graph.add_entity(
                content=concept,
                type="python_concept",
                metadata={
                    "description": desc,
                    "language": "python"
                }
            )
        
    async def process(self, query: Query) -> Response:
        """Process Python queries with knowledge context"""
        # Extract context from query metadata
        context = query.context.get("knowledge_graph", {})
        
        # Generate response using contextual knowledge
        response_content = self._generate_response(query.content, context)
        
        return Response(
            content=response_content,
            metadata={
                "module": self.MODULE_ID,
                "capabilities": [cap.value for cap in self.CAPABILITIES],
                "context_used": bool(context)
            },
            metrics={"python_processing": 0.42}
        )
        
    def _generate_response(self, content: str, context: dict) -> str:
        """Generate response using available knowledge"""
        # Simplified response generation
        if "def " in content:
            return f"Python function suggestion based on {len(context.get('nodes', []))} related concepts..."
        return f"Python code solution referencing {context.get('edges', [])[:2]}..."
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "knowledge_nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n]['type'] == "python_concept"
            ])
        }
    
    async def process(self, query: Query) -> Response:
        """Enhanced processing with visualization support"""
        # Generate standard response
        response = await super().process(query)
        
        # Add visualization if requested
        if "visualize" in query.tags:
            graph_data = self._extract_relevant_subgraph(query.content)
            response.metadata["visualization"] = {
                "type": "knowledge_subgraph",
                "data": graph_data
            }
            
        return response
        
    def _extract_relevant_subgraph(self, content: str) -> dict:
        """Create a subgraph relevant to the query"""
        matches = self.context.graph.find_semantic_matches(content)
        if not matches:
            return {}
            
        central_node = matches[0]["node_id"]
        subgraph = nx.ego_graph(self.context.graph.graph, central_node, radius=2)
        
        return {
            "central_concept": self.context.graph.graph.nodes[central_node],
            "related": [
                {
                    "id": n,
                    "content": self.context.graph.graph.nodes[n]["content"],
                    "type": self.context.graph.graph.nodes[n]["type"],
                    "relations": [
                        {
                            "target": e[1],
                            "type": e[2]["type"],
                            "weight": e[2].get("weight", 1.0)
                        }
                        for e in subgraph.edges(n, data=True)
                    ]
                }
                for n in subgraph.nodes() if n != central_node
            ]
        }


=== modules\module_signature.py ===
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.signature_help import SignatureProvider

class SignatureModule(BaseModule):
    MODULE_ID = "signature"
    CAPABILITIES = ["signature_help"]

    async def initialize(self):
        self.provider = SignatureProvider()

    async def process(self, query: Query) -> Response:
        help_data = self.provider.get_signature_help(
            code=query.context.get("code", ""),
            language=query.context.get("language", "python"),
            cursor_pos=query.context.get("cursor_pos", 0)
        )
        return Response(
            content=help_data if help_data else "No signature found",
            metadata={"type": "signature_help"}
        )


=== modules\registry.py ===
import importlib
import inspect
from pathlib import Path
from typing import Dict, Type
from .base_module import BaseModule
from core.orchestrator import CapabilityRouter

class ModuleRegistry:
    def __init__(self):
        self._modules: Dict[str, Type[BaseModule]] = {}
        self._instances: Dict[str, BaseModule] = {}
        self.router = CapabilityRouter()
        
    def discover_modules(self, package="modules"):
        """Automatically discover and register all modules"""
        modules_dir = Path(__file__).parent
        
        for module_file in modules_dir.glob("module_*.py"):
            module_name = module_file.stem
            module = importlib.import_module(f"{package}.{module_name}")
            
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) and 
                    issubclass(obj, BaseModule) and 
                    obj != BaseModule):
                    self.register_module(obj)
    
    def register_module(self, module_class: Type[BaseModule]):
        """Register a single module class"""
        instance = module_class()
        self._modules[module_class.MODULE_ID] = module_class
        self._instances[module_class.MODULE_ID] = instance
        self.router.register_module(
            instance,
            module_class.CAPABILITIES,
            module_class.PRIORITY
        )
        return instance
        
    def get_module(self, module_id: str) -> BaseModule:
        return self._instances.get(module_id)


=== monitoring\dashboard.json ===
{
  "metrics": [
    {
      "title": "Requests/Min",
      "query": "rate(llm_requests_total[1m])",
      "type": "line"
    },
    {
      "title": "Latency (99p)",
      "query": "histogram_quantile(0.99, sum by(le)(rate(llm_response_latency_seconds_bucket[1m])))",
      "unit": "s"
    }
  ]
}


=== package.json ===
{
  "name": "llm-code-assistant-ui",
  "version": "1.0.0",
  "scripts": {
    "build": "webpack --mode production",
    "dev": "webpack --watch --mode development",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "d3": "^7.8.5",
    "typescript": "^5.3.3"
  },
  "devDependencies": {
    "@types/d3": "^7.4.2",
    "css-loader": "^6.8.1",
    "mini-css-extract-plugin": "^2.7.6",
    "sass": "^1.69.5",
    "sass-loader": "^13.3.2",
    "ts-loader": "^9.5.1",
    "webpack": "^5.89.0",
    "webpack-cli": "^5.1.4"
  }
}


=== project_compact.txt ===
FILE: .env
---START---
GROQ_API_KEY="your_key"
HF_API_KEY="your_key" 
TEXTGEN_API_KEY="your_key"
---END---

FILE: .gitignore
---START---
---END---

FILE: configs\base.yaml
---START---
project:
  name: "AI-code-assistant"
  version: "0.0.1"
  
paths:
  data_raw: "./data/raw"
  data_processed: "./data/processed"
  model_checkpoints: "./models"
  
languages:
  priority: ["python", "csharp", "c", "javascript", "typescript", "html", "css"]
  
quality_standards:
  min_complexity: 0.4  # 0-1 scale
  required_keys: ["answer", "explanation"]  # For structured responses
  banned_patterns:
    - "eval("
    - "system("
    - "os.popen"
---END---

FILE: configs\integration.yaml
---START---
integrations:
  # Plugin configurations
  plugins:
    ollama:
      enabled: true
      config:
        base_url: "http://localhost:11434"
        default_model: "codellama"
        timeout: 30
        batch_size: 4  # Added batching support

    vllm:
      enabled: true
      config:
        model: "codellama/CodeLlama-7b-hf"
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.9
        max_batch_size: 2048  # Tokens

    textgen:
      enabled: true
      config:
        base_url: "http://localhost:5000"
        api_key: "${TEXTGEN_API_KEY}"  # From environment
        batch_size: 4
        timeout: 45

    huggingface:
      enabled: false  # Disabled by default
      config:
        api_key: "${HF_API_KEY}"
        model_name: "codellama/CodeLlama-7b-hf"
        device: "auto"
        quantize: false
        batch_size: 2

    grok:
      enabled: true
      config:
        api_key: "${GROQ_API_KEY}"
        rate_limit: 5  # Requests per minute
        timeout: 15

    lmstudio:
      enabled: false  # Disabled by default
      config:
        base_url: "http://localhost:1234"
        timeout: 60
        batch_support: false

  # Global integration settings
  settings:
    default_timeout: 30  # Fallback timeout
    priority_order:  # Execution priority
      - "vllm"
      - "ollama" 
      - "grok"
      - "huggingface"
      - "textgen"
      - "lmstudio"

    # Batch processing defaults
    batching:
      enabled: true
      max_batch_size: 8
      max_wait_ms: 50

    # Monitoring
    health_check_interval: 60  # Seconds
  
  load_balancing:
    update_interval: 10  # Seconds
    min_requests: 20     # Minimum data points before activating
    priority_bump: 2.0   # Weight multiplier for high-priority queries
---END---

FILE: configs\model.yaml
---START---
---END---

FILE: configs\predictions.yaml
---START---
# configs/prediction.yaml
cache:
  warmers: 2
  max_predictions: 3
  min_confidence: 0.7
---END---

FILE: configs\sla_tiers.yaml
---START---
tiers:
  critical:
    min_accuracy: 0.96
    max_latency: 1.2
    allowed_providers: ["gpt-4", "claude-2"]
    cost_multiplier: 2.5
    
  standard:
    min_accuracy: 0.88  
    max_latency: 2.5
    allowed_providers: ["gpt-3.5", "claude-instant"]
    
  economy:
    min_accuracy: 0.75
    max_latency: 7.0
    allowed_providers: ["llama2", "local"]
---END---

FILE: core\analysis.py
---START---
# core/analysis.py
import re
from enum import Enum

class ContentType(Enum):
    CODE_PYTHON = "code_python"
    CODE_CSHARP = "code_csharp"
    MATH_SYMBOLIC = "math_symbolic"
    TEXT_QUERY = "text_query"

class ContentAnalyzer:
    CODE_PATTERNS = {
        ContentType.CODE_PYTHON: [
            r'def\s+\w+\(.*\):',
            r'import\s+\w+'
        ],
        ContentType.CODE_CSHARP: [
            r'public\s+(class|interface)\s+\w+',
            r'using\s+\w+;'
        ]
    }
    
    def analyze(self, text: str) -> ContentType:
        for content_type, patterns in self.CODE_PATTERNS.items():
            if any(re.search(p, text) for p in patterns):
                return content_type
        return ContentType.TEXT_QUERY
---END---

FILE: core\completion.py
---START---
from transformers import pipeline
from typing import List, Dict
from shared.schemas import CompletionRequest

class CodeCompleter:
    def __init__(self, model_name="deepseek-coder-6.7b"):
        self.completion_pipeline = pipeline(
            "text-generation",
            model=model_name,
            device="cuda"  # Use "cpu" if no GPU
        )

    def generate_completions(self, request: CompletionRequest) -> Dict[str, List[str]]:
        """Generate code suggestions with context awareness"""
        prompt = self._build_prompt(request.context, request.cursor_context)
        outputs = self.completion_pipeline(
            prompt,
            num_return_sequences=3,
            max_new_tokens=50,
            temperature=0.7,
            stop_sequences=["\n\n"]
        )
        return {"completions": [o["generated_text"] for o in outputs]}

    def _build_prompt(self, context: str, cursor_context: str) -> str:
        """Structured prompt for code completion"""
        return f"""# Code Context:
{context}

# Cursor Position:
{cursor_context}

# Suggested Completion:"""
---END---

FILE: core\context.py
---START---
from shared.knowledge.graph import KnowledgeGraph
from shared.schemas import Query, Response
from typing import Dict, Any, List, Optional
import numpy as np
import hashlib
from datetime import datetime

class ContextManager:
    def __init__(self):
        self.graph = KnowledgeGraph()
        self._setup_foundational_knowledge()
        self.interaction_log = []
        self.routing_history = []  # NEW: Track routing decisions
        
    def _setup_foundational_knowledge(self):
        """Initialize with programming fundamentals"""
        foundations = [
            ("variable", "named storage location", ["storage", "memory"]),
            ("function", "reusable code block", ["abstraction", "parameters"]),
            ("loop", "iteration construct", ["repetition", "termination"]),
            ("class", "object blueprint", ["inheritance", "encapsulation"])
        ]
        
        for concept, desc, tags in foundations:
            node_id = self.graph.add_entity(
                content=concept,
                type="concept",
                metadata={
                    "description": desc,
                    "tags": tags,
                    "source": "system"
                }
            )
    
    # MODIFIED: Enhanced with routing metadata        
    def process_interaction(
        self, 
        query: Query, 
        response: Response,
        metadata: Optional[Dict] = None
    ):
        """
        Learn from user interactions with routing context
        Args:
            metadata: {
                "sla_tier": str,       # critical/standard/economy
                "reasoning_source": str, # graph/rule/llm
                "provider": str         # gpt-4/llama2/etc
            }
        """
        # Generate unique interaction ID
        interaction_id = hashlib.sha256(
            f"{datetime.now().isoformat()}:{query.content}".encode()
        ).hexdigest()
        
        # Enhanced logging (NEW)
        log_entry = {
            "id": interaction_id,
            "query": query.content,
            "response": response.content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        self.interaction_log.append(log_entry)
        
        # Track routing decisions separately (NEW)
        if metadata:
            self.routing_history.append({
                "timestamp": datetime.now().isoformat(),
                "query_hash": hashlib.sha256(query.content.encode()).hexdigest()[:8],
                **metadata
            })
        
        # Extract knowledge from both query and response
        self.graph.expand_from_text(
            query.content, 
            source="query",
            metadata={"sla_tier": metadata.get("sla_tier")} if metadata else None  # NEW
        )
        
        self.graph.expand_from_text(
            response.content,
            source="response",
            metadata={"provider": metadata.get("provider")} if metadata else None  # NEW
        )
        
        # Create relationship between query and response concepts
        query_nodes = self._extract_key_nodes(query.content)
        response_nodes = self._extract_key_nodes(response.content)
        
        for q_node in query_nodes:
            for r_node in response_nodes:
                self.graph.add_relation(
                    q_node, 
                    r_node, 
                    "elicits",
                    metadata=metadata  # NEW: Attach routing info to edges
                )
    
    # NEW METHOD
    def get_routing_context(self, query_content: str) -> Dict[str, Any]:
        """
        Get context specifically for routing decisions
        Returns:
            {
                "is_production": bool,
                "similar_past_queries": List[Dict],
                "preferred_llm": Optional[str],
                "complexity_score": float
            }
        """
        # Existing semantic matching
        matches = self.graph.find_semantic_matches(query_content)
        
        # NEW: Calculate complexity
        complexity = min(len(query_content.split()) / 10, 1.0)  # 0-1 scale
        
        return {
            "is_production": any(
                "production" in node["content"].lower() 
                for node in matches[:3]
            ),
            "similar_past_queries": [
                {
                    "query": self.graph.graph.nodes[m["node_id"]]["content"],
                    "source": self.graph.graph.nodes[m["node_id"]].get("source"),
                    "success": self._get_interaction_success(m["node_id"])
                }
                for m in matches[:3]
            ],
            "complexity_score": complexity,
            "preferred_llm": self._detect_preferred_provider(query_content)
        }
    
    # NEW HELPER METHODS
    def _get_interaction_success(self, node_id: str) -> bool:
        """Check if previous interactions with this node were successful"""
        edges = list(self.graph.graph.edges(node_id, data=True))
        return any(
            e[2].get("metadata", {}).get("success", True)
            for e in edges
        )
    
    def _detect_preferred_provider(self, query: str) -> Optional[str]:
        """Detect if query suggests a preferred provider"""
        query_lower = query.lower()
        if "openai" in query_lower:
            return "gpt-4"
        elif "local" in query_lower:
            return "llama2"
        return None
    
    # EXISTING METHODS (unchanged)
    def _extract_key_nodes(self, text: str) -> List[str]:
        """Identify most important nodes in text"""
        matches = self.graph.find_semantic_matches(text)
        return [m["node_id"] for m in matches[:3]]  # Top 3 matches
        
    def get_context(self, text: str) -> Dict[str, Any]:
        """Get relevant context for given text"""
        matches = self.graph.find_semantic_matches(text)
        context_nodes = set()
        
        # Get related nodes for each match
        for match in matches[:5]:  # Top 5 matches
            neighbors = list(self.graph.graph.neighbors(match["node_id"]))
            context_nodes.update(neighbors)
            
        return {
            "matches": matches,
            "related": [
                {"id": n, **self.graph.graph.nodes[n]}
                for n in context_nodes
            ]
        }
---END---

FILE: core\debugger.py
---START---
from typing import Dict, List
from dataclasses import dataclass
import ast
import traceback

@dataclass
class DebugFrame:
    file: str
    line: int
    context: str
    variables: Dict[str, str]
    error: str

class CodeDebugger:
    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph
        self.error_patterns = self._load_error_patterns()

    def analyze_traceback(self, code: str, error: str) -> List[DebugFrame]:
        """Convert traceback into structured debug frames"""
        frames = []
        tb = traceback.extract_tb(error.__traceback__)
        
        for frame in tb:
            context = self._get_code_context(code, frame.lineno)
            frames.append(DebugFrame(
                file=frame.filename,
                line=frame.lineno,
                context=context,
                variables=self._extract_variables(frame.locals),
                error=str(error)
            ))
        
        return frames

    def suggest_fixes(self, frames: List[DebugFrame]) -> Dict[str, List[str]]:
        """Generate fix suggestions using knowledge graph"""
        suggestions = {}
        for frame in frames:
            error_key = self._match_error_pattern(frame.error)
            if error_key in self.error_patterns:
                suggestions[frame.line] = self._enhance_suggestions(
                    self.error_patterns[error_key],
                    frame.context
                )
        return suggestions

    def _get_code_context(self, code: str, line: int, window=3) -> str:
        lines = code.split('\n')
        start = max(0, line - window - 1)
        end = min(len(lines), line + window)
        return '\n'.join(lines[start:end])

    def _extract_variables(self, locals_dict: Dict) -> Dict[str, str]:
        return {k: repr(v)[:100] for k, v in locals_dict.items() if not k.startswith('_')}

    def _match_error_pattern(self, error: str) -> str:
        for pattern in self.error_patterns:
            if pattern in error:
                return pattern
        return "unknown"

    def _enhance_suggestions(self, base_suggestions: List[str], context: str) -> List[str]:
        enhanced = []
        for suggestion in base_suggestions:
            # Augment with knowledge graph matches
            related = self.kg.find_semantic_matches(context + " " + suggestion)[:2]
            if related:
                enhanced.append(f"{suggestion} (Related: {', '.join(r['content'] for r in related)})")
            else:
                enhanced.append(suggestion)
        return enhanced

    def _load_error_patterns(self) -> Dict[str, List[str]]:
        return {
            "IndexError": [
                "Check list length before accessing",
                "Verify array bounds"
            ],
            "KeyError": [
                "Check if key exists in dictionary",
                "Use dict.get() for safe access"
            ],
            # ... other patterns
        }
---END---

FILE: core\feedback\processor.py
---START---
from datetime import datetime
from typing import Dict, Any
import numpy as np
from shared.knowledge.graph import KnowledgeGraph

class FeedbackProcessor:
    def __init__(self, context_manager):
        self.context = context_manager
        self.feedback_weights = {
            'explicit_rating': 0.7,
            'implicit_engagement': 0.3,
            'correction': 1.0
        }

    def process_feedback(self, feedback: Dict[str, Any]):
        """Handle both explicit and implicit feedback"""
        # Store raw feedback
        self._log_feedback(feedback)

        # Update knowledge graph
        if feedback['type'] == 'correction':
            self._apply_correction(feedback)
        else:
            self._update_edge_weights(feedback)

    def _apply_correction(self, feedback):
        """Direct knowledge corrections"""
        self.context.graph.update_node(
            node_id=feedback['target_node'],
            new_content=feedback['corrected_info'],
            metadata={'last_corrected': datetime.now()}
        )

    def _update_edge_weights(self, feedback):
        """Adjust relationship strengths"""
        current_weight = self.context.graph.get_edge_weight(
            feedback['query_node'],
            feedback['response_node']
        )
        
        new_weight = current_weight * (1 + self._calculate_feedback_impact(feedback))
        self.context.graph.update_edge(
            source=feedback['query_node'],
            target=feedback['response_node'],
            weight=min(new_weight, 1.0)  # Cap at 1.0
        )

    def _calculate_feedback_impact(self, feedback) -> float:
        """Calculate weighted feedback impact"""
        base_score = (
            feedback.get('rating', 0.5) * self.feedback_weights['explicit_rating'] +
            feedback.get('engagement', 0.2) * self.feedback_weights['implicit_engagement']
        )
        return base_score * (2 if feedback['type'] == 'positive' else -1)
---END---

FILE: core\health.py
---START---
from typing import Dict
import requests

class HealthChecker:
    @staticmethod
    def check_endpoint(url: str) -> Dict[str, bool]:
        try:
            resp = requests.get(f"{url}/health", timeout=3)
            return {
                "online": resp.status_code == 200,
                "models_loaded": resp.json().get("models_loaded", 0)
            }
        except:
            return {"online": False}
            
    def check_ollama(base_url="http://localhost:11434"):
        try:
            resp = requests.get(f"{base_url}/api/tags", timeout=3)
            return {
                "status": "online",
                "models": [m['name'] for m in resp.json().get('models', [])]
            }
        except Exception as e:
            return {"status": "error", "details": str(e)}

---END---

FILE: core\integrations\__init__.py
---START---
from importlib import import_module
from pathlib import Path
from typing import Dict, Type
from ..plugin import PluginBase

_PLUGINS: Dict[str, Type[PluginBase]] = {}

def _discover_plugins():
    package_dir = Path(__file__).parent
    for _, module_name, _ in pkgutil.iter_modules([str(package_dir)]):
        if module_name in ("__init__", "manager"):
            continue
        
        try:
            module = import_module(f".{module_name}", package=__package__)
            if (plugin_class := getattr(module, "Plugin", None)) and \
               issubclass(plugin_class, PluginBase):
                _PLUGINS[module_name] = plugin_class
        except (ImportError, TypeError) as e:
            import warnings
            warnings.warn(f"Failed to load {module_name}: {str(e)}")

def get_plugin(name: str) -> Type[PluginBase]:
    """Get plugin class by name (e.g., 'ollama')"""
    return _PLUGINS[name]  # Will raise KeyError if not found

def available_plugins() -> Dict[str, Type[PluginBase]]:
    """Return copy of registered plugins"""
    return _PLUGINS.copy()

# Initialize on import
import pkgutil
_discover_plugins()

__all__ = ['get_plugin', 'available_plugins']
---END---

FILE: core\integrations\grok.py
---START---
import requests
import time
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
from datetime import datetime

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="grok",
            version="0.3.0",
            required_config={
                "api_key": str,
                "rate_limit": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Grok AI API integration with batching"
        )

    def initialize(self):
        self.api_key = self.config["api_key"]
        self.rate_limit = self.config.get("rate_limit", 5)
        self.timeout = self.config.get("timeout", 10)
        self.last_calls = []
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        if not self._check_rate_limit():
            return {"error": "Rate limit exceeded"}

        try:
            self.last_calls.append(time.time())
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            "https://api.grok.ai/v1/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 150)
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _check_rate_limit(self):
        now = time.time()
        self.last_calls = [t for t in self.last_calls if t > now - 60]
        return len(self.last_calls) < self.rate_limit

    def health_check(self):
        return {
            "ready": self._initialized,
            "rate_limit": f"{len(self.last_calls)}/{self.rate_limit}",
            "last_call": self.last_calls[-1] if self.last_calls else None
        }
---END---

FILE: core\integrations\huggingface.py
---START---
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import torch
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="huggingface",
            version="0.5.0",
            required_config={
                "model_name": str,
                "device": str,
                "quantize": bool,
                "batch_size": int
            },
            dependencies=["transformers>=4.30.0", "torch"],
            description="HuggingFace Transformers with batching"
        )

    def initialize(self):
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config["model_name"],
                device_map="auto" if self.config["device"] == "auto" else None
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model_name"]
            )
            if self.config.get("quantize", False):
                self.model = torch.quantization.quantize_dynamic(
                    self.model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
            self.batch_size = self.config.get("batch_size", 4)
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"Initialization failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            if isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(input_data["prompt"], return_tensors="pt").to(self.config["device"])
        outputs = self.model.generate(**inputs)
        return {"response": self.tokenizer.decode(outputs[0])}

    def _batch_execute(self, input_data: Dict) -> Dict:
        inputs = self.tokenizer(
            input_data["prompt"],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.config["device"])
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=input_data.get("max_tokens", 50),
            num_return_sequences=1,
            batch_size=self.batch_size
        )
        
        return {
            "responses": [
                self.tokenizer.decode(output, skip_special_tokens=True)
                for output in outputs
            ]
        }

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("huggingface").observe(time.time() - start_time)
---END---

FILE: core\integrations\lmstudio.py
---START---
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="lmstudio",
            version="0.4.0",
            required_config={
                "base_url": str,
                "timeout": int,
                "batch_support": bool
            },
            dependencies=["requests"],
            description="LM Studio local server with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.timeout = self.config.get("timeout", 60)
        self._batch_support = self.config.get("batch_support", False)
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self._batch_support

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/v1/completions",
            json={
                "prompt": input_data["prompt"],
                "max_tokens": input_data.get("max_tokens", 200),
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        responses = []
        for prompt in input_data["prompt"]:
            responses.append(self._single_execute({"prompt": prompt}))
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("lmstudio").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/v1/models", timeout=5)
            base_status["status"] = "online" if resp.ok else "offline"
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status
---END---

FILE: core\integrations\manager.py
---START---
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any
from core.monitoring.service import Monitoring

class Plugin(PluginBase):
    def __init__(self, config: Dict[str, Any], monitor: Monitoring):
        super().__init__(config)
        self.monitor = monitor

    def get_metadata(self):
        return PluginMetadata(
            name="manager",
            version="0.2.0",
            description="Enhanced plugin management with monitoring",
            required_config={}
        )

    @self.monitor.track_request('plugin_manager')
    def execute(self, command: Dict[str, Any]) -> Dict[str, Any]:
        """Monitored plugin management"""
        try:
            if command["action"] == "list_plugins":
                return self._list_plugins()
            elif command["action"] == "plugin_status":
                return self._plugin_status(command["plugin"])
            else:
                return {"error": "Unknown action"}
        except Exception as e:
            self.monitor.REQUEST_COUNT.labels('manager', 'failed').inc()
            raise

    def _list_plugins(self) -> Dict[str, Any]:
        return {
            "plugins": list(self.context.plugin_manager.plugins.keys()),
            "stats": {
                "ready": sum(1 for p in self.context.plugin_manager.plugins.values() if p.is_ready()),
                "total": len(self.context.plugin_manager.plugins)
            }
        }

    def _plugin_status(self, plugin_name: str) -> Dict[str, Any]:
        plugin = self.context.plugin_manager.get_plugin(plugin_name)
        return {
            "exists": plugin is not None,
            "ready": plugin.is_ready() if plugin else False,
            "metadata": plugin.metadata if plugin else None
        }
---END---

FILE: core\integrations\ollama.py
---START---
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="ollama",
            version="0.5.0",
            required_config={
                "base_url": str,
                "default_model": str,
                "batch_size": int
            },
            dependencies=["requests"],
            description="Ollama with experimental batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.default_model = self.config.get("default_model", "llama2")
        self.batch_size = self.config.get("batch_size", 1)  # Default to no batching
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": input_data.get("model", self.default_model),
                "prompt": input_data["prompt"],
                "stream": False,
                **input_data.get("options", {})
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        # Note: Ollama doesn't natively support batching, so we parallelize
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.batch_size) as executor:
            results = list(executor.map(
                lambda p: self._single_execute({"prompt": p}),
                input_data["prompt"]
            ))
        return {"responses": results}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("ollama").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/tags", timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "models": [m["name"] for m in resp.json().get("models", [])]
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status
---END---

FILE: core\integrations\textgen.py
---START---
import requests
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="textgen",
            version="0.4.0",
            required_config={
                "base_url": str,
                "api_key": str,
                "batch_size": int,
                "timeout": int
            },
            dependencies=["requests"],
            description="Text Generation WebUI API with batching"
        )

    def initialize(self):
        self.base_url = self.config["base_url"].rstrip("/")
        self.api_key = self.config["api_key"]
        self.batch_size = self.config.get("batch_size", 1)
        self.timeout = self.config.get("timeout", 30)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self._initialized = True
        return True

    @property
    def supports_batching(self) -> bool:
        return self.batch_size > 1

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        start_time = time.time()
        try:
            if self.supports_batching and isinstance(input_data["prompt"], list):
                return self._batch_execute(input_data)
            return self._single_execute(input_data)
        finally:
            self._log_latency(start_time)

    def _single_execute(self, input_data: Dict) -> Dict:
        response = requests.post(
            f"{self.base_url}/api/v1/generate",
            headers=self.headers,
            json={
                "prompt": input_data["prompt"],
                **input_data.get("parameters", {})
            },
            timeout=self.timeout
        )
        response.raise_for_status()
        return response.json()

    def _batch_execute(self, input_data: Dict) -> Dict:
        """Execute multiple prompts as a batch"""
        responses = []
        prompts = input_data["prompt"]
        
        # Process in chunks of batch_size
        for i in range(0, len(prompts), self.batch_size):
            chunk = prompts[i:i + self.batch_size]
            response = requests.post(
                f"{self.base_url}/api/v1/generate_batch",  # Note: Your API must support this endpoint
                headers=self.headers,
                json={
                    "prompts": chunk,
                    **input_data.get("parameters", {})
                },
                timeout=self.timeout
            )
            response.raise_for_status()
            responses.extend(response.json()["results"])
            
        return {"responses": responses}

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("textgen").observe(time.time() - start_time)

    def health_check(self):
        base_status = super().health_check()
        try:
            resp = requests.get(f"{self.base_url}/api/v1/model", 
                             headers=self.headers,
                             timeout=5)
            base_status.update({
                "status": "online" if resp.ok else "offline",
                "model": resp.json().get("model_name", "unknown")
            })
        except requests.exceptions.RequestException:
            base_status["status"] = "offline"
        return base_status
---END---

FILE: core\integrations\vllm.py
---START---
from vllm import LLM, SamplingParams
from core.plugin import PluginBase, PluginMetadata
from typing import Dict, Any, List
import time

class Plugin(PluginBase):
    def get_metadata(self):
        return PluginMetadata(
            name="vllm",
            version="0.4.0",
            required_config={
                "model": str,
                "tensor_parallel_size": int,
                "gpu_memory_utilization": float,
                "max_batch_size": int
            },
            dependencies=["vllm>=0.2.0"],
            description="High-performance batched inference"
        )

    def initialize(self):
        try:
            self.llm = LLM(
                model=self.config["model"],
                tensor_parallel_size=self.config.get("tensor_parallel_size", 1),
                gpu_memory_utilization=self.config.get("gpu_memory_utilization", 0.9),
                max_num_batched_tokens=self.config.get("max_batch_size", 2560)
            )
            self.default_params = SamplingParams(
                temperature=0.8,
                top_p=0.95
            )
            self._initialized = True
            return True
        except Exception as e:
            self.logger.error(f"vLLM init failed: {str(e)}")
            return False

    @property
    def supports_batching(self) -> bool:
        return True

    def execute(self, input_data: Dict) -> Dict:
        start = time.time()
        try:
            params = self.default_params.copy()
            if "parameters" in input_data:
                params = SamplingParams(**input_data["parameters"])
            
            if isinstance(input_data["prompt"], list):
                outputs = self.llm.generate(input_data["prompt"], params)
                return {
                    "responses": [o.outputs[0].text for o in outputs]
                }
            else:
                output = self.llm.generate([input_data["prompt"]], params)
                return {"response": output[0].outputs[0].text}
        finally:
            self._log_latency(start)

    def _log_latency(self, start_time: float):
        self.context.monitor.LATENCY.labels("vllm").observe(time.time() - start_time)

    def health_check(self):
        status = super().health_check()
        status["gpu_utilization"] = self._get_gpu_stats()
        status["batch_capacity"] = self.llm.llm_engine.scheduler_config.max_num_batched_tokens
        return status
---END---

FILE: core\interface.py
---START---
---END---

FILE: core\monitoring\service.py
---START---
import time
from prometheus_client import start_http_server, Counter, Gauge, Histogram

class Monitoring:
    def __init__(self, port=9090):
        # Metrics Definitions
        self.REQUEST_COUNT = Counter(
            'llm_requests_total',
            'Total API requests',
            ['module', 'status']
        )
        
        self.LATENCY = Histogram(
            'llm_response_latency_seconds',
            'Response latency distribution',
            ['provider']
        )
        
        self.CACHE_HITS = Gauge(
            'cache_hit_ratio',
            'Current cache hit percentage'
        )
        
        start_http_server(port)

    def track_request(self, module: str):
        """Decorator to monitor request metrics"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start = time.time()
                try:
                    result = func(*args, **kwargs)
                    self.REQUEST_COUNT.labels(module, 'success').inc()
                    return result
                except Exception:
                    self.REQUEST_COUNT.labels(module, 'failed').inc()
                    raise
                finally:
                    self.LATENCY.labels(module).observe(time.time() - start)
            return wrapper
        return decorator

    def update_cache_metrics(self, hits: int, misses: int):
        """Update cache performance metrics"""
        self.CACHE_HITS.set(hits / max(hits + misses, 1))
---END---

FILE: core\orchestration\budget_router.py
---START---
from typing import Dict, Literal
from ..performance.cost import CostMonitor

class BudgetRouter:
    def __init__(self, cost_monitor: CostMonitor):
        self.cost = cost_monitor

    def select_llm(self, query: Dict) -> Literal['premium', 'standard', 'local']:
        """Choose LLM tier based on budget and query criticality"""
        forecast = self.cost.get_spend_forecast()
        criticality = query.get("criticality", 0.5)
        
        if forecast["burn_rate"] > forecast["budget_remaining"] / 7:  # Weekly burn
            return 'local'
        elif criticality > 0.8 and forecast["budget_remaining"] > 50:
            return 'premium'
        else:
            return 'standard'
---END---

FILE: core\orchestration\load_balancer.py
---START---
from typing import Dict, List
import numpy as np
from collections import deque
from ..performance.tracker import PerformanceTracker

class LoadBalancer:
    def __init__(self, tracker: PerformanceTracker):
        self.tracker = tracker
        self.weights = {}  # Provider -> weight
        self.history = deque(maxlen=100)  # Tracks last 100 routing decisions

    def update_weights(self):
        """Calculate new weights based on performance"""
        metrics = self.tracker.get_provider_metrics()
        total = sum(m['requests_per_second'] / (m['avg_latency'] + 1e-6) for m in metrics.values())
        
        self.weights = {
            provider: (m['requests_per_second'] / (m['avg_latency'] + 1e-6)) / total
            for provider, m in metrics.items()
        }

    def select_provider(self, query: Dict) -> str:
        """Select provider using weighted random choice"""
        providers = list(self.weights.keys())
        weights = list(self.weights.values())
        choice = np.random.choice(providers, p=weights)
        self.history.append((query['content'][:50], choice))
        return choice
---END---

FILE: core\orchestration\sla_router.py
---START---
from typing import Dict, Literal
from dataclasses import dataclass
from ..performance.cost import CostMonitor
from ..performance.tracker import PerformanceTracker
import numpy as np

@dataclass
class SLATier:
    name: str
    min_accuracy: float
    max_latency: float  # seconds
    allowed_providers: list
    cost_multiplier: float = 1.0

class SLARouter:
    def __init__(self, cost_monitor: CostMonitor, perf_tracker: PerformanceTracker):
        self.cost = cost_monitor
        self.performance = perf_tracker
        
        # Define service tiers
        self.tiers = {
            "critical": SLATier(
                name="critical",
                min_accuracy=0.95,
                max_latency=1.5,
                allowed_providers=["gpt-4", "claude-2", "vllm"],
                cost_multiplier=2.0
            ),
            "standard": SLATier(
                name="standard",
                min_accuracy=0.85,
                max_latency=3.0,
                allowed_providers=["gpt-3.5", "claude-instant", "llama2"]
            ),
            "economy": SLATier(
                name="economy",
                min_accuracy=0.70,
                max_latency=5.0,
                allowed_providers=["llama2", "local"]
            )
        }

    def select_provider(self, query: Dict) -> Dict[str, str]:
        """Select optimal provider based on SLA and budget"""
        # Determine SLA tier
        tier = self._determine_tier(query)
        
        # Get eligible providers
        candidates = [
            p for p in self.performance.get_available_providers()
            if p in tier.allowed_providers
        ]
        
        # Rank by performance/cost tradeoff
        ranked = sorted(
            candidates,
            key=lambda p: self._score_provider(p, tier)
        )
        
        return {
            "provider": ranked[0],
            "tier": tier.name,
            "reason": f"Best match for {tier.name} SLA"
        }

    def _determine_tier(self, query: Dict) -> SLATier:
        """Auto-select SLA tier based on query properties"""
        if query.get("user_priority") == "high":
            return self.tiers["critical"]
        
        # Auto-detect critical queries
        if ("error" in query.get("intent", "") or 
            "production" in query.get("context", "")):
            return self.tiers["critical"]
            
        # Budget-aware fallback
        budget_status = self.cost.get_spend_forecast()
        if budget_status["burn_rate"] > budget_status["budget_remaining"] / 10:
            return self.tiers["economy"]
            
        return self.tiers["standard"]

    def _score_provider(self, provider: str, tier: SLATier) -> float:
        """Score providers (0-1) based on SLA fit"""
        metrics = self.performance.get_provider_metrics(provider)
        
        # Normalized performance score (higher better)
        accuracy_score = metrics["accuracy"] / tier.min_accuracy
        latency_score = tier.max_latency / max(metrics["latency"], 0.1)
        
        # Cost penalty (lower better)
        cost_rate = self.cost._get_rate(provider.split('-')[0], provider)
        cost_penalty = cost_rate["input"] * tier.cost_multiplier
        
        return np.mean([accuracy_score, latency_score]) / cost_penalty
---END---

FILE: core\orchestrator.py
---START---
from typing import Dict, List
from shared.schemas import Query, Response
from modules.base_module import BaseModule
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.validation.quality_gates import QualityValidator
from core.orchestration.sla_router import SLARouter
from core.orchestration.load_balancer import LoadBalancer
from core.reasoning.engine import HybridEngine
from core.prediction.warmer import CacheWarmer
from core.monitoring.service import Monitoring
from core.processing.batcher import AdaptiveBatcher
import logging
import asyncio
import numpy as np

class Orchestrator:
    def __init__(
        self,
        validator: QualityValidator,
        sla_router: SLARouter,
        load_balancer: LoadBalancer,
        registry,
        healing_controller: SelfHealingController,
        context_manager: ContextManager,
        reasoning_engine: HybridEngine,
        monitoring: Monitoring
    ):
        self.validator = validator
        self.sla_router = sla_router
        self.load_balancer = load_balancer
        self.registry = registry
        self.healing = healing_controller
        self.context = context_manager
        self.reasoning = reasoning_engine
        self.monitor = monitoring
        self.logger = logging.getLogger("orchestrator")
        self.cache_warmer = CacheWarmer(self, self.context.cache_predictor)
        self.batcher = AdaptiveBatcher(
            max_batch_size=self.context.config.get("batching.max_size", 8),
            max_wait_ms=self.context.config.get("batching.max_wait_ms", 50)
        )
        self._setup_fallback_strategies()
        asyncio.create_task(self.batcher.background_flush())
        asyncio.create_task(self._update_balancer_weights())

    def _setup_fallback_strategies(self):
        self.fallback_map = {
            "python": "code_generic",
            "csharp": "code_generic",
            "math": "math_basic",
            "chat": "generic"
        }

    async def _update_balancer_weights(self):
        """Periodically update load balancer weights"""
        while True:
            await asyncio.sleep(
                self.context.config.get("load_balancing.update_interval", 10)
            )
            if len(self.load_balancer.history) >= self.context.config.get("load_balancing.min_requests", 20):
                self.load_balancer.update_weights()

    @self.monitor.track_request('orchestrator')
    async def route_query(self, query: Query) -> Response:
        """Enhanced query processing pipeline"""
        try:
            # 1. Get context and routing info
            pre_context = self.context.get_context(query.content)
            
            # Dynamic provider selection (NEW)
            if query.metadata.get("priority", 0) > 0:
                # High-priority uses SLA routing
                routing_decision = self.sla_router.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "user_priority": query.metadata.get("priority", "normal")
                })
                provider = routing_decision["provider"]
            else:
                # Normal traffic uses load balancing
                provider = self.load_balancer.select_provider({
                    "content": query.content,
                    "context": pre_context,
                    "priority": query.metadata.get("priority", 0)
                })
                routing_decision = {"provider": provider, "tier": "balanced"}
            
            query.provider = provider

            # 2. Hybrid reasoning
            reasoning_result = await self.reasoning.process({
                "query": query.content,
                "context": pre_context,
                "llm_preference": provider
            })

            # 3. Module processing with quality gates
            module = self._select_module(
                query,
                pre_context,
                reasoning_source=reasoning_result.get("source")
            )
            enriched_query = query.with_additional_context(reasoning_result)
            
            # Process with batching if enabled
            if query.metadata.get("allow_batching", True):
                batch = await self.batcher.add_query(
                    enriched_query.model_dump(),
                    priority=query.metadata.get("priority", 0)
                )
                if len(batch) > 1:
                    return await self._batch_process(batch)

            raw_response = await module.process(enriched_query)

            # 4. Validate and enhance response
            validation = self.validator.validate(raw_response)
            if not validation["passed"]:
                return await self._handle_quality_failure(enriched_query, validation)

            final_response = self._augment_response(
                validation["original_response"],
                pre_context,
                reasoning_metadata={
                    "sla_tier": routing_decision["tier"],
                    "provider": provider,
                    "reasoning_path": reasoning_result["source"]
                }
            )

            # 5. Learn and cache
            self.context.process_interaction(
                query,
                final_response,
                metadata={
                    "sla_tier": routing_decision["tier"],
                    "reasoning_source": reasoning_result["source"],
                    "provider": provider
                }
            )
            asyncio.create_task(self.cache_warmer.warm_cache(query.content))

            return final_response

        except Exception as e:
            self.logger.error(f"Routing failed: {str(e)}")
            return await self._handle_failure(query, e)

    async def _batch_process(self, batch: List[Dict]) -> Response:
        """Process batched queries through LLM"""
        try:
            # Get first provider that supports batching
            provider = next(
                p for p in {
                    query.get("provider") for query in batch
                } 
                if (plugin := self.context.plugin_manager.get_plugin(p)) 
                and plugin.supports_batching
            )
            
            llm = self.context.plugin_manager.get_plugin(provider)
            combined = [q["content"] for q in batch]
            responses = await llm.batch_complete(combined)
            
            # Return only the response for our original query
            original_query = batch[0]["content"]
            return next(
                Response(content=r) 
                for q, r in zip(combined, responses)
                if q == original_query
            )
        except Exception as e:
            self.logger.warning(f"Batch processing failed: {str(e)}")
            return await self.route_query(Query(**batch[0]))

    async def _handle_quality_failure(self, query: Query, validation: Dict) -> Response:
        """Process failed quality checks"""
        self.logger.warning(f"Quality check failed: {validation['checks']}")
        return await self._retry_with_stricter_llm(query)

    def _select_module(self, query: Query, context: dict, reasoning_source: str = None) -> BaseModule:
        """Enhanced module selection"""
        if reasoning_source == "graph":
            return self.registry.get_module("knowledge")
        
        if any(match["type"] == "code" for match in context["matches"]):
            lang = self._detect_language(context["matches"])
            return self.registry.get_module(f"code_{lang}")
            
        return self.registry.get_module("chat")
        
    def _detect_language(self, matches: List[dict]) -> str:
        """Detect programming language from knowledge matches"""
        lang_keywords = {
            "python": ["def", "import", "lambda"],
            "csharp": ["var", "using", "namespace"]
        }
        
        for match in matches:
            content = match.get("content", "").lower()
            for lang, keywords in lang_keywords.items():
                if any(kw in content for kw in keywords):
                    return lang
        return "generic"
        
    def _augment_response(self, response: Response, context: dict, reasoning_metadata: dict = None) -> Response:
        """Enhance response with contextual knowledge"""
        if not response.metadata:
            response.metadata = {}
            
        response.metadata.update({
            "context": {
                "matched_concepts": [
                    {"id": m["node_id"], "content": m["content"]}
                    for m in context["matches"][:3]
                ],
                "related_concepts": [
                    {"id": n["id"], "content": n["content"]}
                    for n in context["related"][:5]
                ]
            },
            "processing": reasoning_metadata or {}
        })
        return response
        
    async def _handle_failure(self, query: Query, error: Exception) -> Response:
        """Handle routing failures with fallback logic"""
        module_id = query.content_type.split("_")[-1]
        fallback_id = self.fallback_map.get(module_id, "generic")
        
        if fallback := self.registry.get_module(fallback_id):
            response = await fallback.process(query)
            self.context.process_interaction(query, response)
            return response
            
        raise RuntimeError("All fallback strategies failed")

    async def _retry_with_stricter_llm(self, query: Query) -> Response:
        """Fallback strategy for quality failures"""
        query.metadata["require_quality"] = True
        return await self.route_query(query)
---END---

FILE: core\performance\cost.py
---START---
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, Literal, Optional
import warnings

Provider = Literal['openai', 'anthropic', 'ollama', 'huggingface']

class CostMonitor:
    def __init__(self, config: Dict):
        self.config = config
        self.cost_db = Path("data/cost_tracking.json")
        self._init_db()
        self.current_spend = 0.0
        self._load_current_period()

    def _init_db(self):
        """Initialize cost database with default structure"""
        if not self.cost_db.exists():
            with open(self.cost_db, 'w') as f:
                json.dump({
                    "monthly_budget": self.config.get("monthly_budget", 100.0),
                    "periods": [],
                    "provider_rates": {
                        "openai": {"gpt-4": 0.03, "gpt-3.5": 0.002},
                        "anthropic": {"claude-2": 0.0465, "claude-instant": 0.0163},
                        "ollama": {"llama2": 0.0, "mistral": 0.0},
                        "huggingface": {"default": 0.0}
                    }
                }, f)

    def _load_current_period(self):
        """Load or create current billing period"""
        with open(self.cost_db, 'r') as f:
            data = json.load(f)
        
        current_date = datetime.now().strftime("%Y-%m")
        if not data["periods"] or data["periods"][-1]["period"] != current_date:
            data["periods"].append({
                "period": current_date,
                "total_spend": 0.0,
                "breakdown": {p: 0.0 for p in data["provider_rates"].keys()}
            })
        
        self.current_period = data["periods"][-1]
        self.current_spend = self.current_period["total_spend"]

    def record_llm_call(
        self,
        provider: Provider,
        model: str,
        input_tokens: int,
        output_tokens: int
    ):
        """Calculate and record API call costs"""
        rate = self._get_rate(provider, model)
        cost = (input_tokens * rate["input"] + output_tokens * rate["output"]) / 1000
        
        with open(self.cost_db, 'r+') as f:
            data = json.load(f)
            current = data["periods"][-1]
            current["total_spend"] += cost
            current["breakdown"][provider] += cost
            self.current_spend = current["total_spend"]
            
            # Check budget threshold (80% warning)
            if current["total_spend"] > data["monthly_budget"] * 0.8:
                warnings.warn(
                    f"Approaching budget limit: {current['total_spend']:.2f}/{data['monthly_budget']}",
                    RuntimeWarning
                )
            
            f.seek(0)
            json.dump(data, f, indent=2)

    def _get_rate(self, provider: Provider, model: str) -> Dict[str, float]:
        """Get current token rates for a provider/model"""
        with open(self.cost_db, 'r') as f:
            rates = json.load(f)["provider_rates"]
            provider_rates = rates.get(provider, {})
            return {
                "input": provider_rates.get(model, provider_rates.get("default", 0.0)),
                "output": provider_rates.get(model, provider_rates.get("default", 0.0))
            }

    def get_spend_forecast(self) -> Dict:
        """Predict end-of-period spend"""
        now = datetime.now()
        days_in_month = (now.replace(month=now.month+1, day=1) - timedelta(days=1)).day
        days_elapsed = now.day
        daily_avg = self.current_spend / days_elapsed
        
        return {
            "current_spend": self.current_spend,
            "projected_spend": daily_avg * days_in_month,
            "budget_remaining": self.config["monthly_budget"] - self.current_spend,
            "burn_rate": daily_avg
        }
---END---

FILE: core\performance\hashing.py
---START---
import hashlib
import json
from typing import Dict, Any

class QueryHasher:
    @staticmethod
    def hash_query(query: Dict[str, Any]) -> str:
        """Create consistent hash for similar queries"""
        normalized = {
            "code": query.get("code", "").strip(),
            "intent": query.get("intent", ""),
            "context": sorted(query.get("context", []))
        }
        return hashlib.sha256(
            json.dumps(normalized, sort_keys=True).encode()
        ).hexdigest()
---END---

FILE: core\performance\tracker.py
---START---
from datetime import datetime
from pathlib import Path
import json
import statistics
from typing import Dict, List, Literal

SolutionSource = Literal['graph', 'rule', 'llm', 'learned_rule']

class PerformanceTracker:
    def __init__(self):
        self.metrics_path = Path("data/performance_metrics.json")
        self._init_storage()
        self.session_metrics: List[Dict] = []

    def _init_storage(self):
        self.metrics_path.parent.mkdir(exist_ok=True)
        if not self.metrics_path.exists():
            with open(self.metrics_path, 'w') as f:
                json.dump({"sessions": []}, f)

    def record_metric(
        self,
        source: SolutionSource,
        latency: float,
        success: bool,
        query_hash: str
    ):
        """Record performance metrics for each solution"""
        metric = {
            "timestamp": datetime.utcnow().isoformat(),
            "source": source,
            "latency_ms": latency * 1000,
            "success": success,
            "query_hash": query_hash[:8]  # Truncated for privacy
        }
        self.session_metrics.append(metric)

    def get_recommended_source(self, query_hash: str) -> SolutionSource:
        """Determine optimal solution source based on history"""
        history = self._load_history()
        
        # Check for identical past queries
        if query_hash:
            for m in reversed(history):
                if m['query_hash'] == query_hash:
                    if m['success']:
                        return m['source']
                    break

        # Calculate source effectiveness
        success_rates = {}
        latencies = {}
        
        for source in ['graph', 'rule', 'llm', 'learned_rule']:
            source_metrics = [m for m in history if m['source'] == source]
            if source_metrics:
                success_rates[source] = sum(
                    1 for m in source_metrics if m['success']
                ) / len(source_metrics)
                latencies[source] = statistics.median(
                    [m['latency_ms'] for m in source_metrics]
                )

        # Prioritize by success then speed
        if success_rates:
            best_source = max(
                success_rates.keys(),
                key=lambda k: (success_rates[k], -latencies[k])
            )
            return best_source
        return 'llm'  # Default fallback

    def _load_history(self) -> List[Dict]:
        """Load historical metrics"""
        with open(self.metrics_path, 'r') as f:
            data = json.load(f)
            return data['sessions'] + self.session_metrics
            
    def get_provider_metrics(self) -> Dict[str, Dict]:
        """Calculate real-time performance metrics"""
        history = self._load_history()
        window = [m for m in history if m['timestamp'] > time.time() - 60]  # Last 60s
        
        metrics = {}
        for provider in set(m['source'] for m in window):
            provider_metrics = [m for m in window if m['source'] == provider]
            metrics[provider] = {
                'requests_per_second': len(provider_metrics) / 60,
                'avg_latency': np.mean([m['latency_ms'] for m in provider_metrics]) / 1000,
                'error_rate': sum(1 for m in provider_metrics if not m['success']) / len(provider_metrics)
            }
        return metrics

    def get_available_providers(self) -> List[str]:
        """List all currently enabled providers"""
        return ["gpt-4", "gpt-3.5", "claude-2", "llama2", "local"]  # From config
---END---

FILE: core\plugin.py
---START---
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Type
from dataclasses import dataclass
import importlib
import logging
from pathlib import Path

# ---------- Core Definitions ----------
@dataclass
class PluginMetadata:
    name: str
    version: str
    author: str = "Unknown"
    compatible_versions: str = ">=0.1.0"
    required_config: Dict[str, Any] = None
    dependencies: List[str] = None
    description: str = ""

class PluginBase(ABC):
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metadata = self.get_metadata()
        self._initialized = False
        self.logger = logging.getLogger(f"plugin.{self.metadata.name}")
        self._validate_config()

    # ---------- Required Interface ----------
    @abstractmethod
    def get_metadata(self) -> PluginMetadata:
        """Return plugin metadata"""
        pass

    @abstractmethod
    def initialize(self) -> bool:
        """Initialize plugin resources"""
        pass

    @abstractmethod
    def execute(self, input_data: Any) -> Any:
        """Main execution method"""
        pass

    # ---------- Core Functionality ----------
    def is_ready(self) -> bool:
        """Check if plugin is operational"""
        return self._initialized

    def cleanup(self):
        """Release all resources"""
        self._initialized = False
        self.logger.info(f"Cleanup completed for {self.metadata.name}")

    # ---------- Advanced Features ----------
    def _validate_config(self):
        """Validate configuration against metadata requirements"""
        if self.metadata.required_config:
            for field, expected_type in self.metadata.required_config.items():
                if field not in self.config:
                    raise ValueError(f"Missing config field: {field}")
                if not isinstance(self.config[field], expected_type):
                    raise TypeError(
                        f"Config field {field} requires {expected_type}, "
                        f"got {type(self.config[field])}"
                    )

    def health_check(self) -> Dict[str, Any]:
        """Detailed health report"""
        return {
            "name": self.metadata.name,
            "ready": self.is_ready(),
            "config_keys": list(self.config.keys()),
            "dependencies": self.metadata.dependencies or []
        }

    # ---------- Context Manager Support ----------
    def __enter__(self):
        if not self._initialized:
            self._initialized = self.initialize()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

# ---------- Plugin Manager ----------
class PluginManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.plugins: Dict[str, PluginBase] = {}
        self._discover_plugins()

    def _discover_plugins(self):
        """Discover and initialize all available plugins"""
        plugin_dir = Path(__file__).parent / "integrations"
        for py_file in plugin_dir.glob("*.py"):
            if py_file.stem == "__init__":
                continue
            
            try:
                module = importlib.import_module(
                    f"core.integrations.{py_file.stem}"
                )
                if hasattr(module, "Plugin"):
                    plugin_class = getattr(module, "Plugin")
                    if issubclass(plugin_class, PluginBase):
                        self._load_plugin(plugin_class)
            except Exception as e:
                logging.error(f"Failed to load {py_file.stem}: {str(e)}")

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Initialize and register a plugin"""
        plugin_name = plugin_class.__name__.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            plugin = plugin_class(plugin_config)
            if plugin.initialize():
                self.plugins[plugin_name] = plugin
                logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")

    def get_plugin(self, name: str) -> Optional[PluginBase]:
        """Retrieve a loaded plugin by name"""
        return self.plugins.get(name.lower())

    def list_plugins(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all plugins"""
        return {
            name: {
                "metadata": plugin.metadata,
                "ready": plugin.is_ready()
            }
            for name, plugin in self.plugins.items()
        }
    
    def reload_plugin(self, name: str) -> bool:
        """Hot-reload a plugin by name"""
        if name not in self.plugins:
            return False

        plugin = self.plugins[name]
        plugin.cleanup()
        
        try:
            module = importlib.import_module(f"core.integrations.{name}")
            importlib.reload(module)
            plugin_class = getattr(module, "Plugin")
            self._load_plugin(plugin_class)
            return True
        except Exception as e:
            logging.error(f"Failed to reload {name}: {str(e)}")
            return False

    def _resolve_dependencies(self, metadata: PluginMetadata) -> bool:
        """Install missing dependencies automatically"""
        if not metadata.dependencies:
            return True

        missing = []
        for dep in metadata.dependencies:
            try:
                req = requirements.Requirement(dep)
                importlib.import_module(req.name)
            except (ImportError, requirements.InvalidRequirement):
                missing.append(dep)

        if missing:
            logging.info(f"Installing dependencies: {', '.join(missing)}")
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", *missing],
                    stdout=subprocess.DEVNULL
                )
                return True
            except subprocess.CalledProcessError:
                logging.error(f"Failed to install dependencies: {missing}")
                return False
        return True

    def _check_version_compatibility(self, metadata: PluginMetadata) -> bool:
        """Verify plugin matches core version requirements"""
        try:
            core_req = requirements.Requirement(f"open_llm{metadata.compatible_versions}")
            current_version = requirements.Requirement(f"open_llm=={self.config['version']}")
            return current_version.specifier in core_req.specifier
        except requirements.InvalidRequirement:
            logging.warning(f"Invalid version spec: {metadata.compatible_versions}")
            return False

    def _load_plugin(self, plugin_class: Type[PluginBase]):
        """Enhanced plugin loading with new features"""
        metadata = plugin_class({}).get_metadata()
        
        if not self._check_version_compatibility(metadata):
            logging.error(f"Version mismatch for {metadata.name}")
            return

        if not self._resolve_dependencies(metadata):
            logging.error(f"Missing dependencies for {metadata.name}")
            return

        plugin_name = metadata.name.lower()
        plugin_config = self.config.get(plugin_name, {})
        
        try:
            with plugin_class(plugin_config) as plugin:
                if plugin.is_ready():
                    self.plugins[plugin_name] = plugin
                    logging.info(f"Successfully loaded {plugin_name}")
        except Exception as e:
            logging.error(f"Plugin {plugin_name} failed: {str(e)}")
---END---

FILE: core\prediction\cache.py
---START---
# core/prediction/cache.py
from typing import List, Dict
import numpy as np
from collections import deque

class CachePredictor:
    def __init__(self, context_manager, max_predictions=5):
        self.context = context_manager
        self.query_buffer = deque(maxlen=10)
        self.predictions = []
        
    def analyze_query_stream(self, new_query: str) -> List[str]:
        """Predict next 3 likely questions"""
        self.query_buffer.append(new_query)
        
        # 1. Get similar historical sequences
        similar_flows = self._find_similar_flows()
        
        # 2. Generate predictions (simplified example)
        return [
            "How to debug this?",
            "Better implementation?",
            "Related documentation"
        ][:max_predictions]

    def _find_similar_flows(self) -> List[Dict]:
        """Find similar query patterns in history"""
        # Implementation using your KnowledgeGraph
        return self.context.graph.find_similar_sequences(list(self.query_buffer))
---END---

FILE: core\prediction\warmer.py
---START---
# core/prediction/warmer.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

class CacheWarmer:
    def __init__(self, orchestrator, cache_predictor):
        self.orchestrator = orchestrator
        self.predictor = cache_predictor
        self.executor = ThreadPoolExecutor(2)

    async def warm_cache(self, current_query: str):
        """Pre-generate responses for predicted queries"""
        predicted = self.predictor.analyze_query_stream(current_query)
        
        # Run in background thread
        await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self._generate_responses,
            predicted
        )

    def _generate_responses(self, queries: List[str]):
        for query in queries:
            self.orchestrator.route_query(Query(content=query))
---END---

FILE: core\processing\batcher.py
---START---
# core/processing/batcher.py
from typing import List, Dict
import heapq
from dataclasses import dataclass, field
from sortedcontainers import SortedList

@dataclass(order=True)
class BatchItem:
    priority: int
    query: Dict = field(compare=False)
    created_at: float = field(default_factory=time.time, compare=False)

class AdaptiveBatcher:
    def __init__(self, max_batch_size=8, max_wait_ms=50):
        self.max_batch_size = max_batch_size
        self.max_wait = max_wait_ms / 1000
        self.pending = SortedList(key=lambda x: -x.priority)
        self.semaphore = asyncio.Semaphore(0)

    async def add_query(self, query: Dict, priority: int = 0) -> List[Dict]:
        """Add query to current batch, return completed batches if ready"""
        heapq.heappush(self.pending, BatchItem(priority, query))
        
        if len(self.pending) >= self.max_batch_size:
            return self._release_batch()
        
        await asyncio.wait_for(
            self.semaphore.acquire(),
            timeout=self.max_wait
        )
        return self._release_batch()

    def _release_batch(self) -> List[Dict]:
        """Extract queries for processing"""
        batch = [item.query for item in 
                heapq.nsmallest(self.max_batch_size, self.pending)]
        del self.pending[:len(batch)]
        return batch

    async def background_flush(self):
        """Periodically flush partial batches"""
        while True:
            await asyncio.sleep(self.max_wait)
            if self.pending:
                self.semaphore.release()
---END---

FILE: core\reasoning\engine.py
---START---
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from ..knowledge.graph import KnowledgeGraph
from ..context import ContextManager
import logging

class HybridEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph = context.graph
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.logger = logging.getLogger("reasoning.engine")
        self._init_rules()
        self.learning = SelfLearningEngine(context)
        self.performance = PerformanceTracker()
        self.query_hasher = QueryHasher()

    def _init_rules(self):
        """Load rule-based patterns"""
        self.rules = {
            'list_comp': {
                'pattern': '[x for x in {iterable} if {condition}]',
                'vars': ['iterable', 'condition']
            },
            'context_mgr': {
                'pattern': 'with {expr} as {var}:',
                'vars': ['expr', 'var']
            }
        }

    async def process(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """Main reasoning pipeline"""
        # Stage 1: Local Graph Check
        if graph_result := await self._check_graph(query):
            return graph_result

        # Stage 2: Rule Application
        if rule_result := self._apply_rules(query):
            return rule_result

        # Stage 3: LLM Fallback
        return await self._query_llm(query)
        
        result = await self._process_query(query)
        
        # Self-learning hook
        if result.get('success', True):
            self.learning.observe_solution(
                problem=query.get('code', ''),
                solution=str(result),
                source=result.get('source', 'llm')
            )
        
        return result
        
        query_hash = self.query_hasher.hash_query(query)
        recommended_source = self.performance.get_recommended_source(query_hash)
        
        # Route based on performance
        if recommended_source == 'graph':
            result = await self._check_graph(query)
        elif recommended_source == 'rule':
            result = self._apply_rules(query)
        else:
            result = await self._query_llm(query)

        # Record metrics
        self.performance.record_metric(
            source=result.get('source', 'llm'),
            latency=result['latency'],
            success=result.get('success', True),
            query_hash=query_hash
        )
        
        return result

    async def _check_graph(self, query: Dict) -> Optional[Dict]:
        """Check knowledge graph for solutions"""
        try:
            if 'code_context' in query:
                matches = self.graph.find_similar(
                    query['code_context'],
                    threshold=0.7
                )
                if matches:
                    return {'source': 'graph', 'result': matches[0]['solution']}
        except Exception as e:
            self.logger.error(f"Graph query failed: {str(e)}")
        return None

    def _apply_rules(self, query: Dict) -> Optional[Dict]:
        """Apply pre-defined coding patterns"""
        code = query.get('code', '')
        for rule_name, rule in self.rules.items():
            if all(var in code for var in rule['vars']):
                return {
                    'source': 'rule',
                    'rule': rule_name,
                    'template': rule['pattern'].format(**query)
                }
        return None

    async def _query_llm(self, query: Dict) -> Dict:
        """Route to best-suited LLM"""
        llm_pref = query.get('llm', self.context.config.get('default_llm'))
        return await self.context.plugin_manager.execute_llm(
            llm_pref,
            self._build_llm_payload(query)
        )

    def _build_llm_payload(self, query: Dict) -> Dict:
        """Enhance query with context"""
        return {
            **query,
            'context': self.context.get_relevant_context(query),
            'history': self.context.get_interaction_history()
        }
---END---

FILE: core\reasoning\rules.py
---START---
CODE_PATTERNS = {
    "list_comprehension": {
        "pattern": "[x for x in iterable if condition]",
        "transform": lambda match: {
            "template": match["pattern"],
            "variables": ["iterable", "condition"]
        }
    },
    "context_manager": {
        "pattern": "with expression as var:",
        "transform": lambda match: {
            "solution": f"with {match['expression']} as {match['var']}:"
        }
    }
}

class RuleEngine:
    @staticmethod
    def apply_pattern(code: str) -> Dict|None:
        """Match code against known patterns"""
        for pattern_name, pattern_data in CODE_PATTERNS.items():
            if pattern_data["pattern"] in code:
                return pattern_data["transform"](code)
        return None
---END---

FILE: core\self_healing.py
---START---
from dataclasses import dataclass
from enum import Enum, auto
import time
import asyncio
from typing import Dict, List, Optional
from modules.base_module import BaseModule

class HealthStatus(Enum):
    HEALTHY = auto()
    DEGRADED = auto()
    FAILED = auto()

@dataclass
class ModuleHealth:
    module_id: str
    status: HealthStatus
    last_checked: float
    failure_count: int = 0
    last_error: Optional[str] = None

class SelfHealingController:
    def __init__(self, registry):
        self.registry = registry
        self.health_status: Dict[str, ModuleHealth] = {}
        self._monitor_task = None
        
    async def start_monitoring(self, interval=60):
        """Start periodic health checks"""
        self._monitor_task = asyncio.create_task(self._monitor_loop(interval))
        
    async def _monitor_loop(self, interval):
        while True:
            await self.check_all_modules()
            await asyncio.sleep(interval)
            
    async def check_all_modules(self):
        """Check health of all registered modules"""
        for module_id, module in self.registry._instances.items():
            try:
                health_data = module.health_check()
                status = (
                    HealthStatus.DEGRADED if health_data.get("degraded", False) 
                    else HealthStatus.HEALTHY
                )
                self.health_status[module_id] = ModuleHealth(
                    module_id=module_id,
                    status=status,
                    last_checked=time.time()
                )
            except Exception as e:
                self._handle_module_failure(module_id, str(e))
                
    def _handle_module_failure(self, module_id: str, error: str):
        """Process module failure and initiate recovery"""
        if module_id not in self.health_status:
            self.health_status[module_id] = ModuleHealth(
                module_id=module_id,
                status=HealthStatus.FAILED,
                last_checked=time.time(),
                failure_count=1,
                last_error=error
            )
        else:
            self.health_status[module_id].failure_count += 1
            self.health_status[module_id].last_error = error
            self.health_status[module_id].status = HealthStatus.FAILED
            
        if self.health_status[module_id].failure_count > 3:
            self._attempt_recovery(module_id)
            
    def _attempt_recovery(self, module_id: str):
        """Execute recovery procedures for failed module"""
        module = self.registry._instances[module_id]
        try:
            # Attempt reinitialization
            module.initialize()
            self.health_status[module_id].status = HealthStatus.HEALTHY
            self.health_status[module_id].failure_count = 0
        except Exception as e:
            # If recovery fails, disable module temporarily
            self.health_status[module_id].status = HealthStatus.FAILED
            # TODO: Notify operators
            
    def get_available_modules(self) -> List[str]:
        """List modules currently available for routing"""
        return [
            module_id for module_id, health in self.health_status.items()
            if health.status != HealthStatus.FAILED
        ]
---END---

FILE: core\self_learning\engine.py
---START---
from typing import Dict, Any
from pathlib import Path
import json
import hashlib
from datetime import datetime
from ..knowledge.graph import KnowledgeGraph

class SelfLearningEngine:
    def __init__(self, context: ContextManager):
        self.context = context
        self.graph: KnowledgeGraph = context.graph
        self.learned_rules_path = Path("data/learned_rules.json")
        self._init_storage()

    def _init_storage(self):
        """Ensure learning storage exists"""
        self.learned_rules_path.parent.mkdir(exist_ok=True)
        if not self.learned_rules_path.exists():
            with open(self.learned_rules_path, 'w') as f:
                json.dump({"rules": []}, f)

    def observe_solution(self, problem: str, solution: str, source: str):
        """Record successful solutions"""
        problem_hash = hashlib.sha256(problem.encode()).hexdigest()
        
        # Store in knowledge graph
        self.graph.cache_solution(
            problem=problem,
            solution=solution,
            metadata={
                "source": source,
                "timestamp": datetime.utcnow().isoformat(),
                "usage_count": 0
            }
        )
        
        # Auto-generate rules for pattern-like solutions
        if self._is_pattern_candidate(solution):
            self._extract_rule(problem, solution)

    def _is_pattern_candidate(self, solution: str) -> bool:
        """Check if solution is generalizable"""
        return (solution.count('\n') <= 2 and 
                solution.count('(') < 3 and 
                'for ' in solution or 'with ' in solution)

    def _extract_rule(self, problem: str, solution: str):
        """Convert solutions into reusable rules"""
        # Basic pattern extraction
        vars = {
            'iterable': self._find_between(solution, 'for ', ' in'),
            'var': self._find_between(solution, 'for ', ' in').split()[0]
        } if 'for ' in solution else {
            'expr': self._find_between(solution, 'with ', ' as'),
            'var': self._find_between(solution, 'as ', ':').strip()
        }
        
        new_rule = {
            "template": solution,
            "vars": list(vars.keys()),
            "source_problem": problem,
            "last_used": None,
            "success_rate": 1.0
        }
        
        self._save_rule(new_rule)

    def _save_rule(self, rule: Dict[str, Any]):
        """Persist learned rules"""
        with open(self.learned_rules_path, 'r+') as f:
            data = json.load(f)
            data["rules"].append(rule)
            f.seek(0)
            json.dump(data, f, indent=2)
---END---

FILE: core\self_learning\rule_applier.py
---START---
import ast
from typing import Dict, Any

class RuleApplier:
    @staticmethod
    def apply_learned_rules(code: str, rules: list) -> Dict[str, Any]:
        """Apply learned rules to code context"""
        try:
            tree = ast.parse(code)
            for rule in sorted(rules, key=lambda x: x['success_rate'], reverse=True):
                if RuleApplier._matches_pattern(tree, rule['template']):
                    return {
                        "solution": rule['template'],
                        "confidence": rule['success_rate'],
                        "source": "learned_rule"
                    }
        except SyntaxError:
            pass
        return {}

    @staticmethod
    def _matches_pattern(tree: ast.AST, template: str) -> bool:
        """Check if code matches rule pattern"""
        try:
            template_tree = ast.parse(template)
            return ast.dump(tree) == ast.dump(template_tree)
        except:
            return False
---END---

FILE: core\service.py
---START---
from fastapi import FastAPI, APIRouter, HTTPException, WebSocket
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
import uvicorn
import asyncio
from typing import Dict, Any
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field
from shared.schemas import FeedbackRating, FeedbackCorrection  # Your new schemas

# Core imports
from core.integrations.manager import IntegrationManager
from core.reasoning.engine import HybridEngine
from core.orchestrator import Orchestrator
from core.self_healing import SelfHealingController
from core.context import ContextManager
from core.visualization import KnowledgeVisualizer
from core.versioning import KnowledgeVersioner

# Module system
from modules.registry import ModuleRegistry
from shared.schemas import Query

class AIService:
    def __init__(self, config: Dict[str, Any]):
        self.load_balancer = LoadBalancer(self.monitor)
        asyncio.create_task(self._update_weights_loop())
        self.config = config
        self.integration_manager = IntegrationManager(config.get("plugins", {}))
        self.reasoning = HybridEngine(self.context)
        
        self.app = FastAPI(
            title="AI Code Assistant",
            version="0.6.0",  # Bumped version
            docs_url="/api-docs"
        )
        
        # Core systems
        self.registry = ModuleRegistry()
        self.context = ContextManager()
        self.healing = SelfHealingController(self.registry)
        self.orchestrator = Orchestrator(
            registry=self.registry,
            healing=self.healing,
            context=self.context,
            reasoning=self.reasoning  # New dependency
        )
        self.visualizer = KnowledgeVisualizer(self.context.graph)
        self.versioner = KnowledgeVersioner(self.context.graph)
        
        self._setup()
        
        from core.feedback.processor import FeedbackProcessor  # Lazy import
        self.feedback_processor = FeedbackProcessor(self.context)
        
    async def _update_weights_loop(self):
        while True:
            await asyncio.sleep(self.config["load_balancing"]["update_interval"])
            self.load_balancer.update_weights()
    
    async def process_query(self, query: Dict) -> Dict:
        """Enhanced processing pipeline"""
        return await self.reasoning.process(query)

    def _setup(self):
        """Initialize all components"""
        # Setup filesystem
        Path("static").mkdir(exist_ok=True)
        Path("templates").mkdir(exist_ok=True)
        
        # Initialize modules
        self.registry.discover_modules()
        for module in self.registry._instances.values():
            module.context = self.context
            module.initialize()
            
        # Start background services
        asyncio.create_task(self.healing.start_monitoring())
        
        # Setup routes
        self._setup_routes()
        self._mount_static()

    def _setup_routes(self):
        @self.app.post("/process")
        async def process(query: Query):
            """Main processing endpoint with hybrid reasoning"""
            try:
                return await self.orchestrator.route_query(query)
            except Exception as e:
                raise HTTPException(status_code=503, detail=str(e))
                
        # Knowledge endpoints
        knowledge_router = APIRouter(prefix="/knowledge")
        
        @knowledge_router.get("")
        async def get_knowledge(concept: str = None):
            if concept:
                return self.context.graph.find_semantic_matches(concept)
            return {
                "stats": {
                    "nodes": len(self.context.graph.graph.nodes()),
                    "edges": len(self.context.graph.graph.edges()),
                    "interactions": len(self.context.interaction_log)
                }
            }
        
        # Specialized endpoints
        @self.app.post("/debug")
        async def debug_code(query: Query):
            try:
                return await self.registry.get_module("debug").process(query)
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
                
        @self.app.get("/health")
        async def health_check():
            """Simplified health check"""
            return {
                "status": "healthy",
                "services": {
                    name: plugin.is_ready()
                    for name, plugin in self.integration_manager.plugins.items()
                }
            }

        self.app.include_router(knowledge_router)
        
        @self.app.get("/cost-monitoring")
        async def get_cost_metrics():
            return {
                "current": service.cost_monitor.current_spend,
                "forecast": service.cost_monitor.get_spend_forecast(),
                "budget": service.cost_monitor.config["monthly_budget"]
            }
            
        # ===== ADD FEEDBACK ROUTES HERE =====
        @self.app.post("/feedback/rate", tags=["Feedback"])
        async def record_rating(feedback: FeedbackRating):
            """Record explicit user ratings (1-5 stars)"""
            self.feedback_processor.process_feedback({
                'type': 'positive' if feedback.rating >= 3 else 'negative',
                'rating': feedback.rating / 5,  # Normalize to 0-1
                'query_node': feedback.query_hash,
                'response_node': feedback.response_hash,
                'user_comment': feedback.comment,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "rating_recorded"}

        @self.app.post("/feedback/correct", tags=["Feedback"])
        async def record_correction(feedback: FeedbackCorrection):
            """Handle factual corrections from users"""
            self.feedback_processor.process_feedback({
                'type': 'correction',
                'target_node': feedback.node_id,
                'corrected_info': feedback.corrected_content,
                'severity': feedback.severity,
                'timestamp': datetime.utcnow().isoformat()
            })
            return {"status": "correction_applied"}

    def _mount_static(self):
        self.app.mount("/static", StaticFiles(directory="static"), name="static")
        
        @self.app.get("/")
        async def serve_ui():
            return FileResponse("templates/index.html")

    async def start_service(self, host: str = "0.0.0.0", port: int = 8000):
        """Corrected instance method"""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info"
        )
        server = uvicorn.Server(config)
        await server.serve()
---END---

FILE: core\signature_help.py
---START---
import re
from typing import Dict, List, Optional
from shared.schemas import SignatureHelp

class SignatureProvider:
    def __init__(self):
        self.patterns = {
            "python": r"def\s+(\w+)\s*\((.*?)\)",
            "javascript": r"function\s+(\w+)\s*\((.*?)\)",
            "csharp": r"public\s+\w+\s+(\w+)\s*\((.*?)\)"
        }

    def get_signature_help(self, code: str, language: str, cursor_pos: int) -> Optional[SignatureHelp]:
        """Extract function signature at cursor position"""
        matches = self._find_function_defs(code, language)
        current_func = self._get_function_at_pos(matches, cursor_pos)
        
        if current_func:
            params = self._parse_parameters(current_func[1])
            return SignatureHelp(
                name=current_func[0],
                parameters=params,
                active_parameter=self._get_active_param(current_func[1], cursor_pos)
            )
        return None

    def _find_function_defs(self, code: str, language: str) -> List[tuple]:
        """Find all function definitions in code"""
        pattern = self.patterns.get(language, self.patterns["python"])
        return re.findall(pattern, code, re.DOTALL)

    def _get_function_at_pos(self, functions: List[tuple], cursor_pos: int) -> Optional[tuple]:
        """Find which function contains the cursor position"""
        # Simplified - in reality would need AST parsing
        for func in functions:
            # Check if cursor is within function bounds
            if func[2] <= cursor_pos <= func[3]:  # (start_pos, end_pos)
                return func
        return None

    def _parse_parameters(self, param_str: str) -> List[Dict[str, str]]:
        """Parse parameter string into structured format"""
        params = []
        for p in param_str.split(','):
            p = p.strip()
            if p:
                parts = p.split()
                params.append({
                    "name": parts[-1],
                    "type": parts[0] if len(parts) > 1 else "any"
                })
        return params

    def _get_active_param(self, param_str: str, cursor_pos: int) -> int:
        """Determine which parameter is active based on cursor position"""
        if not param_str:
            return 0
        commas = [m.start() for m in re.finditer(',', param_str)]
        for i, pos in enumerate(commas):
            if cursor_pos <= pos:
                return i
        return len(commas)
---END---

FILE: core\state_manager.py
---START---
from typing import Dict, Any
from shared.schemas import Query, Response

class SessionState:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.context = {}
        self.history = []
        
    def update(self, query: Query, response: Response):
        self.history.append((query, response))
        self._update_context(query, response)
        
    def _update_context(self, query: Query, response: Response):
        """Extract and store relevant context"""
        self.context.update({
            "last_module": response.metadata.get("module"),
            "last_type": query.content_type
        })

class StateManager:
    def __init__(self):
        self.sessions: Dict[str, SessionState] = {}
        
    def get_session(self, session_id: str) -> SessionState:
        if session_id not in self.sessions:
            self.sessions[session_id] = SessionState(session_id)
        return self.sessions[session_id]
---END---

FILE: core\validation\quality_gates.py
---START---
from typing import Dict, Any
import re
from shared.schemas import Response

class QualityValidator:
    def __init__(self, config: Dict[str, Any]):
        self.standards = config.get("quality_standards", {})
        self.compiled_rules = {
            "code_safety": re.compile(r"(eval\(|system\(|os\.popen)"),
            "min_complexity": float(self.standards.get("min_complexity", 0.3))
        }

    def validate(self, response: Response) -> Dict[str, Any]:
        """Run all quality checks"""
        checks = {
            "safety": self._check_code_safety(response.content),
            "complexity": self._check_complexity(response.content),
            "formatting": self._check_formatting(response.content)
        }
        
        return {
            "passed": all(checks.values()),
            "checks": checks,
            "original_response": response
        }

    def _check_code_safety(self, content: str) -> bool:
        """Block dangerous code patterns"""
        if "code" not in content:
            return True
        return not self.compiled_rules["code_safety"].search(content["code"])

    def _check_complexity(self, content: str) -> bool:
        """Ensure sufficient solution quality"""
        complexity = self._calculate_complexity(content)
        return complexity >= self.compiled_rules["min_complexity"]

    def _calculate_complexity(self, text: str) -> float:
        """Simple complexity heuristic (0-1 scale)"""
        lines = text.split('\n')
        return min(
            (len([l for l in lines if l.strip()]) * 0.1) +
            (len(re.findall(r"\b(for|while|def|class)\b", text)) * 0.3),
            1.0
        )

    def _check_formatting(self, content: str) -> bool:
        """Validate basic structure"""
        return bool(
            isinstance(content, (str, dict)) and 
            (not isinstance(content, dict) or "answer" in content)
        )
---END---

FILE: modules\base_module.py
---START---
from abc import ABC, abstractmethod
from typing import List, Optional
from enum import Enum
from shared.schemas import Query, Response
from core.orchestrator import Capability

class BaseModule(ABC):
    MODULE_ID: str
    VERSION: str
    CAPABILITIES: List[Capability]
    PRIORITY: int = 0
    
    def __init__(self):
        self.context = None  # Will be set by service
        self._usage_count = 0
        
    @classmethod
    def get_metadata(cls) -> dict:
        return {
            "id": cls.MODULE_ID,
            "version": cls.VERSION,
            "capabilities": [cap.value for cap in cls.CAPABILITIES],
            "priority": cls.PRIORITY
        }
    
    async def initialize(self):
        """Initialize with module-specific knowledge"""
        self._load_domain_knowledge()
        self._ready = True
        
    @abstractmethod
    def _load_domain_knowledge(self):
        """Preload module-specific knowledge"""
        pass
        
    @abstractmethod
    async def process(self, query: Query) -> Response:
        """Process query using contextual knowledge"""
        pass
        
    def health_check(self) -> dict:
        """Report health including knowledge metrics"""
        return {
            "status": "ready" if self._ready else "loading",
            "version": self.VERSION,
            "usage": self._usage_count,
            "knowledge": self._get_knowledge_stats()
        }
        
    def _get_knowledge_stats(self) -> dict:
        """Get module-specific knowledge metrics"""
        if not self.context:
            return {}
            
        return {
            "nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n].get("module") == self.MODULE_ID
            ]),
            "relationships": len([
                e for e in self.context.graph.graph.edges()
                if self.context.graph.graph.edges[e].get("module") == self.MODULE_ID
            ])
        }
---END---

FILE: modules\module_ai.py
---START---
from modules.base_module import BaseModule
from core.integrations.manager import IntegrationManager

class AIModule(BaseModule):
    MODULE_ID = "ai_integration"
    CAPABILITIES = ["text_generation"]
    
    def __init__(self):
        self.integrations = {}
    
    async def initialize(self):
        # Initialize configured integrations
        self.integrations["ollama"] = IntegrationManager.get_integration("ollama")
        # Add others from config
        
    async def process(self, query: Query) -> Response:
        integration = self.integrations.get(query.metadata.get("integration"))
        if not integration:
            return Response.error("Integration not configured")
        
        try:
            result = integration.generate(
                query.content,
                **query.metadata.get("params", {})
            )
            return Response(content=result)
        except Exception as e:
            return Response.error(f"Generation failed: {str(e)}")
---END---

FILE: modules\module_completion.py
---START---
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.completion import CodeCompleter

class CompletionModule(BaseModule):
    MODULE_ID = "completion"
    CAPABILITIES = ["code_completion"]

    async def initialize(self):
        self.completer = CodeCompleter()

    async def process(self, query: Query) -> Response:
        completions = self.completer.generate_completions({
            "context": query.context.get("code", ""),
            "cursor_context": query.content
        })
        return Response(
            content="\n---\n".join(completions["completions"]),
            metadata={
                "type": "completion",
                "language": query.context.get("language", "unknown")
            }
        )
---END---

FILE: modules\module_debug.py
---START---
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.debugger import CodeDebugger

class DebugModule(BaseModule):
    MODULE_ID = "debug"
    CAPABILITIES = ["error_diagnosis", "fix_suggestion"]
    
    async def initialize(self):
        self.debugger = CodeDebugger(self.context.graph)
        
    async def process(self, query: Query) -> Response:
        if not query.context.get("error"):
            return Response(content="No error provided", metadata={})
            
        frames = self.debugger.analyze_traceback(
            query.context["code"],
            query.context["error"]
        )
        suggestions = self.debugger.suggest_fixes(frames)
        
        return Response(
            content=self._format_report(frames, suggestions),
            metadata={
                "frames": [f.__dict__ for f in frames],
                "suggestions": suggestions
            }
        )
        
    def _format_report(self, frames, suggestions) -> str:
        report = []
        for frame in frames:
            report.append(f"File {frame.file}, line {frame.line}:")
            report.append(f"Context:\n{frame.context}")
            report.append(f"Error: {frame.error}")
            if frame.line in suggestions:
                report.append("Suggestions:")
                report.extend(f"- {sug}" for sug in suggestions[frame.line])
            report.append("")
        return '\n'.join(report)
---END---

FILE: modules\module_generic.py
---START---
from modules.base_module import BaseModule
from shared.schemas import Response, Query

class GenericCodeModule(BaseModule):
    MODULE_ID = "code_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all code requests"""
        return Response(
            content=f"Generic code processing: {query.content[:200]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True,
                "warning": "Primary module unavailable"
            },
            metrics={"generic_processing": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_code_processing"]
        }

class GenericChatModule(BaseModule):
    MODULE_ID = "chat_generic"
    VERSION = "0.1.0"
    
    async def initialize(self):
        self._ready = True
        
    async def process(self, query: Query) -> Response:
        """Fallback processing for all requests"""
        return Response(
            content=f"Generic response: {query.content[:150]}...",
            metadata={
                "module": self.MODULE_ID,
                "fallback": True
            },
            metrics={"generic_response": 1.0}
        )
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "features": ["basic_text_processing"]
        }
---END---

FILE: modules\module_python.py
---START---
from modules.base_module import BaseModule
from shared.schemas import Response, Query
from core.orchestrator import Capability

class PythonModule(BaseModule):
    MODULE_ID = "python"
    VERSION = "0.2.0"
    CAPABILITIES = [
        Capability.CODE_COMPLETION,
        Capability.DEBUGGING,
        Capability.DOCSTRING
    ]
    PRIORITY = 10
    
    async def initialize(self):
        self._ready = True
        # Initialize with Python-specific knowledge
        self._init_python_knowledge()
        
    def _init_python_knowledge(self):
        """Preload Python-specific concepts"""
        python_concepts = [
            ("list", "mutable sequence"),
            ("dict", "key-value mapping"),
            ("generator", "iterator creator"),
            ("decorator", "function wrapper")
        ]
        
        for concept, desc in python_concepts:
            self.context.graph.add_entity(
                content=concept,
                type="python_concept",
                metadata={
                    "description": desc,
                    "language": "python"
                }
            )
        
    async def process(self, query: Query) -> Response:
        """Process Python queries with knowledge context"""
        # Extract context from query metadata
        context = query.context.get("knowledge_graph", {})
        
        # Generate response using contextual knowledge
        response_content = self._generate_response(query.content, context)
        
        return Response(
            content=response_content,
            metadata={
                "module": self.MODULE_ID,
                "capabilities": [cap.value for cap in self.CAPABILITIES],
                "context_used": bool(context)
            },
            metrics={"python_processing": 0.42}
        )
        
    def _generate_response(self, content: str, context: dict) -> str:
        """Generate response using available knowledge"""
        # Simplified response generation
        if "def " in content:
            return f"Python function suggestion based on {len(context.get('nodes', []))} related concepts..."
        return f"Python code solution referencing {context.get('edges', [])[:2]}..."
        
    def health_check(self) -> dict:
        return {
            "status": "ready",
            "version": self.VERSION,
            "knowledge_nodes": len([
                n for n in self.context.graph.graph.nodes()
                if self.context.graph.graph.nodes[n]['type'] == "python_concept"
            ])
        }
    
    async def process(self, query: Query) -> Response:
        """Enhanced processing with visualization support"""
        # Generate standard response
        response = await super().process(query)
        
        # Add visualization if requested
        if "visualize" in query.tags:
            graph_data = self._extract_relevant_subgraph(query.content)
            response.metadata["visualization"] = {
                "type": "knowledge_subgraph",
                "data": graph_data
            }
            
        return response
        
    def _extract_relevant_subgraph(self, content: str) -> dict:
        """Create a subgraph relevant to the query"""
        matches = self.context.graph.find_semantic_matches(content)
        if not matches:
            return {}
            
        central_node = matches[0]["node_id"]
        subgraph = nx.ego_graph(self.context.graph.graph, central_node, radius=2)
        
        return {
            "central_concept": self.context.graph.graph.nodes[central_node],
            "related": [
                {
                    "id": n,
                    "content": self.context.graph.graph.nodes[n]["content"],
                    "type": self.context.graph.graph.nodes[n]["type"],
                    "relations": [
                        {
                            "target": e[1],
                            "type": e[2]["type"],
                            "weight": e[2].get("weight", 1.0)
                        }
                        for e in subgraph.edges(n, data=True)
                    ]
                }
                for n in subgraph.nodes() if n != central_node
            ]
        }
---END---

FILE: modules\module_signature.py
---START---
from modules.base_module import BaseModule
from shared.schemas import Query, Response
from core.signature_help import SignatureProvider

class SignatureModule(BaseModule):
    MODULE_ID = "signature"
    CAPABILITIES = ["signature_help"]

    async def initialize(self):
        self.provider = SignatureProvider()

    async def process(self, query: Query) -> Response:
        help_data = self.provider.get_signature_help(
            code=query.context.get("code", ""),
            language=query.context.get("language", "python"),
            cursor_pos=query.context.get("cursor_pos", 0)
        )
        return Response(
            content=help_data if help_data else "No signature found",
            metadata={"type": "signature_help"}
        )
---END---

FILE: modules\registry.py
---START---
import importlib
import inspect
from pathlib import Path
from typing import Dict, Type
from .base_module import BaseModule
from core.orchestrator import CapabilityRouter

class ModuleRegistry:
    def __init__(self):
        self._modules: Dict[str, Type[BaseModule]] = {}
        self._instances: Dict[str, BaseModule] = {}
        self.router = CapabilityRouter()
        
    def discover_modules(self, package="modules"):
        """Automatically discover and register all modules"""
        modules_dir = Path(__file__).parent
        
        for module_file in modules_dir.glob("module_*.py"):
            module_name = module_file.stem
            module = importlib.import_module(f"{package}.{module_name}")
            
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) and 
                    issubclass(obj, BaseModule) and 
                    obj != BaseModule):
                    self.register_module(obj)
    
    def register_module(self, module_class: Type[BaseModule]):
        """Register a single module class"""
        instance = module_class()
        self._modules[module_class.MODULE_ID] = module_class
        self._instances[module_class.MODULE_ID] = instance
        self.router.register_module(
            instance,
            module_class.CAPABILITIES,
            module_class.PRIORITY
        )
        return instance
        
    def get_module(self, module_id: str) -> BaseModule:
        return self._instances.get(module_id)
---END---

FILE: monitoring\dashboard.json
---START---
{
  "metrics": [
    {
      "title": "Requests/Min",
      "query": "rate(llm_requests_total[1m])",
      "type": "line"
    },
    {
      "title": "Latency (99p)",
      "query": "histogram_quantile(0.99, sum by(le)(rate(llm_response_latency_seconds_bucket[1m])))",
      "unit": "s"
    }
  ]
}
---END---

FILE: package.json
---START---
{
  "name": "llm-code-assistant-ui",
  "version": "1.0.0",
  "scripts": {
    "build": "webpack --mode production",
    "dev": "webpack --watch --mode development",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "d3": "^7.8.5",
    "typescript": "^5.3.3"
  },
  "devDependencies": {
    "@types/d3": "^7.4.2",
    "css-loader": "^6.8.1",
    "mini-css-extract-plugin": "^2.7.6",
    "sass": "^1.69.5",
    "sass-loader": "^13.3.2",
    "ts-loader": "^9.5.1",
    "webpack": "^5.89.0",
    "webpack-cli": "^5.1.4"
  }
}
---END---

FILE: README.md
---START---
# Open LLM Code Assistant

An AI-powered coding assistant with hybrid reasoning, self-learning capabilities, and multi-LLM orchestration.

## Features

✅ **Hybrid Reasoning Engine** - Combines rule-based patterns, knowledge graphs, and LLMs  
✅ **Multi-LLM Integration** - Supports Ollama, vLLM, HuggingFace, Groq, and more  
✅ **Adaptive Routing** - Dynamic load balancing and SLA-aware prioritization  
✅ **Self-Learning** - Improves from user feedback and corrections  
✅ **Quality Gates** - Automated validation of all responses  
✅ **Predictive Caching** - Anticipates and pre-computes likely queries  

## Installation

```bash
git clone https://github.com/bozozeclown/open_llm.git
cd open_llm
pip install -r requirements.txt

# Configure your integrations
cp configs/integration.example.yaml configs/integration.yaml
Configuration
Edit configs/integration.yaml to enable your preferred LLM providers:

yaml
plugins:
  ollama:
    enabled: true
    config:
      base_url: "http://localhost:11434"
      default_model: "codellama"
Usage
Start the service:

bash
python -m core.service
Access the web interface at http://localhost:8000 or use the API:

python
from client import OpenLLMClient
client = OpenLLMClient()
response = client.query("How to reverse a list in Python?")


TO DO

Core Improvements

- Implement cross-provider benchmark testing
- Add automatic failover when providers go offline
- Develop prompt versioning system

Performance

- Optimize knowledge graph queries
- Add GPU memory monitoring for vLLM
- Implement query result compression

New Features

- Add multi-modal support (images + code)
- Develop VS Code extension
- Create collaboration history replay

Documentation

- Write API usage examples
- Create architecture diagrams
- Add developer onboarding guide

Maintenance

- Upgrade to Pydantic v2
- Add integration test suite
- Set up CI/CD pipeline

Contributing

- Fork the repository
- Create a feature branch (git checkout -b feature/your-feature)
- Commit your changes
- Push to the branch
- Open a pull request

License
MIT License - See LICENSE for details.
---END---

FILE: requirements.txt
---START---
torch>=2.0.1
transformers>=4.30.0
accelerate>=0.20.0
datasets>=2.12.0
peft>=0.4.0
wandb>=0.15.0
fastapi>=0.95.0
uvicorn>=0.22.0
---END---

FILE: shared\config\init.py
---START---
import yaml
from pathlib import Path
from typing import Any, Dict

class ConfigManager:
    _config: Dict[str, Any] = {}
    
    @classmethod
    def load_configs(cls, config_dir: str = "shared/config"):
        config_path = Path(config_dir)
        
        for config_file in config_path.glob("*.yaml"):
            with open(config_file) as f:
                cls._config[config_file.stem] = yaml.safe_load(f)
                
    @classmethod
    def get(cls, key: str, default: Any = None) -> Any:
        keys = key.split(".")
        value = cls._config
        
        for k in keys:
            value = value.get(k)
            if value is None:
                return default
                
        return value
---END---

FILE: shared\config\loader.py
---START---
# shared/config/loader.py
import watchdog.events
import yaml

class ConfigWatcher(watchdog.events.FileSystemEventHandler):
    def __init__(self, callback):
        self.callback = callback
        
    def on_modified(self, event):
        if event.src_path.endswith('.yaml'):
            self.callback(event.src_path)

# Enhanced ConfigManager
class ConfigManager:
    def __init__(self):
        self._callbacks = []
        self.load_configs()
        
    def register_callback(self, callback):
        self._callbacks.append(callback)
        
    def _notify_changes(self, changed_file):
        for callback in self._callbacks:
            callback(self, changed_file)
    
    def load_config():
    """Safe config reload that preserves existing connections"""
    new_config = yaml.safe_load(open("configs/integration.yaml"))
    for key in current_config:
        if key in new_config:
            current_config[key].update(new_config[key])
---END---

FILE: shared\knowledge\graph.py
---START---
from dataclasses import dataclass
from typing import Dict, List, Set, Optional
import networkx as nx
from enum import Enum
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
import spacy

class EntityType(Enum):
    CONCEPT = "concept"
    CODE = "code"
    API = "api"
    LIBRARY = "library"
    ERROR = "error"
    PATTERN = "pattern"

@dataclass
class KnowledgeNode:
    id: str
    type: EntityType
    content: str
    metadata: dict
    embeddings: Optional[np.ndarray] = None

class KnowledgeGraph:
    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.nlp = spacy.load("en_core_web_sm")
        self._setup_indices()
        
    def _setup_indices(self):
        """Initialize data structures for efficient lookup"""
        self.content_index = {}  # content -> node_id
        self.embedding_index = []  # List of (node_id, embedding)
        
    def _generate_id(self, content: str, type: EntityType) -> str:
        """Create deterministic node ID"""
        return hashlib.sha256(f"{type.value}:{content}".encode()).hexdigest()
        
    def add_entity(self, content: str, type: EntityType, metadata: dict = None) -> str:
        """Add or update an entity with enhanced NLP processing"""
        # Preprocess content
        doc = self.nlp(content)
        normalized_content = " ".join([token.lemma_ for token in doc if not token.is_stop])
        
        node_id = self._generate_id(normalized_content, type)
        embedding = self.encoder.encode(normalized_content)
        
        if node_id not in self.graph:
            self.graph.add_node(node_id, 
                type=type,
                content=content,
                normalized=normalized_content,
                metadata=metadata or {},
                embedding=embedding
            )
            self.content_index[normalized_content] = node_id
            self.embedding_index.append((node_id, embedding))
        else:
            # Update existing node
            self.graph.nodes[node_id]['metadata'].update(metadata or {})
            
        return node_id
        
    def add_relation(self, source_id: str, target_id: str, relation_type: str, weight: float = 1.0):
        """Create weighted relationship between entities"""
        if source_id in self.graph and target_id in self.graph:
            self.graph.add_edge(source_id, target_id, 
                type=relation_type,
                weight=weight
            )
            
    def find_semantic_matches(self, query: str, threshold: float = 0.7) -> List[dict]:
        """Find knowledge nodes semantically similar to query"""
        query_embedding = self.encoder.encode(query)
        matches = []
        
        for node_id, emb in self.embedding_index:
            similarity = np.dot(query_embedding, emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(emb)
            )
            if similarity > threshold:
                matches.append({
                    "node_id": node_id,
                    "similarity": float(similarity),
                    **self.graph.nodes[node_id]
                })
                
        return sorted(matches, key=lambda x: x["similarity"], reverse=True)
        
    def expand_from_text(self, text: str, source: str = "user"):
        """Automatically extract and add knowledge from text"""
        doc = self.nlp(text)
        
        # Extract entities and noun phrases
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        noun_chunks = [(chunk.text, "NOUN_PHRASE") for chunk in doc.noun_chunks]
        
        # Add to knowledge graph
        nodes = []
        for content, label in entities + noun_chunks:
            node_id = self.add_entity(
                content=content,
                type=self._map_spacy_label(label),
                metadata={"source": source}
            )
            nodes.append(node_id)
            
        # Create relationships based on syntactic dependencies
        for sent in doc.sents:
            for token in sent:
                if token.dep_ in ("dobj", "nsubj", "attr"):
                    source = self._get_node_for_token(token.head)
                    target = self._get_node_for_token(token)
                    if source and target:
                        self.add_relation(source, target, token.dep_)
    
    def _map_spacy_label(self, label: str) -> EntityType:
        """Map Spacy labels to our entity types"""
        mapping = {
            "PERSON": "concept",
            "ORG": "concept",
            "GPE": "concept",
            "PRODUCT": "api",
            "NOUN_PHRASE": "concept"
        }
        return EntityType(mapping.get(label, "concept"))
        
    def _get_node_for_token(self, token) -> Optional[str]:
        """Find or create node for a Spacy token"""
        text = token.lemma_
        return self.content_index.get(text)
        
    def get_statistics(self) -> dict:
        """Return comprehensive graph statistics"""
        centrality = nx.degree_centrality(self.graph)
        top_nodes = sorted(
            [(n, c) for n, c in centrality.items()],
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            "basic": {
                "nodes": len(self.graph.nodes()),
                "edges": len(self.graph.edges()),
                "components": nx.number_weakly_connected_components(self.graph)
            },
            "centrality": {
                "top_concepts": [
                    {"id": n[0], "content": self.graph.nodes[n[0]]["content"], "score": n[1]}
                    for n in top_nodes
                ]
            },
            "types": {
                nt: sum(1 for n in self.graph.nodes() if self.graph.nodes[n]["type"] == nt)
                for nt in set(nx.get_node_attributes(self.graph, "type").values())
            }
        }

    def export_gexf(self, path: str):
        """Export graph to GEXF format for external tools"""
        nx.write_gexf(self.graph, path)
        
    def find_code_patterns(self, pattern_type: str) -> list:
        """Enhanced pattern matching for common code structures"""
        return self._query_graph(
            f"""
            MATCH (n:CodePattern {{type: $pattern_type}})
            RETURN n
            """,
            {"pattern_type": pattern_type}
        )
    
    def cache_solution(self, problem_hash: str, solution: Dict):
        """Store successful solutions for future reuse"""
        self._store_node("Solution", {
            "hash": problem_hash,
            "solution": solution
        })
        
    def find_similar(self, code_snippet: str, threshold: float = 0.8):
        """Find similar code patterns using vector similarity"""
        query_embedding = self.embedder.encode(code_snippet)
        results = []
        
        for node in self.graph.nodes(data=True):
            if 'embedding' in node[1]:
                similarity = cosine_similarity(
                    query_embedding,
                    node[1]['embedding']
                )
                if similarity > threshold:
                    results.append({
                        'node': node[0],
                        'similarity': similarity,
                        'solution': node[1].get('solution')
                    })
        
        return sorted(results, key=lambda x: x['similarity'], reverse=True)

    def cache_solution(self, problem: str, solution: str):
        """Store successful solutions with embeddings"""
        embedding = self.embedder.encode(problem)
        self.graph.add_node(problem, solution=solution, embedding=embedding)
---END---

FILE: shared\schemas.py
---START---
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

class CompletionRequest(BaseModel):
    context: str  # Full file content
    cursor_context: str  # Line/fragment near cursor
    
class SignatureHelp(BaseModel):
    name: str
    parameters: List[Dict[str, str]]
    active_parameter: int

class SignatureRequest(BaseModel):
    code: str
    language: str
    cursor_pos: int
    
class HealthStatus(BaseModel):
    service: str
    status: Literal["online", "degraded", "offline"]
    models: List[str] = []
    latency: Optional[float]

class IntegrationConfig(BaseModel):
    priority: int
    timeout: int = 30
    
class Query(BaseModel):
    """Enhanced query class with routing support"""
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    # Add these new methods
    def with_additional_context(self, reasoning_data: Dict) -> 'Query':
        """
        Create a new query instance with reasoning context
        Usage:
            enriched_query = original_query.with_additional_context(
                {"source": "graph", "confidence": 0.9}
            )
        """
        return self.copy(
            update={
                "metadata": {
                    **self.metadata,
                    "reasoning": reasoning_data
                }
            }
        )
    
    @property
    def preferred_provider(self) -> Optional[str]:
        """Get preferred LLM provider if specified"""
        return self.metadata.get('preferred_provider')
    
    @preferred_provider.setter
    def preferred_provider(self, provider: str):
        """Set preferred LLM provider"""
        self.metadata['preferred_provider'] = provider
        
class FeedbackRating(BaseModel):
    query_hash: str
    response_hash: str
    rating: float = Field(..., ge=0, le=5)
    comment: Optional[str]
    
class FeedbackCorrection(BaseModel):
    node_id: str
    corrected_content: str
    severity: Literal["low", "medium", "high"] = "medium"
---END---

FILE: static\css\debugger.css
---START---
.debug-panel {
    border: 1px solid #e1e4e8;
    border-radius: 6px;
    padding: 16px;
    margin-top: 20px;
    background: #f6f8fa;
}

.debug-frame {
    margin-bottom: 20px;
    padding: 15px;
    background: white;
    border-radius: 4px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.debug-frame pre {
    background: #f0f0f0;
    padding: 8px;
    border-radius: 3px;
    overflow-x: auto;
}

.var {
    display: inline-block;
    background: #e1f5fe;
    padding: 2px 6px;
    border-radius: 3px;
    margin-right: 5px;
    font-family: monospace;
}

.suggestions ul {
    margin: 5px 0 0 20px;
    padding: 0;
}

.suggestions li {
    margin: 5px 0;
}
---END---

FILE: static\css\graph.css
---START---
/* Layout */
#graph-explorer-container {
    width: 100%;
    height: 800px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.controls {
    padding: 12px;
    background: #f5f5f5;
    margin-bottom: 12px;
    border-radius: 4px;
    display: flex;
    gap: 12px;
    align-items: center;
}

.controls button {
    padding: 6px 12px;
    background-color: #2962FF;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-size: 14px;
}

.controls button:hover {
    background-color: #1E4DCC;
}

.controls label {
    display: flex;
    align-items: center;
    gap: 6px;
    font-size: 14px;
}

/* Graph Canvas */
#graph-canvas {
    width: 100%;
    height: calc(100% - 120px);
    border: 1px solid #ddd;
    border-radius: 4px;
    background: white;
}

/* Graph Elements */
.graph-node {
    cursor: pointer;
    stroke: white;
    stroke-width: 1.5px;
}

.graph-node:hover {
    stroke-width: 2px;
}

.graph-link {
    stroke: #888;
    stroke-opacity: 0.6;
}

.node-label {
    pointer-events: none;
    user-select: none;
    font-family: 'Segoe UI', sans-serif;
}

/* Analytics Panel */
#graph-analytics {
    margin-top: 12px;
    padding: 12px;
    background: #f5f5f5;
    border-radius: 4px;
}

#graph-analytics h3 {
    margin: 0 0 8px 0;
    color: #2962FF;
    font-size: 16px;
}

.metric {
    margin-bottom: 8px;
}

.metric-label {
    font-weight: bold;
    color: #FF6D00;
    margin-right: 8px;
}

.metric-values {
    display: inline-flex;
    gap: 12px;
}
---END---

FILE: static\css\signature.css
---START---
.signature-tooltip {
    position: fixed;
    background: #2d2d2d;
    color: white;
    padding: 8px 12px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 14px;
    z-index: 1000;
    box-shadow: 0 2px 8px rgba(0,0,0,0.2);
    max-width: 500px;
}

.signature-title {
    color: #569cd6;
}

.active-param {
    color: #9cdcfe;
    font-weight: bold;
}

.signature-tooltip span {
    margin: 0 2px;
}
---END---

FILE: static\js\completion.js
---START---
class CompletionUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.setupListeners();
    }

    setupListeners() {
        this.editor.addEventListener("keydown", async (e) => {
            if (e.key === "Tab" || (e.key === " " && e.ctrlKey)) {
                e.preventDefault();
                const completions = await this.fetchCompletions();
                this.showCompletions(completions);
            }
        });
    }

    async fetchCompletions() {
        const response = await fetch("/completion", {
            method: "POST",
            body: JSON.stringify({
                content: this.getCursorContext(),
                context: {
                    code: this.editor.value,
                    language: "python"  # Dynamic in real impl
                }
            })
        });
        return await response.json();
    }

    getCursorContext() {
        const cursorPos = this.editor.selectionStart;
        return this.editor.value.substring(
            Math.max(0, cursorPos - 50), 
            cursorPos
        );
    }

    showCompletions(completions) {
        // Render as dropdown or inline suggestions
        console.log("Suggestions:", completions);
    }
}
---END---

FILE: static\js\debugger.js
---START---
class DebuggerUI {
    constructor() {
        this.container = document.getElementById('debug-container');
        this.setupUI();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="debug-panel">
                <h3>Debug Assistant</h3>
                <div class="debug-frames"></div>
                <button id="analyze-btn">Analyze Error</button>
            </div>
        `;
        
        document.getElementById('analyze-btn').addEventListener('click', () => this.analyzeError());
    }

    async analyzeError() {
        const code = document.getElementById('code-editor').value;
        const error = document.getElementById('error-output').value;
        
        const response = await fetch('/debug', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                content: "debug_request",
                context: { code, error }
            })
        });
        
        const result = await response.json();
        this.displayResults(result);
    }

    displayResults(debugData) {
        const framesContainer = document.querySelector('.debug-frames');
        framesContainer.innerHTML = debugData.metadata.frames.map(frame => `
            <div class="debug-frame">
                <h4>${frame.file}:${frame.line}</h4>
                <pre>${frame.context}</pre>
                <div class="variables">${this.formatVariables(frame.variables)}</div>
                ${this.formatSuggestions(debugData.metadata.suggestions[frame.line] || [])}
            </div>
        `).join('');
    }

    formatVariables(vars) {
        return Object.entries(vars).map(([k, v]) => 
            `<span class="var">${k}=${v}</span>`
        ).join(' ');
    }

    formatSuggestions(suggestions) {
        if (!suggestions.length) return '';
        return `
            <div class="suggestions">
                <h5>Suggestions:</h5>
                <ul>${suggestions.map(s => `<li>${s}</li>`).join('')}</ul>
            </div>
        `;
    }
}
---END---

FILE: static\js\graph-explorer.js
---START---
// Configuration
const CONFIG = {
    nodeColors: {
        concept: '#FF6D00',
        code: '#2962FF',
        api: '#00C853',
        error: '#D50000',
        default: '#666666'
    },
    apiBaseUrl: '/knowledge/graph'
};

class GraphExplorer {
    constructor(containerId) {
        this.container = document.getElementById(containerId);
        this.simulation = null;
        this.currentGraph = { nodes: [], edges: [] };
        this.init();
    }

    async init() {
        this.setupUI();
        await this.loadGraph();
        this.setupEventListeners();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="controls">
                <button id="refresh-graph">Refresh</button>
                <label>
                    Depth: <input type="range" id="graph-depth" min="1" max="3" value="2">
                </label>
                <label>
                    Physics: <input type="checkbox" id="toggle-physics" checked>
                </label>
                <button id="analyze-btn">Run Analysis</button>
            </div>
            <div id="graph-canvas"></div>
            <div id="graph-analytics"></div>
        `;
    }

    async loadGraph(depth = 2, physics = true) {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/json?depth=${depth}`);
            this.currentGraph = await response.json();
            this.renderGraph(physics);
        } catch (error) {
            console.error('Failed to load graph:', error);
        }
    }

    renderGraph(enablePhysics) {
        const canvas = document.getElementById('graph-canvas');
        canvas.innerHTML = '';
        
        const width = canvas.clientWidth;
        const height = canvas.clientHeight;
        
        const svg = d3.select(canvas)
            .append('svg')
            .attr('width', width)
            .attr('height', height);
        
        // Create simulation
        this.simulation = d3.forceSimulation()
            .force('link', d3.forceLink().id(d => d.id))
            .force('charge', d3.forceManyBody().strength(-100))
            .force('center', d3.forceCenter(width / 2, height / 2));
        
        // Draw links
        const link = svg.append('g')
            .selectAll('line')
            .data(this.currentGraph.edges)
            .enter().append('line')
            .attr('class', 'graph-link')
            .attr('stroke-width', d => Math.sqrt(d.value));
        
        // Draw nodes
        const node = svg.append('g')
            .selectAll('circle')
            .data(this.currentGraph.nodes)
            .enter().append('circle')
            .attr('class', 'graph-node')
            .attr('r', 8)
            .attr('fill', d => CONFIG.nodeColors[d.type] || CONFIG.nodeColors.default)
            .call(d3.drag()
                .on('start', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0.3).restart();
                    d.fx = d.x;
                    d.fy = d.y;
                })
                .on('drag', (event, d) => {
                    d.fx = event.x;
                    d.fy = event.y;
                })
                .on('end', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0);
                    d.fx = null;
                    d.fy = null;
                }));
        
        // Add labels
        const label = svg.append('g')
            .selectAll('text')
            .data(this.currentGraph.nodes)
            .enter().append('text')
            .attr('class', 'node-label')
            .text(d => d.label)
            .attr('font-size', 10)
            .attr('dx', 10)
            .attr('dy', 4);
        
        // Update positions
        this.simulation.nodes(this.currentGraph.nodes)
            .on('tick', () => {
                link.attr('x1', d => d.source.x)
                    .attr('y1', d => d.source.y)
                    .attr('x2', d => d.target.x)
                    .attr('y2', d => d.target.y);
                
                node.attr('cx', d => d.x)
                    .attr('cy', d => d.y);
                
                label.attr('x', d => d.x)
                    .attr('y', d => d.y);
            });
        
        this.simulation.force('link')
            .links(this.currentGraph.edges);
        
        if (!enablePhysics) {
            this.simulation.stop();
        }
    }

    setupEventListeners() {
        document.getElementById('refresh-graph').addEventListener('click', () => {
            const depth = document.getElementById('graph-depth').value;
            const physics = document.getElementById('toggle-physics').checked;
            this.loadGraph(depth, physics);
        });

        document.getElementById('analyze-btn').addEventListener('click', () => {
            this.runAnalytics();
        });
    }

    async runAnalytics() {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/analytics`);
            const analytics = await response.json();
            this.displayAnalytics(analytics);
        } catch (error) {
            console.error('Failed to run analytics:', error);
        }
    }

    displayAnalytics(analytics) {
        const panel = document.getElementById('graph-analytics');
        panel.innerHTML = `
            <h3>Graph Analytics</h3>
            <div class="metric">
                <span class="metric-label">Central Nodes:</span>
                <div class="metric-values">
                    ${analytics.centrality.map(n => `
                        <div>${n.label} (${n.value.toFixed(3)})</div>
                    `).join('')}
                </div>
            </div>
            <div class="metric">
                <span class="metric-label">Communities:</span>
                <div>${analytics.community.count} detected</div>
            </div>
        `;
    }
	
	highlightNode(nodeId, clientId) {
        const color = this.getClientColor(clientId);
        d3.select(`circle[data-id="${nodeId}"]`)
            .transition()
            .attr("stroke", color)
            .attr("stroke-width", 3);
    }
    
    getClientColor(clientId) {
        // Simple deterministic color assignment
        const colors = ["#FF00FF", "#00FFFF", "#FFFF00", "#FF9900"];
        const index = parseInt(clientId.split("-")[1]) % colors.length;
        return colors[index];
    }
    
    refreshNode(nodeId) {
        // Refresh node visualization
        this.loadGraph(this.currentDepth, true);
    }
	
	renderHistoricalGraph(graphData) {
        // Clear current graph
        d3.select("#graph-canvas").selectAll("*").remove();
        
        // Render historical version
        this.currentGraph = graphData;
        this.renderGraph(false); // Disable physics for historical views
        
        // Visual indication
        d3.select("#graph-canvas")
            .append("rect")
            .attr("width", "100%")
            .attr("height", "100%")
            .attr("fill", "rgba(0,0,0,0.1)")
            .attr("class", "historical-overlay");
    }
	
}

// Initialize when loaded
window.addEventListener('DOMContentLoaded', () => {
    new GraphExplorer('graph-explorer-container');
});
---END---

FILE: static\js\signature.js
---START---
class SignatureUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.tooltip = this._createTooltip();
        this._setupListeners();
    }

    _createTooltip() {
        const tooltip = document.createElement('div');
        tooltip.className = 'signature-tooltip';
        tooltip.style.display = 'none';
        document.body.appendChild(tooltip);
        return tooltip;
    }

    _setupListeners() {
        this.editor.addEventListener('mousemove', this._debounce(async (e) => {
            const pos = this._getCursorPosition(e);
            const signature = await this._fetchSignature(pos);
            if (signature) this._showSignature(signature);
        }, 300));
    }

    async _fetchSignature(cursorPos) {
        const response = await fetch('/signature', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                content: this.editor.value,
                context: {
                    code: this.editor.value,
                    language: 'python',
                    cursor_pos: cursorPos
                }
            })
        });
        return await response.json();
    }

    _showSignature(data) {
        if (!data.name) {
            this.tooltip.style.display = 'none';
            return;
        }

        const params = data.parameters.map((p, i) => 
            `<span class="${i === data.active_parameter ? 'active-param' : ''}">
                ${p.type ? `${p.type} ` : ''}${p.name}
            </span>`
        ).join(', ');

        this.tooltip.innerHTML = `
            <div class="signature-title">${data.name}(${params})</div>
        `;
        this._positionTooltip();
        this.tooltip.style.display = 'block';
    }

    _positionTooltip() {
        // Position near cursor (simplified)
        const rect = this.editor.getBoundingClientRect();
        this.tooltip.style.left = `${rect.left + 20}px`;
        this.tooltip.style.top = `${rect.top - 40}px`;
    }

    _debounce(func, delay) {
        let timeout;
        return (...args) => {
            clearTimeout(timeout);
            timeout = setTimeout(() => func.apply(this, args), delay);
        };
    }
}
---END---

FILE: templates\index.html
---START---
<!DOCTYPE html>
<html>
<head>
    <title>Knowledge Graph Explorer</title>
    <link rel="stylesheet" href="/static/css/graph.css">
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
    <div id="graph-explorer-container"></div>
    <script src="/static/js/graph-explorer.js"></script>
</body>
</html>
---END---

FILE: webpack.config.js
---START---
const path = require('path');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');

module.exports = {
    entry: {
        app: './static/ts/graph-explorer.ts',
        styles: './static/scss/main.scss'
    },
    output: {
        filename: 'js/[name].bundle.js',
        path: path.resolve(__dirname, 'static/dist')
    },
    resolve: {
        extensions: ['.ts', '.js', '.scss']
    },
    module: {
        rules: [
            {
                test: /\.ts$/,
                use: 'ts-loader',
                exclude: /node_modules/
            },
            {
                test: /\.scss$/,
                use: [
                    MiniCssExtractPlugin.loader,
                    'css-loader',
                    'sass-loader'
                ]
            }
        ]
    },
    plugins: [
        new MiniCssExtractPlugin({
            filename: 'css/[name].css'
        })
    ],
    externals: {
        d3: 'd3'
    }
};
---END---




=== README.md ===
# Open LLM Code Assistant

An AI-powered coding assistant with hybrid reasoning, self-learning capabilities, and multi-LLM orchestration.

## Features

✅ **Hybrid Reasoning Engine** - Combines rule-based patterns, knowledge graphs, and LLMs  
✅ **Multi-LLM Integration** - Supports Ollama, vLLM, HuggingFace, Groq, and more  
✅ **Adaptive Routing** - Dynamic load balancing and SLA-aware prioritization  
✅ **Self-Learning** - Improves from user feedback and corrections  
✅ **Quality Gates** - Automated validation of all responses  
✅ **Predictive Caching** - Anticipates and pre-computes likely queries  

## Installation

```bash
git clone https://github.com/bozozeclown/open_llm.git
cd open_llm
pip install -r requirements.txt

# Configure your integrations
cp configs/integration.example.yaml configs/integration.yaml
Configuration
Edit configs/integration.yaml to enable your preferred LLM providers:

yaml
plugins:
  ollama:
    enabled: true
    config:
      base_url: "http://localhost:11434"
      default_model: "codellama"
Usage
Start the service:

bash
python -m core.service
Access the web interface at http://localhost:8000 or use the API:

python
from client import OpenLLMClient
client = OpenLLMClient()
response = client.query("How to reverse a list in Python?")


TO DO

Core Improvements

- Implement cross-provider benchmark testing
- Add automatic failover when providers go offline
- Develop prompt versioning system

Performance

- Optimize knowledge graph queries
- Add GPU memory monitoring for vLLM
- Implement query result compression

New Features

- Add multi-modal support (images + code)
- Develop VS Code extension
- Create collaboration history replay

Documentation

- Write API usage examples
- Create architecture diagrams
- Add developer onboarding guide

Maintenance

- Upgrade to Pydantic v2
- Add integration test suite
- Set up CI/CD pipeline

Contributing

- Fork the repository
- Create a feature branch (git checkout -b feature/your-feature)
- Commit your changes
- Push to the branch
- Open a pull request

License
MIT License - See LICENSE for details.


=== requirements.txt ===
torch>=2.0.1
transformers>=4.30.0
accelerate>=0.20.0
datasets>=2.12.0
peft>=0.4.0
wandb>=0.15.0
fastapi>=0.95.0
uvicorn>=0.22.0


=== shared\config\init.py ===
import yaml
from pathlib import Path
from typing import Any, Dict

class ConfigManager:
    _config: Dict[str, Any] = {}
    
    @classmethod
    def load_configs(cls, config_dir: str = "shared/config"):
        config_path = Path(config_dir)
        
        for config_file in config_path.glob("*.yaml"):
            with open(config_file) as f:
                cls._config[config_file.stem] = yaml.safe_load(f)
                
    @classmethod
    def get(cls, key: str, default: Any = None) -> Any:
        keys = key.split(".")
        value = cls._config
        
        for k in keys:
            value = value.get(k)
            if value is None:
                return default
                
        return value


=== shared\config\loader.py ===
# shared/config/loader.py
import watchdog.events
import yaml

class ConfigWatcher(watchdog.events.FileSystemEventHandler):
    def __init__(self, callback):
        self.callback = callback
        
    def on_modified(self, event):
        if event.src_path.endswith('.yaml'):
            self.callback(event.src_path)

# Enhanced ConfigManager
class ConfigManager:
    def __init__(self):
        self._callbacks = []
        self.load_configs()
        
    def register_callback(self, callback):
        self._callbacks.append(callback)
        
    def _notify_changes(self, changed_file):
        for callback in self._callbacks:
            callback(self, changed_file)
    
    def load_config():
    """Safe config reload that preserves existing connections"""
    new_config = yaml.safe_load(open("configs/integration.yaml"))
    for key in current_config:
        if key in new_config:
            current_config[key].update(new_config[key])


=== shared\knowledge\graph.py ===
from dataclasses import dataclass
from typing import Dict, List, Set, Optional
import networkx as nx
from enum import Enum
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
import spacy

class EntityType(Enum):
    CONCEPT = "concept"
    CODE = "code"
    API = "api"
    LIBRARY = "library"
    ERROR = "error"
    PATTERN = "pattern"

@dataclass
class KnowledgeNode:
    id: str
    type: EntityType
    content: str
    metadata: dict
    embeddings: Optional[np.ndarray] = None

class KnowledgeGraph:
    def __init__(self):
        self.graph = nx.MultiDiGraph()
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.nlp = spacy.load("en_core_web_sm")
        self._setup_indices()
        
    def _setup_indices(self):
        """Initialize data structures for efficient lookup"""
        self.content_index = {}  # content -> node_id
        self.embedding_index = []  # List of (node_id, embedding)
        
    def _generate_id(self, content: str, type: EntityType) -> str:
        """Create deterministic node ID"""
        return hashlib.sha256(f"{type.value}:{content}".encode()).hexdigest()
        
    def add_entity(self, content: str, type: EntityType, metadata: dict = None) -> str:
        """Add or update an entity with enhanced NLP processing"""
        # Preprocess content
        doc = self.nlp(content)
        normalized_content = " ".join([token.lemma_ for token in doc if not token.is_stop])
        
        node_id = self._generate_id(normalized_content, type)
        embedding = self.encoder.encode(normalized_content)
        
        if node_id not in self.graph:
            self.graph.add_node(node_id, 
                type=type,
                content=content,
                normalized=normalized_content,
                metadata=metadata or {},
                embedding=embedding
            )
            self.content_index[normalized_content] = node_id
            self.embedding_index.append((node_id, embedding))
        else:
            # Update existing node
            self.graph.nodes[node_id]['metadata'].update(metadata or {})
            
        return node_id
        
    def add_relation(self, source_id: str, target_id: str, relation_type: str, weight: float = 1.0):
        """Create weighted relationship between entities"""
        if source_id in self.graph and target_id in self.graph:
            self.graph.add_edge(source_id, target_id, 
                type=relation_type,
                weight=weight
            )
            
    def find_semantic_matches(self, query: str, threshold: float = 0.7) -> List[dict]:
        """Find knowledge nodes semantically similar to query"""
        query_embedding = self.encoder.encode(query)
        matches = []
        
        for node_id, emb in self.embedding_index:
            similarity = np.dot(query_embedding, emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(emb)
            )
            if similarity > threshold:
                matches.append({
                    "node_id": node_id,
                    "similarity": float(similarity),
                    **self.graph.nodes[node_id]
                })
                
        return sorted(matches, key=lambda x: x["similarity"], reverse=True)
        
    def expand_from_text(self, text: str, source: str = "user"):
        """Automatically extract and add knowledge from text"""
        doc = self.nlp(text)
        
        # Extract entities and noun phrases
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        noun_chunks = [(chunk.text, "NOUN_PHRASE") for chunk in doc.noun_chunks]
        
        # Add to knowledge graph
        nodes = []
        for content, label in entities + noun_chunks:
            node_id = self.add_entity(
                content=content,
                type=self._map_spacy_label(label),
                metadata={"source": source}
            )
            nodes.append(node_id)
            
        # Create relationships based on syntactic dependencies
        for sent in doc.sents:
            for token in sent:
                if token.dep_ in ("dobj", "nsubj", "attr"):
                    source = self._get_node_for_token(token.head)
                    target = self._get_node_for_token(token)
                    if source and target:
                        self.add_relation(source, target, token.dep_)
    
    def _map_spacy_label(self, label: str) -> EntityType:
        """Map Spacy labels to our entity types"""
        mapping = {
            "PERSON": "concept",
            "ORG": "concept",
            "GPE": "concept",
            "PRODUCT": "api",
            "NOUN_PHRASE": "concept"
        }
        return EntityType(mapping.get(label, "concept"))
        
    def _get_node_for_token(self, token) -> Optional[str]:
        """Find or create node for a Spacy token"""
        text = token.lemma_
        return self.content_index.get(text)
        
    def get_statistics(self) -> dict:
        """Return comprehensive graph statistics"""
        centrality = nx.degree_centrality(self.graph)
        top_nodes = sorted(
            [(n, c) for n, c in centrality.items()],
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            "basic": {
                "nodes": len(self.graph.nodes()),
                "edges": len(self.graph.edges()),
                "components": nx.number_weakly_connected_components(self.graph)
            },
            "centrality": {
                "top_concepts": [
                    {"id": n[0], "content": self.graph.nodes[n[0]]["content"], "score": n[1]}
                    for n in top_nodes
                ]
            },
            "types": {
                nt: sum(1 for n in self.graph.nodes() if self.graph.nodes[n]["type"] == nt)
                for nt in set(nx.get_node_attributes(self.graph, "type").values())
            }
        }

    def export_gexf(self, path: str):
        """Export graph to GEXF format for external tools"""
        nx.write_gexf(self.graph, path)
        
    def find_code_patterns(self, pattern_type: str) -> list:
        """Enhanced pattern matching for common code structures"""
        return self._query_graph(
            f"""
            MATCH (n:CodePattern {{type: $pattern_type}})
            RETURN n
            """,
            {"pattern_type": pattern_type}
        )
    
    def cache_solution(self, problem_hash: str, solution: Dict):
        """Store successful solutions for future reuse"""
        self._store_node("Solution", {
            "hash": problem_hash,
            "solution": solution
        })
        
    def find_similar(self, code_snippet: str, threshold: float = 0.8):
        """Find similar code patterns using vector similarity"""
        query_embedding = self.embedder.encode(code_snippet)
        results = []
        
        for node in self.graph.nodes(data=True):
            if 'embedding' in node[1]:
                similarity = cosine_similarity(
                    query_embedding,
                    node[1]['embedding']
                )
                if similarity > threshold:
                    results.append({
                        'node': node[0],
                        'similarity': similarity,
                        'solution': node[1].get('solution')
                    })
        
        return sorted(results, key=lambda x: x['similarity'], reverse=True)

    def cache_solution(self, problem: str, solution: str):
        """Store successful solutions with embeddings"""
        embedding = self.embedder.encode(problem)
        self.graph.add_node(problem, solution=solution, embedding=embedding)



=== shared\schemas.py ===
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional

class CompletionRequest(BaseModel):
    context: str  # Full file content
    cursor_context: str  # Line/fragment near cursor
    
class SignatureHelp(BaseModel):
    name: str
    parameters: List[Dict[str, str]]
    active_parameter: int

class SignatureRequest(BaseModel):
    code: str
    language: str
    cursor_pos: int
    
class HealthStatus(BaseModel):
    service: str
    status: Literal["online", "degraded", "offline"]
    models: List[str] = []
    latency: Optional[float]

class IntegrationConfig(BaseModel):
    priority: int
    timeout: int = 30
    
class Query(BaseModel):
    """Enhanced query class with routing support"""
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    # Add these new methods
    def with_additional_context(self, reasoning_data: Dict) -> 'Query':
        """
        Create a new query instance with reasoning context
        Usage:
            enriched_query = original_query.with_additional_context(
                {"source": "graph", "confidence": 0.9}
            )
        """
        return self.copy(
            update={
                "metadata": {
                    **self.metadata,
                    "reasoning": reasoning_data
                }
            }
        )
    
    @property
    def preferred_provider(self) -> Optional[str]:
        """Get preferred LLM provider if specified"""
        return self.metadata.get('preferred_provider')
    
    @preferred_provider.setter
    def preferred_provider(self, provider: str):
        """Set preferred LLM provider"""
        self.metadata['preferred_provider'] = provider
        
class FeedbackRating(BaseModel):
    query_hash: str
    response_hash: str
    rating: float = Field(..., ge=0, le=5)
    comment: Optional[str]
    
class FeedbackCorrection(BaseModel):
    node_id: str
    corrected_content: str
    severity: Literal["low", "medium", "high"] = "medium"


=== static\css\debugger.css ===
.debug-panel {
    border: 1px solid #e1e4e8;
    border-radius: 6px;
    padding: 16px;
    margin-top: 20px;
    background: #f6f8fa;
}

.debug-frame {
    margin-bottom: 20px;
    padding: 15px;
    background: white;
    border-radius: 4px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.debug-frame pre {
    background: #f0f0f0;
    padding: 8px;
    border-radius: 3px;
    overflow-x: auto;
}

.var {
    display: inline-block;
    background: #e1f5fe;
    padding: 2px 6px;
    border-radius: 3px;
    margin-right: 5px;
    font-family: monospace;
}

.suggestions ul {
    margin: 5px 0 0 20px;
    padding: 0;
}

.suggestions li {
    margin: 5px 0;
}


=== static\css\graph.css ===
/* Layout */
#graph-explorer-container {
    width: 100%;
    height: 800px;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.controls {
    padding: 12px;
    background: #f5f5f5;
    margin-bottom: 12px;
    border-radius: 4px;
    display: flex;
    gap: 12px;
    align-items: center;
}

.controls button {
    padding: 6px 12px;
    background-color: #2962FF;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    font-size: 14px;
}

.controls button:hover {
    background-color: #1E4DCC;
}

.controls label {
    display: flex;
    align-items: center;
    gap: 6px;
    font-size: 14px;
}

/* Graph Canvas */
#graph-canvas {
    width: 100%;
    height: calc(100% - 120px);
    border: 1px solid #ddd;
    border-radius: 4px;
    background: white;
}

/* Graph Elements */
.graph-node {
    cursor: pointer;
    stroke: white;
    stroke-width: 1.5px;
}

.graph-node:hover {
    stroke-width: 2px;
}

.graph-link {
    stroke: #888;
    stroke-opacity: 0.6;
}

.node-label {
    pointer-events: none;
    user-select: none;
    font-family: 'Segoe UI', sans-serif;
}

/* Analytics Panel */
#graph-analytics {
    margin-top: 12px;
    padding: 12px;
    background: #f5f5f5;
    border-radius: 4px;
}

#graph-analytics h3 {
    margin: 0 0 8px 0;
    color: #2962FF;
    font-size: 16px;
}

.metric {
    margin-bottom: 8px;
}

.metric-label {
    font-weight: bold;
    color: #FF6D00;
    margin-right: 8px;
}

.metric-values {
    display: inline-flex;
    gap: 12px;
}


=== static\css\signature.css ===
.signature-tooltip {
    position: fixed;
    background: #2d2d2d;
    color: white;
    padding: 8px 12px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 14px;
    z-index: 1000;
    box-shadow: 0 2px 8px rgba(0,0,0,0.2);
    max-width: 500px;
}

.signature-title {
    color: #569cd6;
}

.active-param {
    color: #9cdcfe;
    font-weight: bold;
}

.signature-tooltip span {
    margin: 0 2px;
}


=== static\js\completion.js ===
class CompletionUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.setupListeners();
    }

    setupListeners() {
        this.editor.addEventListener("keydown", async (e) => {
            if (e.key === "Tab" || (e.key === " " && e.ctrlKey)) {
                e.preventDefault();
                const completions = await this.fetchCompletions();
                this.showCompletions(completions);
            }
        });
    }

    async fetchCompletions() {
        const response = await fetch("/completion", {
            method: "POST",
            body: JSON.stringify({
                content: this.getCursorContext(),
                context: {
                    code: this.editor.value,
                    language: "python"  # Dynamic in real impl
                }
            })
        });
        return await response.json();
    }

    getCursorContext() {
        const cursorPos = this.editor.selectionStart;
        return this.editor.value.substring(
            Math.max(0, cursorPos - 50), 
            cursorPos
        );
    }

    showCompletions(completions) {
        // Render as dropdown or inline suggestions
        console.log("Suggestions:", completions);
    }
}


=== static\js\debugger.js ===
class DebuggerUI {
    constructor() {
        this.container = document.getElementById('debug-container');
        this.setupUI();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="debug-panel">
                <h3>Debug Assistant</h3>
                <div class="debug-frames"></div>
                <button id="analyze-btn">Analyze Error</button>
            </div>
        `;
        
        document.getElementById('analyze-btn').addEventListener('click', () => this.analyzeError());
    }

    async analyzeError() {
        const code = document.getElementById('code-editor').value;
        const error = document.getElementById('error-output').value;
        
        const response = await fetch('/debug', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                content: "debug_request",
                context: { code, error }
            })
        });
        
        const result = await response.json();
        this.displayResults(result);
    }

    displayResults(debugData) {
        const framesContainer = document.querySelector('.debug-frames');
        framesContainer.innerHTML = debugData.metadata.frames.map(frame => `
            <div class="debug-frame">
                <h4>${frame.file}:${frame.line}</h4>
                <pre>${frame.context}</pre>
                <div class="variables">${this.formatVariables(frame.variables)}</div>
                ${this.formatSuggestions(debugData.metadata.suggestions[frame.line] || [])}
            </div>
        `).join('');
    }

    formatVariables(vars) {
        return Object.entries(vars).map(([k, v]) => 
            `<span class="var">${k}=${v}</span>`
        ).join(' ');
    }

    formatSuggestions(suggestions) {
        if (!suggestions.length) return '';
        return `
            <div class="suggestions">
                <h5>Suggestions:</h5>
                <ul>${suggestions.map(s => `<li>${s}</li>`).join('')}</ul>
            </div>
        `;
    }
}


=== static\js\graph-explorer.js ===
// Configuration
const CONFIG = {
    nodeColors: {
        concept: '#FF6D00',
        code: '#2962FF',
        api: '#00C853',
        error: '#D50000',
        default: '#666666'
    },
    apiBaseUrl: '/knowledge/graph'
};

class GraphExplorer {
    constructor(containerId) {
        this.container = document.getElementById(containerId);
        this.simulation = null;
        this.currentGraph = { nodes: [], edges: [] };
        this.init();
    }

    async init() {
        this.setupUI();
        await this.loadGraph();
        this.setupEventListeners();
    }

    setupUI() {
        this.container.innerHTML = `
            <div class="controls">
                <button id="refresh-graph">Refresh</button>
                <label>
                    Depth: <input type="range" id="graph-depth" min="1" max="3" value="2">
                </label>
                <label>
                    Physics: <input type="checkbox" id="toggle-physics" checked>
                </label>
                <button id="analyze-btn">Run Analysis</button>
            </div>
            <div id="graph-canvas"></div>
            <div id="graph-analytics"></div>
        `;
    }

    async loadGraph(depth = 2, physics = true) {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/json?depth=${depth}`);
            this.currentGraph = await response.json();
            this.renderGraph(physics);
        } catch (error) {
            console.error('Failed to load graph:', error);
        }
    }

    renderGraph(enablePhysics) {
        const canvas = document.getElementById('graph-canvas');
        canvas.innerHTML = '';
        
        const width = canvas.clientWidth;
        const height = canvas.clientHeight;
        
        const svg = d3.select(canvas)
            .append('svg')
            .attr('width', width)
            .attr('height', height);
        
        // Create simulation
        this.simulation = d3.forceSimulation()
            .force('link', d3.forceLink().id(d => d.id))
            .force('charge', d3.forceManyBody().strength(-100))
            .force('center', d3.forceCenter(width / 2, height / 2));
        
        // Draw links
        const link = svg.append('g')
            .selectAll('line')
            .data(this.currentGraph.edges)
            .enter().append('line')
            .attr('class', 'graph-link')
            .attr('stroke-width', d => Math.sqrt(d.value));
        
        // Draw nodes
        const node = svg.append('g')
            .selectAll('circle')
            .data(this.currentGraph.nodes)
            .enter().append('circle')
            .attr('class', 'graph-node')
            .attr('r', 8)
            .attr('fill', d => CONFIG.nodeColors[d.type] || CONFIG.nodeColors.default)
            .call(d3.drag()
                .on('start', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0.3).restart();
                    d.fx = d.x;
                    d.fy = d.y;
                })
                .on('drag', (event, d) => {
                    d.fx = event.x;
                    d.fy = event.y;
                })
                .on('end', (event, d) => {
                    if (!event.active) this.simulation.alphaTarget(0);
                    d.fx = null;
                    d.fy = null;
                }));
        
        // Add labels
        const label = svg.append('g')
            .selectAll('text')
            .data(this.currentGraph.nodes)
            .enter().append('text')
            .attr('class', 'node-label')
            .text(d => d.label)
            .attr('font-size', 10)
            .attr('dx', 10)
            .attr('dy', 4);
        
        // Update positions
        this.simulation.nodes(this.currentGraph.nodes)
            .on('tick', () => {
                link.attr('x1', d => d.source.x)
                    .attr('y1', d => d.source.y)
                    .attr('x2', d => d.target.x)
                    .attr('y2', d => d.target.y);
                
                node.attr('cx', d => d.x)
                    .attr('cy', d => d.y);
                
                label.attr('x', d => d.x)
                    .attr('y', d => d.y);
            });
        
        this.simulation.force('link')
            .links(this.currentGraph.edges);
        
        if (!enablePhysics) {
            this.simulation.stop();
        }
    }

    setupEventListeners() {
        document.getElementById('refresh-graph').addEventListener('click', () => {
            const depth = document.getElementById('graph-depth').value;
            const physics = document.getElementById('toggle-physics').checked;
            this.loadGraph(depth, physics);
        });

        document.getElementById('analyze-btn').addEventListener('click', () => {
            this.runAnalytics();
        });
    }

    async runAnalytics() {
        try {
            const response = await fetch(`${CONFIG.apiBaseUrl}/analytics`);
            const analytics = await response.json();
            this.displayAnalytics(analytics);
        } catch (error) {
            console.error('Failed to run analytics:', error);
        }
    }

    displayAnalytics(analytics) {
        const panel = document.getElementById('graph-analytics');
        panel.innerHTML = `
            <h3>Graph Analytics</h3>
            <div class="metric">
                <span class="metric-label">Central Nodes:</span>
                <div class="metric-values">
                    ${analytics.centrality.map(n => `
                        <div>${n.label} (${n.value.toFixed(3)})</div>
                    `).join('')}
                </div>
            </div>
            <div class="metric">
                <span class="metric-label">Communities:</span>
                <div>${analytics.community.count} detected</div>
            </div>
        `;
    }
	
	highlightNode(nodeId, clientId) {
        const color = this.getClientColor(clientId);
        d3.select(`circle[data-id="${nodeId}"]`)
            .transition()
            .attr("stroke", color)
            .attr("stroke-width", 3);
    }
    
    getClientColor(clientId) {
        // Simple deterministic color assignment
        const colors = ["#FF00FF", "#00FFFF", "#FFFF00", "#FF9900"];
        const index = parseInt(clientId.split("-")[1]) % colors.length;
        return colors[index];
    }
    
    refreshNode(nodeId) {
        // Refresh node visualization
        this.loadGraph(this.currentDepth, true);
    }
	
	renderHistoricalGraph(graphData) {
        // Clear current graph
        d3.select("#graph-canvas").selectAll("*").remove();
        
        // Render historical version
        this.currentGraph = graphData;
        this.renderGraph(false); // Disable physics for historical views
        
        // Visual indication
        d3.select("#graph-canvas")
            .append("rect")
            .attr("width", "100%")
            .attr("height", "100%")
            .attr("fill", "rgba(0,0,0,0.1)")
            .attr("class", "historical-overlay");
    }
	
}

// Initialize when loaded
window.addEventListener('DOMContentLoaded', () => {
    new GraphExplorer('graph-explorer-container');
});


=== static\js\signature.js ===
class SignatureUI {
    constructor(editorElementId) {
        this.editor = document.getElementById(editorElementId);
        this.tooltip = this._createTooltip();
        this._setupListeners();
    }

    _createTooltip() {
        const tooltip = document.createElement('div');
        tooltip.className = 'signature-tooltip';
        tooltip.style.display = 'none';
        document.body.appendChild(tooltip);
        return tooltip;
    }

    _setupListeners() {
        this.editor.addEventListener('mousemove', this._debounce(async (e) => {
            const pos = this._getCursorPosition(e);
            const signature = await this._fetchSignature(pos);
            if (signature) this._showSignature(signature);
        }, 300));
    }

    async _fetchSignature(cursorPos) {
        const response = await fetch('/signature', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                content: this.editor.value,
                context: {
                    code: this.editor.value,
                    language: 'python',
                    cursor_pos: cursorPos
                }
            })
        });
        return await response.json();
    }

    _showSignature(data) {
        if (!data.name) {
            this.tooltip.style.display = 'none';
            return;
        }

        const params = data.parameters.map((p, i) => 
            `<span class="${i === data.active_parameter ? 'active-param' : ''}">
                ${p.type ? `${p.type} ` : ''}${p.name}
            </span>`
        ).join(', ');

        this.tooltip.innerHTML = `
            <div class="signature-title">${data.name}(${params})</div>
        `;
        this._positionTooltip();
        this.tooltip.style.display = 'block';
    }

    _positionTooltip() {
        // Position near cursor (simplified)
        const rect = this.editor.getBoundingClientRect();
        this.tooltip.style.left = `${rect.left + 20}px`;
        this.tooltip.style.top = `${rect.top - 40}px`;
    }

    _debounce(func, delay) {
        let timeout;
        return (...args) => {
            clearTimeout(timeout);
            timeout = setTimeout(() => func.apply(this, args), delay);
        };
    }
}


=== templates\index.html ===
<!DOCTYPE html>
<html>
<head>
    <title>Knowledge Graph Explorer</title>
    <link rel="stylesheet" href="/static/css/graph.css">
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
    <div id="graph-explorer-container"></div>
    <script src="/static/js/graph-explorer.js"></script>
</body>
</html>


=== webpack.config.js ===
const path = require('path');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');

module.exports = {
    entry: {
        app: './static/ts/graph-explorer.ts',
        styles: './static/scss/main.scss'
    },
    output: {
        filename: 'js/[name].bundle.js',
        path: path.resolve(__dirname, 'static/dist')
    },
    resolve: {
        extensions: ['.ts', '.js', '.scss']
    },
    module: {
        rules: [
            {
                test: /\.ts$/,
                use: 'ts-loader',
                exclude: /node_modules/
            },
            {
                test: /\.scss$/,
                use: [
                    MiniCssExtractPlugin.loader,
                    'css-loader',
                    'sass-loader'
                ]
            }
        ]
    },
    plugins: [
        new MiniCssExtractPlugin({
            filename: 'css/[name].css'
        })
    ],
    externals: {
        d3: 'd3'
    }
};


