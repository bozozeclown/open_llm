# =============================================================================
# Model Configuration
# =============================================================================
models:
  # Default model settings
  default_model: "codellama"
  
  # Model paths and directories
  paths:
    models_dir: "./models"
    cache_dir: "./models/cache"
    downloads_dir: "./models/downloads"
  
  # Model-specific configurations
  configurations:
    codellama:
      name: "CodeLlama"
      variants:
        - name: "CodeLlama-7b"
          path: "codellama/CodeLlama-7b-hf"
          quantization: false
          device: "auto"
          max_length: 2048
          temperature: 0.7
          top_p: 0.9
          repetition_penalty: 1.1
        
        - name: "CodeLlama-13b"
          path: "codellama/CodeLlama-13b-hf"
          quantization: false
          device: "auto"
          max_length: 2048
          temperature: 0.7
          top_p: 0.9
          repetition_penalty: 1.1
        
        - name: "CodeLlama-34b"
          path: "codellama/CodeLlama-34b-hf"
          quantization: true
          device: "auto"
          max_length: 2048
          temperature: 0.7
          top_p: 0.9
          repetition_penalty: 1.1
    
    gpt:
      name: "GPT"
      variants:
        - name: "gpt-4"
          api_provider: "openai"
          max_tokens: 4096
          temperature: 0.7
          top_p: 1.0
        
        - name: "gpt-3.5-turbo"
          api_provider: "openai"
          max_tokens: 4096
          temperature: 0.7
          top_p: 1.0
    
    claude:
      name: "Claude"
      variants:
        - name: "claude-2"
          api_provider: "anthropic"
          max_tokens: 4096
          temperature: 0.7
          top_p: 1.0
        
        - name: "claude-instant"
          api_provider: "anthropic"
          max_tokens: 4096
          temperature: 0.7
          top_p: 1.0
  
  # Model selection strategy
  selection:
    strategy: "priority"  # priority, round_robin, load_balanced
    fallback_enabled: true
    health_check_interval: 60
  
  # Model caching settings
  caching:
    enabled: true
    max_cache_size: "10GB"
    cache_ttl: 3600  # seconds
    compression: true
  
  # Model download settings
  downloads:
    concurrent_downloads: 3
    retry_attempts: 3
    timeout: 300  # seconds
    verify_checksums: true
  
  # Model performance settings
  performance:
    batch_size: 1
    max_batch_size: 8
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_total_tokens: 4096
  
  # Model security settings
  security:
    allow_internet_access: false
    sandbox_enabled: true
    max_execution_time: 30  # seconds
    allowed_operations: ["inference", "tokenization"]