# =============================================================================
# LLM Provider Integration Configuration
# =============================================================================
plugins:
  # Ollama (Local Models)
  ollama:
    enabled: true
    config:
      base_url: "http://localhost:11434"
      default_model: "codellama"
      timeout: 30
      batch_size: 1

  # vLLM (High-Performance Inference)
  vllm:
    enabled: true
    config:
      model: "codellama/CodeLlama-7b-hf"
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.9
      max_batch_size: 2048

  # HuggingFace Transformers
  huggingface:
    enabled: false  # Disabled by default due to resource requirements
    config:
      api_key: "${HF_API_KEY}"
      model_name: "codellama/CodeLlama-7b-hf"
      device: "auto"
      quantize: false
      batch_size: 2

  # Text Generation WebUI
  textgen:
    enabled: true
    config:
      base_url: "http://localhost:5000"
      api_key: "${TEXTGEN_API_KEY}"
      batch_size: 4
      timeout: 45

  # Grok AI
  grok:
    enabled: true
    config:
      api_key: "${GROQ_API_KEY}"
      rate_limit: 5
      timeout: 15

  # LM Studio
  lmstudio:
    enabled: false  # Disabled by default
    config:
      base_url: "http://localhost:1234"
      timeout: 60
      batch_support: false

# Global integration settings
settings:
  default_timeout: 30
  priority_order:
    - "vllm"
    - "ollama" 
    - "grok"
    - "huggingface"
    - "textgen"
    - "lmstudio"

# Batch processing settings
batching:
  enabled: true
  max_batch_size: 8
  max_wait_ms: 50

# Monitoring settings
health_check_interval: 60

# Load balancing settings
load_balancing:
  update_interval: 10
  min_requests: 20
  priority_bump: 2.0