# Open LLM Code Assistant

An AI-powered coding assistant with hybrid reasoning, self-learning capabilities, multi-LLM orchestration, and comprehensive enterprise features.

## ‚ú® Features

### Core Architecture
- **Hybrid Reasoning Engine** - Combines rule-based patterns, knowledge graphs, and LLMs
- **Multi-LLM Integration** - Supports Ollama, vLLM, HuggingFace, Grok, and more
- **Adaptive Routing** - Dynamic load balancing and SLA-aware prioritization
- **Self-Learning** - Improves from user feedback and corrections
- **Quality Gates** - Automated validation of all responses
- **Predictive Caching** - Anticipates and pre-computes likely queries

### Advanced Capabilities
- **Multi-Modal Code Analysis** - Extract and analyze code from images/screenshots
- **Advanced Refactoring Engine** - Intelligent code improvement suggestions
- **Real-time Collaboration** - Live coding sessions with multiple users
- **VS Code Extension** - Seamless IDE integration
- **Comprehensive Analytics Dashboard** - Real-time metrics and insights
- **ML Model Management** - Automated model updates and versioning
- **Knowledge Graph Versioning** - Track and restore knowledge graph states

### Offline & Voice Support
- **Offline Mode** - Cache responses for use without internet connectivity
- **Voice Commands** - Natural language interaction with wake word detection
- **CLI Tool** - Command-line interface for all major features

### Enterprise Features
- **SSO Integration** - OAuth2 (Google, Microsoft, GitHub) and SAML 2.0 support
- **Team Management** - Role-based permissions, member invitation, resource sharing
- **Audit Logging** - Comprehensive compliance tracking with searchable audit trails
- **Enterprise Deployment** - Production-ready with high availability and monitoring

### Security & Reliability
- **Authentication & Authorization** - API key-based access and JWT tokens
- **Rate Limiting** - Advanced throttling with multiple strategies
- **Circuit Breakers** - Resilient error handling and failover
- **Health Monitoring** - Comprehensive system health checks
- **Performance Optimization** - Database optimization and caching

## üöÄ Installation

### Prerequisites
- Python 3.8+
- Redis (for caching)
- PostgreSQL (for analytics and enterprise features)
- Docker (optional, for containerized deployment)
- GPU (optional, for optimal performance with local models)

### Quick Start
```bash
git clone https://github.com/bozozeclown/open_llm.git
cd open_llm
pip install -r requirements.txt
```

### Configuration
1. Copy example configuration:
```bash
cp configs/integration.example.yaml configs/integration.yaml
```

2. Edit `configs/integration.yaml` to enable your preferred LLM providers:
```yaml
plugins:
  ollama:
    enabled: true
    config:
      base_url: "http://localhost:11434"
      default_model: "codellama"
  vllm:
    enabled: true
    config:
      model: "codellama/CodeLlama-7b-hf"
```

3. Set environment variables:
```bash
# API Keys
export GROQ_API_KEY="your_groq_api_key"
export HF_API_KEY="your_huggingface_api_key"
export TEXTGEN_API_KEY="your_textgen_api_key"

# Database
export DATABASE_URL="postgresql://user:password@localhost:5432/openllm"
export REDIS_URL="redis://localhost:6379"

# Security
export SECRET_KEY="your_secret_key_here"
export JWT_SECRET="your_jwt_secret_here"

# Enterprise Features (optional)
export ENTERPRISE_ENABLED="true"
export SAML_IDP_METADATA_URL="your_idp_metadata_url"
```

### Docker Deployment
```bash
# Standard deployment
docker-compose up -d

# Enterprise deployment with all features
cd deploy/enterprise
docker-compose up -d

# Access the application
# Web Interface: http://localhost:8000
# Analytics Dashboard: http://localhost:8000/analytics/dashboard
# Grafana: http://localhost:3000
# Kibana: http://localhost:5601
```

## üìñ Usage

### Web Interface
Start the service:
```bash
python -m core.service
```

Access the web interface at `http://localhost:8000`

### CLI Tool
Install the CLI tool:
```bash
pip install -e .
```

Usage examples:
```bash
# Ask coding questions
openllm query "How to reverse a list in Python?"

# Analyze code files
openllm analyze -f my_code.py --language python --type refactor

# Create collaboration sessions
openllm session "My Session" --code "print('Hello World')" --language python

# Manage knowledge graph versions
openllm version create "Added optimization patterns"
openllm version list
```

### API Usage
```python
from client import OpenLLMClient

client = OpenLLMClient()

# Basic code completion
response = client.query("How to reverse a list in Python?")
print(response.content)

# Code refactoring suggestions
suggestions = client.analyze_refactoring("your_code_here", "python")

# Multi-modal analysis
analysis = client.analyze_image("path/to/code/image.png")

# Real-time collaboration
session = client.create_session("My Coding Session", "print('Hello World')", "python")

# Knowledge graph versioning
version_id = client.create_version("Initial knowledge state")
restored = client.restore_version(version_id)
```

### VS Code Extension
1. Install the Open LLM Code Assistant extension from the VS Code marketplace
2. Configure your API endpoint in VS Code settings:
```json
{
  "open-llm.apiUrl": "http://localhost:8000",
  "open-llm.apiKey": "your_api_key"
}
```

### Voice Commands
Enable voice interaction:
```bash
# Start voice listening
curl -X POST http://localhost:8000/voice/command

# Say "Hey assistant, how do I reverse a list in Python?"
# The system will respond with voice and process your query

# Stop voice listening
curl -X POST http://localhost:8000/voice/stop
```

### Offline Mode
The system automatically caches responses for offline use:
```python
# Works without internet connection using cached responses
response = client.query("How to reverse a list in Python?")  # Returns cached response
```

## üìä Analytics Dashboard

Access the comprehensive analytics dashboard at `http://localhost:8000/analytics/dashboard` to monitor:
- **Usage Statistics**: Request trends, active users, success rates
- **Performance Metrics**: Response times, latency distribution
- **User Analytics**: Activity patterns, top users
- **Code Quality Trends**: Language distribution, refactoring patterns
- **Enterprise Metrics**: Team activities, audit logs, compliance tracking

## üîß Configuration

### Environment Variables
```bash
# API Keys
GROQ_API_KEY="your_groq_api_key"
HF_API_KEY="your_huggingface_api_key"
TEXTGEN_API_KEY="your_textgen_api_key"

# Database
DATABASE_URL="postgresql://user:password@localhost:5432/openllm"
REDIS_URL="redis://localhost:6379"

# Security
SECRET_KEY="your_secret_key_here"
JWT_SECRET="your_jwt_secret_here"

# Monitoring
PROMETHEUS_ENABLED=true
GRAFANA_ENABLED=true

# Enterprise Features
ENTERPRISE_ENABLED="true"
SAML_IDP_METADATA_URL="your_idp_metadata_url"
SP_ENTITY_ID="your_sp_entity_id"
SP_KEY_FILE="/path/to/sp_key.pem"
SP_CERT_FILE="/path/to/sp_cert.pem"
```

### Model Management
```python
from core.ml.model_manager import ModelManager

manager = ModelManager()

# Download and load models
await manager.download_model(ModelType.MULTIMODAL)
await manager.load_model(ModelType.MULTIMODAL)

# Check model status
model_info = manager.get_model_info(ModelType.MULTIMODAL)
print(f"Model status: {model_info.status}")
```

### Enterprise Configuration
```python
# Configure SSO providers
enterprise_config = {
    "oauth": {
        "google": {
            "enabled": true,
            "client_id": "your_google_client_id",
            "client_secret": "your_google_client_secret",
            "user_info_url": "https://www.googleapis.com/oauth2/v2/userinfo",
            "scopes": ["openid", "email", "profile"]
        },
        "microsoft": {
            "enabled": true,
            "client_id": "your_microsoft_client_id",
            "client_secret": "your_microsoft_client_secret",
            "user_info_url": "https://graph.microsoft.com/v1.0/me",
            "scopes": ["openid", "email", "profile"]
        }
    },
    "saml": {
        "enabled": true,
        "sp_entity_id": "https://your-domain.com/metadata",
        "acs_url": "https://your-domain.com/saml/acs",
        "idp_metadata_url": "https://your-idp.com/metadata",
        "sp_key_file": "/path/to/sp_key.pem",
        "sp_cert_file": "/path/to/sp_cert.pem"
    }
}
```

## üõ†Ô∏è Development

### Project Structure
```
open_llm/
‚îú‚îÄ‚îÄ configs/                    # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ base.yaml              # Base project configuration
‚îÇ   ‚îú‚îÄ‚îÄ integration.yaml        # LLM provider integrations
‚îÇ   ‚îú‚îÄ‚îÄ model.yaml             # Model management settings
‚îÇ   ‚îú‚îÄ‚îÄ predictions.yaml       # Prediction caching settings
‚îÇ   ‚îî‚îÄ‚îÄ sla_tiers.yaml        # Service level agreements
‚îú‚îÄ‚îÄ core/                      # Core application logic
‚îÇ   ‚îú‚îÄ‚îÄ analysis/              # Code analysis components
‚îÇ   ‚îú‚îÄ‚îÄ analytics/             # Analytics dashboard
‚îÇ   ‚îú‚îÄ‚îÄ collaboration/        # Real-time collaboration
‚îÇ   ‚îú‚îÄ‚îÄ completion/            # Code completion
‚îÇ   ‚îú‚îÄ‚îÄ context.py             # Context and knowledge management
‚îÇ   ‚îú‚îÄ‚îÄ database/             # Database management
‚îÇ   ‚îú‚îÄ‚îÄ debugging/            # Debugging tools
‚îÇ   ‚îú‚îÄ‚îÄ enterprise/            # Enterprise features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/             # Authentication (SSO, SAML)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ teams/            # Team management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audit/            # Audit logging
‚îÇ   ‚îú‚îÄ‚îÄ errors/               # Error handling
‚îÇ   ‚îú‚îÄ‚îÄ feedback/             # User feedback processing
‚îÇ   ‚îú‚îÄ‚îÄ health.py             # Health monitoring
‚îÇ   ‚îú‚îÄ‚îÄ integrations/         # LLM provider integrations
‚îÇ   ‚îú‚îÄ‚îÄ interface.py          # API interface
‚îÇ   ‚îú‚îÄ‚îÄ ml/                   # Machine learning
‚îÇ   ‚îú‚îÄ‚îÄ multimodal/           # Multi-modal analysis
‚îÇ   ‚îú‚îÄ‚îÄ offline/              # Offline support
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/           # Performance monitoring
‚îÇ   ‚îú‚îÄ‚îÄ orchestration/        # Request orchestration
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py       # Main orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ performance/          # Performance optimization
‚îÇ   ‚îú‚îÄ‚îÄ personalization/      # User personalization
‚îÇ   ‚îú‚îÄ‚îÄ plugin.py             # Plugin system
‚îÇ   ‚îú‚îÄ‚îÄ prediction/           # Predictive caching
‚îÇ   ‚îú‚îÄ‚îÄ processing/           # Request processing
‚îÇ   ‚îú‚îÄ‚îÄ reasoning/            # Reasoning engine
‚îÇ   ‚îú‚îÄ‚îÄ refactoring/          # Code refactoring
‚îÇ   ‚îú‚îÄ‚îÄ security/             # Security features
‚îÇ   ‚îú‚îÄ‚îÄ self_healing.py       # Self-healing system
‚îÇ   ‚îú‚îÄ‚îÄ self_learning/        # Self-learning capabilities
‚îÇ   ‚îú‚îÄ‚îÄ service.py            # Main service entry point
‚îÇ   ‚îú‚îÄ‚îÄ signature_help.py     # Code signature help
‚îÇ   ‚îú‚îÄ‚îÄ state_manager.py      # Session state management
‚îÇ   ‚îú‚îÄ‚îÄ testing/              # Test generation
‚îÇ   ‚îú‚îÄ‚îÄ ux/                   # User experience
‚îÇ   ‚îú‚îÄ‚îÄ validation/           # Response validation
‚îÇ   ‚îú‚îÄ‚îÄ versioning/           # Knowledge versioning
‚îÇ   ‚îî‚îÄ‚îÄ voice/                # Voice support
‚îú‚îÄ‚îÄ deploy/                   # Deployment configuration
‚îÇ   ‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ enterprise/
‚îÇ       ‚îî‚îÄ‚îÄ docker-compose.enterprise.yml
‚îú‚îÄ‚îÄ docs/                     # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ DEVELOPER_GUIDE.md
‚îú‚îÄ‚îÄ modules/                  # Processing modules
‚îÇ   ‚îú‚îÄ‚îÄ base_module.py
‚îÇ   ‚îú‚îÄ‚îÄ module_ai.py
‚îÇ   ‚îú‚îÄ‚îÄ module_completion.py
‚îÇ   ‚îú‚îÄ‚îÄ module_debug.py
‚îÇ   ‚îú‚îÄ‚îÄ module_generic.py
‚îÇ   ‚îú‚îÄ‚îÄ module_python.py
‚îÇ   ‚îú‚îÄ‚îÄ module_signature.py
‚îÇ   ‚îî‚îÄ‚îÄ registry.py
‚îú‚îÄ‚îÄ monitoring/               # Monitoring configuration
‚îÇ   ‚îú‚îÄ‚îÄ alert_rules.yml
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.json
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îú‚îÄ‚îÄ shared/                   # Shared components
‚îÇ   ‚îú‚îÄ‚îÄ config/               # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ knowledge/            # Knowledge graph
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py            # Data schemas
‚îú‚îÄ‚îÄ static/                   # Static web assets
‚îÇ   ‚îú‚îÄ‚îÄ css/                  # Stylesheets
‚îÇ   ‚îú‚îÄ‚îÄ js/                   # JavaScript
‚îÇ   ‚îî‚îÄ‚îÄ templates/            # HTML templates
‚îú‚îÄ‚îÄ tests/                    # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/          # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ performance/          # Performance tests
‚îÇ   ‚îú‚îÄ‚îÄ enterprise/           # Enterprise tests
‚îÇ   ‚îî‚îÄ‚îÄ test_orchestrator.py
‚îú‚îÄ‚îÄ vscode-extension/          # VS Code extension
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ cli/                      # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ mobile-app/               # React Native mobile app
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ .env                      # Environment variables
‚îú‚îÄ‚îÄ .gitignore               # Git ignore rules
‚îú‚îÄ‚îÄ package.json             # Node.js dependencies
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ webpack.config.js        # Webpack configuration
```

### Key Components

#### Core System (`core/`)
- **Orchestrator**: Central query processing and routing with offline support
- **Integrations**: Plugin system for LLM providers (Ollama, vLLM, HuggingFace, etc.)
- **Context**: Knowledge graph management and interaction tracking
- **Analytics**: Real-time monitoring dashboard
- **Enterprise**: SSO, team management, audit logging, compliance
- **Offline**: Cache management for offline operation
- **Voice**: Speech recognition and synthesis
- **Collaboration**: Live coding session management
- **Multimodal**: Image-based code analysis
- **Refactoring**: Intelligent code improvement suggestions
- **Self-Learning**: System that improves from user interactions
- **Versioning**: Knowledge graph state management and restoration
- **Security**: Authentication, authorization, and rate limiting

#### Modules (`modules/`)
- Specialized processing units for different tasks (Python, debugging, completion, etc.)
- Extensible plugin architecture
- Registry for dynamic module discovery and loading

#### Enterprise Features (`core/enterprise/`)
- **Authentication**: OAuth2 and SAML 2.0 integration for enterprise SSO
- **Teams**: Role-based access control, member management, resource sharing
- **Audit**: Comprehensive compliance logging with searchable audit trails

#### Configuration (`configs/`)
- Centralized configuration management
- Environment-specific settings
- SLA tiers and quality standards

### Contributing
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run pre-commit hooks
pre-commit install

# Start development server with hot reload
uvicorn core.service:app --reload

# Verify project consistency
python -c "from core.orchestrator import Orchestrator; print('‚úÖ Orchestrator OK')"
python -c "from core.service import AIService; print('‚úÖ Service OK')"
python -c "from modules.registry import ModuleRegistry; print('‚úÖ Registry OK')"
python -c "from core.enterprise.auth import EnterpriseAuthManager; print('‚úÖ Enterprise Auth OK')"
```

## üß™ Testing

Run the comprehensive test suite:
```bash
# Run all tests
pytest tests/

# Run specific test categories
pytest tests/unit/
pytest tests/integration/
pytest tests/performance/
pytest tests/enterprise/

# Run with coverage
pytest --cov=core tests/

# Verify project consistency
python -c "from core.orchestrator import Orchestrator; print('‚úÖ Orchestrator OK')"
python -c "from core.service import AIService; print('‚úÖ Service OK')"
python -c "from modules.registry import ModuleRegistry; print('‚úÖ Registry OK')"
python -c "from core.enterprise.auth import EnterpriseAuthManager; print('‚úÖ Enterprise Auth OK')"
```

## üìà Performance

The system is optimized for:
- **High Throughput**: 100+ requests per second
- **Low Latency**: <2s average response time
- **Memory Efficiency**: Optimized database queries and caching
- **Scalability**: Horizontal scaling with Docker and load balancing
- **Enterprise Ready**: High availability, audit compliance, team management

## üìã TO DO

### ‚úÖ Completed
- [x] Implement core architecture and orchestration
- [x] Add multi-LLM integration support
- [x] Implement self-learning and feedback processing
- [x] Add quality gates and validation
- [x] Implement caching and performance optimization
- [x] Add comprehensive error handling
- [x] Implement security layer (authentication, authorization)
- [x] Add testing infrastructure
- [x] Implement Docker containerization
- [x] Add monitoring and alerting
- [x] Implement multi-modal code analysis
- [x] Add advanced refactoring suggestions
- [x] Implement real-time collaboration features
- [x] Create VS Code extension
- [x] Build comprehensive analytics dashboard
- [x] Implement ML model management system
- [x] Add performance testing and optimization
- [x] Implement advanced rate limiting
- [x] Add database optimization
- [x] Implement knowledge graph versioning system
- [x] Fix all import and naming inconsistencies
- [x] Add CLI tool for command-line usage
- [x] Implement offline mode capabilities
- [x] Add voice command support
- [x] Create mobile app structure (React Native)
- [x] Implement enterprise SSO integration (OAuth2, SAML)
- [x] Add team management and role-based permissions
- [x] Implement comprehensive audit logging and compliance
- [x] Create enterprise deployment templates

### üöß In Progress
- [ ] Add mobile app UI implementation
- [ ] Implement advanced AI capabilities (code generation from natural language)
- [ ] Add ecosystem integrations (GitHub, GitLab, Jira, Slack/Teams)

### üìã Next Phase
- [ ] **Advanced AI Capabilities**
  - [ ] Implement automated test generation
  - [ ] Add bug prediction and prevention
  - [ ] Implement code documentation generation
- [ ] **Ecosystem Integration**
  - [ ] Integrate with GitHub/GitLab for seamless workflow
  - [ ] Add Jira integration for issue tracking
  - [ ] Create browser extension for web-based IDEs
- [ ] **Performance Enhancements**
  - [ ] Implement distributed caching cluster
  - [ ] Add horizontal scaling with Kubernetes
  - [ ] Implement edge caching for global users

## ü§ù Community

- **Documentation**: [Wiki](https://github.com/bozozeclown/open_llm/wiki)
- **Issues**: [GitHub Issues](https://github.com/bozozeclown/open_llm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/bozozeclown/open_llm/discussions)
- **Discord**: [Community Server](https://discord.gg/fTtyhu38)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üëè Acknowledgments

- **Open Source Community**: For the amazing libraries and tools that make this project possible
- **Contributors**: Everyone who has helped shape this project
- **Early Adopters**: For providing valuable feedback and suggestions
- **Enterprise Partners**: For guidance on compliance and security requirements

---

**Built with ‚ù§Ô∏è by the Open LLM community**